{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl7yR7P3CTTj"
   },
   "source": [
    "### ライブラリの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7IQqCE7mCeD"
   },
   "source": [
    "###モジュールのインポートとGoogleドライブのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLEMbaPJOs9v",
    "outputId": "f47ad633-eb58-49cd-8893-914d53aa4475"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.autograd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import datetime\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "import gc\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import skimage.transform\n",
    "from collections import deque\n",
    "from typing import Sequence, Dict, Tuple, Union, Optional\n",
    "#from argparse import Namespace\n",
    "from dataclasses import dataclass, field\n",
    "from sacrebleu.metrics import BLEU\n",
    "from evaluate import load\n",
    "import jiwer\n",
    "#from comet import download_model, load_from_checkpoint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "#from timm.scheduler import CosineLRScheduler\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "from evaluate import load\n",
    "\n",
    "import util\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "from torch.amp import autocast, GradScaler\n",
    "from collections import OrderedDict\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import json\n",
    "import collections\n",
    "from collections import Counter\n",
    "import plotly\n",
    "import optuna\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "import time\n",
    "from torch.profiler import profile, record_function, ProfilerActivity\n",
    "from multiprocessing import Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 位置エンコーディング\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5jwqxtGH7JR"
   },
   "source": [
    "### 位置エンコーディングの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja_g99AUIJTF"
   },
   "source": [
    "### Transformerデコーダの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siyThdM4Icll"
   },
   "source": [
    "### CaptioningTransformerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "#sos_token_id = 1\n",
    "#eos_token_id = 2\n",
    "a_token_id = tokenizer.encode( \"a\"  )[1]\n",
    "#print( a_token_id )\n",
    "the_token_id = tokenizer.encode( \"the\" )[1]\n",
    "and_token_id = tokenizer.encode( \"and\" )[1]\n",
    "in_token_id = tokenizer.encode( \"in\" )[1]\n",
    "we_token_id = tokenizer.encode( \"we\" )[1]\n",
    "i_token_id = tokenizer.encode( \"i\" )[1]\n",
    "he_token_id = tokenizer.encode( \"he\" )[1]\n",
    "she_token_id = tokenizer.encode( \"she\" )[1]\n",
    "it_token_id = tokenizer.encode( \"it\" )[1]\n",
    "they_token_id = tokenizer.encode( \"they\" )[1]\n",
    "period_token_id = tokenizer.encode( \".\" )[1]\n",
    "comma_token_id = tokenizer.encode( \",\" )[1]\n",
    "dbl_token_id = tokenizer.encode( '\"' )[1]\n",
    "sgl_token_id = tokenizer.encode( \"'\" )[1]\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "class ComputeReward(nn.Module):\n",
    "    def __init__(self, reward_t = 'ordinary', decode_t = 'ordinary', device=\"cpu\", \n",
    "                 repeat_thresh = (3,2,2,2,2), repeat_weight = (1, 1, 1, 1, 1), cider_coef = 1.0, rouge_coef = 1.0, clip_coef = 1.0, \n",
    "                 bert_coef = 1.0, use_amp = True ):\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tgt_lang = \"en\"\n",
    "        self.device = device\n",
    "\n",
    "        self.scorer = Cider()\n",
    "        self.rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        self.bert = load('bertscore' )\n",
    "        self.metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "        for param in self.metric.parameters():\n",
    "            param.requires_grad = False\n",
    "        self.reward_t = reward_t\n",
    "        self.repeat_thresh = repeat_thresh\n",
    "        self.repeat_weight = repeat_weight\n",
    "        self.decode_t = decode_t\n",
    "        self.cider_coef = cider_coef\n",
    "        self.rouge_coef = rouge_coef\n",
    "        self.clip_coef = clip_coef\n",
    "        self.bert_coef = bert_coef\n",
    "        self.use_amp = use_amp\n",
    "    \n",
    "    def _compute_reward_ord(self, preds, targets, imgs2, sources=None):\n",
    "        \"\"\"\n",
    "        Compute reward metric for a batch of prediction and target sentences\n",
    "        \"\"\"\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        # detokenize (convert to str) preds & targets\n",
    "        if self.decode_t == 'no-endoftext':\n",
    "            preds_str = [self.tokenizer.decode(\n",
    "                [pred[i] for i in range( 1,  len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                ) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(\n",
    "                [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                ) for target in targets]\n",
    "        elif self.decode_t == 'no-pad':\n",
    "            preds1 = preds.clone().to(self.device )\n",
    "            targets1 = targets.clone().to(self.device )\n",
    "            preds1[preds1 == eos_token_id] = pad_token_id\n",
    "            decoded = self.tokenizer.batch_decode(preds1, skip_special_tokens=False)\n",
    "            preds_str = [s.replace(\"[PAD]\", \"\").replace(\"[unused1]\", \"\").strip() for s in decoded]            \n",
    "            preds_str2 = self.tokenizer.batch_decode(preds1, skip_special_tokens = True )\n",
    "            targets1[targets1 == eos_token_id] = pad_token_id\n",
    "            decoded = self.tokenizer.batch_decode(targets1, skip_special_tokens=False)\n",
    "            targets_str = [s.replace(\"[PAD]\", \"\").replace(\"[unused1]\", \"\").strip() for s in decoded]            \n",
    "        else:\n",
    "            preds_str = [self.tokenizer.decode(pred) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(target) for target in targets]\n",
    "        sources_str = [self.tokenizer.decode(source, ref=\"src\") for source in sources] if sources is not None else None\n",
    "        #print( \"preds size:\", preds.size() )\n",
    "        #print( \"targets size:\", targets.size() )\n",
    "        \n",
    "        #print(f'1st target sent: {targets_str[0]}')\n",
    "        #print(f'1st pred sent: {preds_str[0]}')\n",
    "        \n",
    "        # compute reward metric\n",
    "        seq_len = preds.shape[1]\n",
    "\n",
    "        pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "        target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "        score, scores = self.scorer.compute_score(target_dict, pred_dict)\n",
    "        reward_cider = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "       \n",
    "        reward_rouge = [[self.rougeL.score(target, pred)['rougeL'][0]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "        reward_rouge = torch.tensor( reward_rouge ).to( self.device )\n",
    "        with autocast(str(self.device),enabled=self.use_amp):\n",
    "            with torch.no_grad():\n",
    "                processed = self.metric.processor(text=preds_str2, images=imgs2, return_tensors=\"pt\", padding=True, \\\n",
    "                                                  truncation=True, max_length=77, do_resize=False, do_rescale=False ).to(self.device)\n",
    "                outputs = self.metric.model(**processed)\n",
    "                # 特徴量の正規化\n",
    "                image_features = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                text_features = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                individual_scores = torch.clamp( (image_features.to(self.device) * \\\n",
    "                                                  text_features.to(self.device)).sum(axis=-1), min=0)\n",
    "                clip_scores = individual_scores[:,None].expand( -1, seq_len ) / 100.0\n",
    "                reward_clip = clip_scores\n",
    "                model_name = 'distilbert-base-uncased' \n",
    "                bert_scores = self.bert.compute(\n",
    "                    predictions=preds_str, \n",
    "                    references=targets_str,\n",
    "                    model_type=model_name,\n",
    "                    use_fast_tokenizer=True, \n",
    "                    lang='en', \n",
    "                    device=self.device,\n",
    "                    batch_size=config.batch_size * config.num_samples,  # メモリ許容範囲で大きく設定\n",
    "                    rescale_with_baseline=False\n",
    "                )['f1']\n",
    "                reward_bert = torch.tensor( bert_scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "        reward = self.cider_coef * reward_cider + self.rouge_coef * reward_rouge \\\n",
    "            + self.clip_coef * reward_clip + self.bert_coef * reward_bert\n",
    "        reward2 = reward_cider + reward_rouge + reward_bert + reward_clip\n",
    "        \n",
    "        return reward, reward2\n",
    "\n",
    "    def compute_length_reward(self, preds, targets):\n",
    "        # preds.size(1) を L として取得\n",
    "        max_len = preds.size(1)\n",
    "        # 1から始まるインデックスを作成 (長さとして計算するため)\n",
    "        arange_index = torch.arange(1, max_len + 1, device=self.device).float()\n",
    "        \n",
    "        def get_length(tokens):\n",
    "            # eosの位置を特定\n",
    "            eos_mask = (tokens == eos_token_id)\n",
    "            # 最初のeosだけを抽出\n",
    "            first_eos = (eos_mask.int().cumsum(dim=1) == 1) & eos_mask\n",
    "            \n",
    "            # eosがある場合はその位置(1-indexed)を、ない場合は最大長を返す\n",
    "            lengths = torch.sum(first_eos.float() * arange_index, dim=1)\n",
    "            # eosが一度も出現しなかった行は 0 になっているので、max_len で埋める\n",
    "            no_eos_mask = (eos_mask.sum(dim=1) == 0)\n",
    "            lengths[no_eos_mask] = float(max_len)\n",
    "            \n",
    "            # 正規化 (0.0 ~ 1.0)\n",
    "            return lengths / max_len\n",
    "\n",
    "        pred_lengths = get_length(preds)\n",
    "        target_lengths = get_length(targets)\n",
    "        #\n",
    "        # MSEの計算 (負の値を報酬とする)\n",
    "        # reduction='none' なので [batch_size] の形\n",
    "        reward_lengths = - ((pred_lengths - target_lengths) ** 2)\n",
    "        \n",
    "        # [batch_size, 1] にしてから [batch_size, seq_len] に拡張\n",
    "        reward = reward_lengths.unsqueeze(1).expand(-1, max_len)\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    def unique_ngram_ratio(self, preds):\n",
    "        bsz, seq_len = preds.size()\n",
    "        ng = 5\n",
    "        device = preds.device\n",
    "    \n",
    "        # 1. 各シーケンスの有効な長さを特定 (eosまで)\n",
    "        # cumsumを使って最初のeos以降をマスクする\n",
    "        eos_mask = (preds == eos_token_id)\n",
    "        first_eos_idx = (eos_mask.cumsum(dim=1) == 1) & eos_mask\n",
    "        # eosがない場合はseq_lenとする\n",
    "        lengths = torch.where(first_eos_idx.any(dim=1), \n",
    "                              first_eos_idx.float().argmax(dim=1), \n",
    "                              torch.tensor(seq_len, device=device)).unsqueeze(1)\n",
    "    \n",
    "        unr_list = []\n",
    "        \n",
    "        # n-gramのサイズごとに一括処理\n",
    "        for n in range(1, ng + 1):\n",
    "            # unfoldで全n-gramを抽出: (bsz, seq_len - n + 1, n)\n",
    "            if seq_len < n:\n",
    "                unr_list.append(torch.zeros(bsz, 1, device=device))\n",
    "                continue\n",
    "                \n",
    "            ngrams = preds.unfold(1, n, 1)\n",
    "            num_ngrams = ngrams.size(1)\n",
    "    \n",
    "            # マスク作成: 有効な長さ（eos前）に含まれるn-gramのみを残す\n",
    "            # n-gramの開始位置が (length - n + 1) 未満である必要がある\n",
    "            ngram_indices = torch.arange(num_ngrams, device=device).expand(bsz, -1)\n",
    "            valid_mask = ngram_indices < (lengths - n + 1)\n",
    "            \n",
    "            # 非常に大きい値で無効なn-gramを埋める（uniqueカウントから除外するため）\n",
    "            # または、バッチを跨いでユニーク判定するためにハッシュ化\n",
    "            # ここでは各バッチ行ごとにユニーク数を数える必要があるため、\n",
    "            # 完全なベクトル化には torch.unique の制限（バッチ非対応）を回避する工夫が必要\n",
    "            \n",
    "            # 解決策: 各行をユニークなオフセットでシフトして全体でuniqueをとる手法\n",
    "            # ただし、シンプルさとメモリ効率のため、ngループのみ残すのが現実的です。\n",
    "            \n",
    "            # 行ごとのUniqueカウント (Vectorized version of unique per row)\n",
    "            # 完全にforを消す場合、各n-gramを1つのスカラにパッキング(ハッシュ)して処理\n",
    "            if n == 1:\n",
    "                packed = ngrams.squeeze(-1)\n",
    "            else:\n",
    "                # 各要素を大きな基数でシフトして足し合わせ、1つの整数にする\n",
    "                max_val = preds.max() + 1\n",
    "                coeffs = max_val ** torch.arange(n, device=device)\n",
    "                packed = (ngrams * coeffs).sum(dim=-1)\n",
    "    \n",
    "            # 無効な位置にユニークな（重ならない）負の値を代入\n",
    "            invalid_fill = -1 - torch.arange(bsz * num_ngrams, device=device).reshape(bsz, num_ngrams)\n",
    "            packed = torch.where(valid_mask, packed.float(), invalid_fill.float())\n",
    "    \n",
    "            # 行ごとにユニーク数をカウント\n",
    "            # Note: PyTorchのuniqueはバッチ未対応なため、以下が最速の回避策の一つ\n",
    "            def count_unique_rowwise(t, mask):\n",
    "                # ソートして隣接要素との差を見ることでユニーク数を算出\n",
    "                t_sorted, _ = torch.sort(t, dim=1)\n",
    "                diffs = (t_sorted[:, 1:] != t_sorted[:, :-1]).int()\n",
    "                # 最初の要素 + 変化があった回数 - 無効値の数(バッチ内の無効分を補正)\n",
    "                unique_counts = diffs.sum(dim=1) + 1\n",
    "                # 無効な埋め草（すべてユニークに設定済み）の数を引いて、\n",
    "                # 有効なものが1つもなければ0にする処理\n",
    "                invalid_count = (~mask).sum(dim=1)\n",
    "                return (unique_counts - invalid_count).float()\n",
    "    \n",
    "            row_unique = count_unique_rowwise(packed, valid_mask)\n",
    "            row_total = valid_mask.sum(dim=1).clamp(min=1).float()\n",
    "            unr_list.append(row_unique / row_total)\n",
    "    \n",
    "        # 結果の整形\n",
    "        unr_tensor = torch.stack(unr_list, dim=1) # (bsz, ng)\n",
    "        return torch.mean(unr_tensor, dim=1)[:, None].expand(-1, seq_len)\n",
    "\n",
    "    def calc_ngram_repeat_fast(self, preds):\n",
    "        # preds が書き換わらないよう、この関数内では元の値を保護する\n",
    "        bsz, seq_len = preds.size()\n",
    "        ngram_cnt = torch.zeros(bsz, device=preds.device, dtype=torch.float)\n",
    "        \n",
    "        base_ignore = [pad_token_id, eos_token_id, cls_token_id, sep_token_id]\n",
    "        extra_ignore = [a_token_id, the_token_id, period_token_id, comma_token_id, and_token_id, in_token_id]\n",
    "    \n",
    "        for n in range(1, 5):\n",
    "            if seq_len < n:\n",
    "                continue\n",
    "    \n",
    "            current_ignore_ids = base_ignore + (extra_ignore if n == 1 else [])\n",
    "            # ignore_mask は bool なので preds に影響しません\n",
    "            ignore_mask = torch.isin(preds, torch.tensor(current_ignore_ids, device=preds.device))\n",
    "            \n",
    "            # 【重要】unfoldの後に .clone() を入れてメモリを切り離す\n",
    "            ngrams = preds.unfold(dimension=1, size=n, step=1).clone()\n",
    "            num_ngrams = ngrams.size(1)\n",
    "            \n",
    "            ngram_ignore_mask = ignore_mask.unfold(dimension=1, size=n, step=1).any(dim=-1)\n",
    "            \n",
    "            if n > 1:\n",
    "                # 語彙サイズに基づくハッシュ化\n",
    "                vocab_size_max = max(preds.max().item(), 100000)\n",
    "                weights = torch.pow(torch.tensor([vocab_size_max], device=preds.device), \n",
    "                                    torch.arange(n, device=preds.device)).long()\n",
    "                hashed_ngrams = (ngrams.long() * weights).sum(dim=-1)\n",
    "            else:\n",
    "                hashed_ngrams = ngrams.squeeze(-1).long()\n",
    "    \n",
    "            # これで hashed_ngrams を書き換えても、clone 済みのデータなので preds は安全\n",
    "            invalid_val = -1\n",
    "            hashed_ngrams[ngram_ignore_mask] = invalid_val\n",
    "    \n",
    "            for b in range(bsz):\n",
    "                b_ngrams = hashed_ngrams[b]\n",
    "                valid_b_ngrams = b_ngrams[b_ngrams != invalid_val]\n",
    "                \n",
    "                if valid_b_ngrams.numel() == 0:\n",
    "                    continue\n",
    "                    \n",
    "                unique_vals, counts = torch.unique(valid_b_ngrams, return_counts=True)\n",
    "                \n",
    "                mask = counts >= self.repeat_thresh[n-1]\n",
    "                ngram_cnt[b] += counts[mask].sum().float() * self.repeat_weight[n-1]\n",
    "    \n",
    "        penalty = - torch.clamp(torch.pow(2.0, ngram_cnt - 1.0) / seq_len, max=1.0)\n",
    "        return penalty[:, None].expand(-1, seq_len)\n",
    "        \n",
    "    def forward(self, b2_preds, b2_targets, b2_imgs2,  sources=None, masks=None):\n",
    "        \"\"\"\n",
    "        outputs: batch x len x d_model\n",
    "        targets: batch x len\n",
    "        sources: batch x len\n",
    "        masks:   batch x len\n",
    "        \"\"\"\n",
    "        self.device = b2_preds.device\n",
    "        # input to device\n",
    "        targets = b2_targets.to(self.device)\n",
    "        bsz, seq_len = b2_preds.size()\n",
    "        eps = 1e-8\n",
    "\n",
    "        if self.reward_t == 'ordinary':\n",
    "            reward_ord = self.compute_reward(b2_preds, b2_targets, b2_imgs2, sources)   #  bsz\n",
    "            reward_repeat = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_length = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == \"ord+rep\":\n",
    "            reward_ord = self._compute_reward_ord(b2_preds, b2_targets, b2_imgs2, sources)  # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( b2_preds ) # bsz * seq_len\n",
    "            reward_length = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == \"ord+rep+len\":\n",
    "            reward_ord, reward_ord2 = self._compute_reward_ord(b2_preds, b2_targets, b2_imgs2, sources)  # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( b2_preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( b2_preds, b2_targets ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == 'ord+len':\n",
    "            reward_ord = self._compute_reward_ord(b2_preds, b2_targets, b2_imgs2, sources)  # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( b2_preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( b2_preds, b2_targets ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == 'ord+rep+len+unr':\n",
    "            reward_ord, reward_ord2 = self._compute_reward_ord(b2_preds, b2_targets, b2_imgs2, sources)  # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( b2_preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( b2_preds, b2_targets ) # bsz * seq_len\n",
    "            reward_unr = self.unique_ngram_ratio(b2_preds)\n",
    "        \n",
    "        ## apply mask\n",
    "        #if masks is not None:\n",
    "        #    masks = masks.to(self.device)\n",
    "        #    probs, targets = probs[masks], targets[masks]\n",
    "        #    # outputs, targets = outputs[masks], targets[masks]\n",
    "        #    reward, preds = reward[masks], preds[masks]\n",
    "       \n",
    "        #print(f'loss: {loss.item():.3f} | reward: {reward:.3f}')    \n",
    "        \n",
    "        return reward_ord, reward_ord2, reward_repeat, reward_length, reward_unr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, dim=1):\n",
    "    return torch.logsumexp(x.float(), dim=dim).type_as(x)\n",
    "\n",
    "class DynamicCRF(nn.Module):\n",
    "    def __init__(self, num_embedding, low_rank=32, beam_size=64, crf_coef=1.0, temp = 0.5, num_samples = 11, ref_t = False ):\n",
    "        super().__init__()\n",
    "\n",
    "        #low_rank = num_embedding\n",
    "        self.E1 = nn.Embedding(num_embedding, low_rank)\n",
    "        self.E2 = nn.Embedding(num_embedding, low_rank)\n",
    "\n",
    "        self.vocb = num_embedding\n",
    "        self.rank = low_rank\n",
    "        self.beam = beam_size\n",
    "        self.crf_coef = crf_coef\n",
    "        self.temp = temp\n",
    "        self.num_samples = num_samples\n",
    "        self.ref_t = ref_t\n",
    "        \n",
    "    def extra_repr(self):\n",
    "        return \"vocab_size={}, low_rank={}, beam_size={}\".format(\n",
    "            self.vocb, self.rank, self.beam)\n",
    "\n",
    "    def forward(self, emissions, top_logits, top_indices, targets, masks, beam=None):\n",
    "        numerator = self._compute_score(emissions, targets, masks)\n",
    "        denominator = self._compute_normalizer(emissions, targets, masks, beam )\n",
    "        beam_probs = self._compute_normalizer2(top_logits, top_indices, targets, masks, beam)\n",
    "\n",
    "        return numerator - denominator, beam_probs\n",
    "    \n",
    "    def forward_decoder(self, emissions, masks=None, beam=None):\n",
    "        return self._viterbi_decode(emissions, masks, beam)\n",
    "\n",
    "    def _compute_score(self, emissions, targets, masks=None):\n",
    "        batch_size, seq_len = targets.size()\n",
    "\n",
    "        emission_scores = emissions.gather(2, targets[:, :, None])[:, :, 0]  # B x T\n",
    "        transition_scores = (self.E1(targets[:, :-1]) * self.E2(targets[:, 1:])).sum(2)\n",
    "       \n",
    "        scores = emission_scores\n",
    "        scores[:, 1:] += transition_scores\n",
    "        \n",
    "        if masks is not None:\n",
    "            scores = scores * masks.type_as(scores)\n",
    "\n",
    "        return scores.sum(-1)\n",
    "        \n",
    "    def _compute_normalizer(self, emissions, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        if targets is not None:\n",
    "            #_emissions = emissions.scatter(2, targets[:, :, None], np.float('inf'))\n",
    "            _emissions = emissions.scatter(2, targets[:, :, None], float('inf'))\n",
    "            beam_targets = _emissions.topk(beam, 2)[1]\n",
    "            beam_emission_scores = emissions.gather(2, beam_targets)\n",
    "        else:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step.\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        for i in range(1, seq_len):\n",
    "            next_score = score[:, :, None] + beam_transition_matrix[:, i-1]\n",
    "            next_score = logsumexp(next_score, dim=1) + beam_emission_scores[:, i]\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i:i+1], next_score, score)\n",
    "            else:\n",
    "                score = next_score\n",
    "\n",
    "        return logsumexp(score, dim=1)\n",
    "\n",
    "    #def _compute_grpo_samples(beam_emission_scores, beam_transition_matrix, beam_targets, targets=None, masks=None, beam=None):\n",
    "    def _compute_grpo_samples(self, beam_emission_scores, beam_transition_matrix, beam_targets, targets=None, masks=None, beam=None):\n",
    "\n",
    "        \n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "        traj_scores2 = []\n",
    "        traj_tokens2 = []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        _score2 = beam_emission_scores[:,0][:,None,:].expand( -1, self.num_samples, -1 ) #B * self.num_samples * K\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score) # bsz * beam\n",
    "            traj_scores2.append( _score2 )\n",
    "            _score2 = _score2[:,:,:,None] + beam_transition_matrix[:, i-1,None,:,:].expand( -1, self.num_samples,-1,-1) \n",
    "                    # bsz, self.num_samples, bema, beam\n",
    "\n",
    "            # greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "\n",
    "            ## multinomial selection\n",
    "            B, N, C, W = _score2.shape\n",
    "            flat_score = _score2.permute(0, 3, 1, 2).reshape(-1, C)\n",
    "            #flat_score = torch.clamp( flat_score, min = -100, max = 100 )\n",
    "            probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            _index_flat = torch.multinomial(probs, num_samples=1, replacement = True )  \n",
    "\n",
    "            _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            _index2 = _index_flat.view(B, W, self.num_samples).transpose(1,2)\n",
    "            _score2 = _score_flat.view(B, W, self.num_samples).transpose(1,2)\n",
    "\n",
    "            _score2 = _score2 + beam_emission_scores[:, i][:,None,:].expand(-1,self.num_samples,-1) # bsz, self.num_samples, beam   \n",
    "\n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score2[:,0,:], _index2[:,0,:]\n",
    "            traj_tokens.append(index)\n",
    "            traj_tokens2.append( _index2 )\n",
    "\n",
    "\n",
    "        all_scores = traj_scores2\n",
    "        all_scores.append( _score2 )\n",
    "        all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 ).to(device) #bsz, seq_len, beam, N\n",
    "        beam_probs = F.softmax( all_scores.transpose( 2, 3 ), dim = 2 ) #bsz, seq_len, beam, N\n",
    "        #beam_probs = F.softmax( all_scores.permute( 3, 0, 1, 2 ), dim = 3 ) #N, bsz, seq_len, beam\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = _score2.max(dim=2) # max( bsz, beam ), bsz, N\n",
    "        finalized_tokens.append(best_index[:, None, :]) #bsz,1, N\n",
    "        finalized_scores.append(best_score[:, None, :]) #bsz,1, N\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens2), reversed(traj_scores2)): # each of seq_len -1, bsz, beam, N \n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(2, previous_index))\n",
    "            finalized_scores.append(scs.gather(2, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse() # seq_len, bsz, N\n",
    "        sampled_beam_idx = torch.cat(finalized_tokens, 1) # seq_len, bsz, N\n",
    "        finalized_tokens = beam_targets.gather(2, sampled_beam_idx)\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        #return beam_probs, sampled_beam_idx, finalized_tokens, finalized_scores \n",
    "        return beam_probs, sampled_beam_idx, finalized_tokens \n",
    "    \n",
    "    '''\n",
    "    def _viterbi_decode(self, emissions, masks=None, beam=None):\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        #print( \"score size:\", score.size() )\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return finalized_scores, finalized_tokens\n",
    "\n",
    "   \n",
    "    '''\n",
    "    def _compute_many_values(self, emissions, targets, top_indices = None, masks=None, beam=None):\n",
    "\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        \n",
    "        if top_indices == None:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        else:\n",
    "            beam_emission_scores = torch.gather( emissions, -1, top_indices )\n",
    "            beam_targets = top_indices\n",
    "        \n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        if not self.ref_t:\n",
    "            traj_tokens, traj_scores = [], []\n",
    "            finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "            # compute the normalizer in the log-space\n",
    "            score = beam_emission_scores[:, 0]  # B x K\n",
    "            dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "            for i in range(1, seq_len):\n",
    "                traj_scores.append(score)\n",
    "                _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "                _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam  step i-1 における 256 → 256 の max から 256 への遷移確率と \n",
    "                                                # 256 → 256 の前の 256 の max のインデックストークン\n",
    "                                                # index b * 256 の 位置が i の token で、値が i-1 のtoken   \n",
    "\n",
    "                _score = _score + beam_emission_scores[:, i] # bsz, beam i における 256 の遷移確率ではない確率を加える。i における 256 の全確率。\n",
    "\n",
    "                if masks is not None:\n",
    "                    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "                else:\n",
    "                    score, index = _score, _index\n",
    "                traj_tokens.append(index)\n",
    "\n",
    "            all_scores = traj_scores\n",
    "            all_scores.append( score )\n",
    "            all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 )\n",
    "        \n",
    "            # now running the back-tracing and find the best\n",
    "            best_score, best_index = score.max(dim=1)\n",
    "            finalized_tokens.append(best_index[:, None]) # 時刻 T における b*256 の確率最大の token\n",
    "            finalized_scores.append(best_score[:, None]) #時刻 T における b*256 の確率最大の score\n",
    "\n",
    "            for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)): #idx,scs は、反転時刻 i と i-1における b * 256のトークンと確率\n",
    "                previous_index = finalized_tokens[-1] # 時刻 Tなど 求めたいトークンと確率の一個後 における token　b * 1\n",
    "                finalized_tokens.append(idx.gather(1, previous_index)) # 時刻 一個後iのトークン previou_index に至るための時刻i-1 のトークン\n",
    "                                                                    # b* 256 の token から b * 1 の previous_idnex token で gather\n",
    "                finalized_scores.append(scs.gather(1, previous_index)) # 時刻一個後 i のトークンに至るための時刻 i-1 の確率\n",
    "\n",
    "            finalized_tokens.reverse()\n",
    "            finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "            finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "            finalized_scores.reverse()\n",
    "            finalized_scores = torch.cat(finalized_scores, 1)\n",
    "            finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "        \n",
    "            if self.crf_coef != 0.0:\n",
    "                numerator = self._compute_score(emissions, targets)\n",
    "                denominator = self._compute_normalizer(emissions, targets)\n",
    "                crf_loss = - ( numerator - denominator ).mean() / seq_len\n",
    "            else:\n",
    "                crf_loss = torch.zeros( (1), device = emissions.device, dtype = torch.float )\n",
    "        \n",
    "        top_probs, sampled_beam_idx, b_finalized_tokens = \\\n",
    "            self._compute_grpo_samples(beam_emission_scores, beam_transition_matrix, beam_targets)\n",
    "\n",
    "        if not self.ref_t:\n",
    "            return finalized_scores, finalized_tokens, top_probs, beam_targets, crf_loss, sampled_beam_idx, b_finalized_tokens\n",
    "        else:\n",
    "            return top_probs[:,:,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, crf_low_rank, crf_beam_size, dropout, padding_idx,\n",
    "                crf_coef = 1.0, temp = 0.5, num_samples = 10, ref_t = False ):\n",
    "        super(TopLayer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.padding_idx = padding_idx\n",
    "        print( \"in TopLayer:\" )\n",
    "        self.crf_layer = DynamicCRF(num_embedding = vocab_size, low_rank = crf_low_rank, beam_size = crf_beam_size, \n",
    "                                    crf_coef=crf_coef, temp=temp, num_samples= num_samples, ref_t = ref_t )\n",
    "\n",
    "        #self.one_more_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        #self.tgt_word_prj = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "        ## gae 学習用\n",
    "        #self.linear_critical = nn.Linear(crf_beam_size, 1 )\n",
    "\n",
    "    def forward(self, src_representation, top_logits, top_indices, src_input, tgt_input, is_training ):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        #assert src_input.size() == tgt_input.size()\n",
    "\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        #seqlen, bsz = src_input.size()\n",
    "        seqlen, bsz = src_input.shape[:2]\n",
    "\n",
    "        src_representation = F.dropout(src_representation, p=self.dropout, training=is_training)\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "\n",
    "        src = src_representation\n",
    "\n",
    "        #emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "        emissions = src_representation\n",
    "        #log_probs = torch.log_softmax(emissions, -1)\n",
    "        #assert log_probs.size() == torch.Size([seqlen, bsz, self.vocab_size])\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz x src_len x vocab_size]\n",
    "        #emission_mask = ~tgt_input.eq(self.padding_idx) # [bsz x src_len] #pad のところは 0 padでないところが 1\n",
    "        emission_mask = torch.ones_like( tgt_input, dtype=torch.bool ) #全部　pad でないとして 1\n",
    "        batch_crf_loss, top_probs = self.crf_layer(emissions, top_logits, top_indices, tgt_input, emission_mask) # [bsz]\n",
    "        #critical_value = self.linear_critical( top_probs )\n",
    "        #critical_value = torch.zeros( ( 1,1,1) )\n",
    "        batch_crf_loss = - batch_crf_loss\n",
    "        assert batch_crf_loss.size() == torch.Size([bsz])\n",
    "        return batch_crf_loss, top_probs\n",
    "\n",
    "    def decoding(self, src_representation, src_input):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        seqlen, bsz = src_input.size()\n",
    "\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "        src = src_representation\n",
    "\n",
    "        emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        _, finalized_tokens = self.crf_layer.forward_decoder(emissions)\n",
    "        assert finalized_tokens.size() == torch.Size([bsz, seqlen])\n",
    "        return finalized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcmR9lKrIbiL"
   },
   "outputs": [],
   "source": [
    "class CaptioningTransformer(nn.Module):\n",
    "    \n",
    "    #CaptioningTransformerのコンストラクタ\n",
    "    #dim_embedding  : 埋め込み次元\n",
    "    #dim_feedforward: FNNの中間特徴次元\n",
    "    #num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    #num_layers     : Transformerデコーダ層の数\n",
    "    #vocab_size     : 辞書の次元\n",
    "    #null_index     : NULLのID\n",
    "    #dropout        : ドロップアウト確率\n",
    "    \n",
    "    def __init__(self, img_size: int,  dim_embedding: int, length_max: int, vocab_size: int, tokenizer, dropout: float = 0.0, \\\n",
    "                 pad_token_id: int=0, use_repeat_logits_half=False, crf_coef = 1.0, temp=0.5, num_samples=10, ref_t = False):\n",
    "        super().__init__()\n",
    "\n",
    "        #CLIP\n",
    "        model_id = \"openai/clip-vit-large-patch14-336\"\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(model_id )\n",
    "        memory = self.clip_model( torch.randn( 1, 3, 336, 336 ) )\n",
    "        memory = memory.last_hidden_state\n",
    "        img_length = memory.size(1)\n",
    "        clip_dim = memory.size(2)\n",
    "        self.connector_pool = nn.AdaptiveAvgPool1d(length_max - 1 )\n",
    "        self.connector_ln = nn.LayerNorm( clip_dim )\n",
    "        self.connector_linear1 = nn.Linear( clip_dim, dim_embedding )\n",
    "        self.connector_gleu = nn.GELU()\n",
    "        self.connector_linear2 = nn.Linear( dim_embedding, dim_embedding )\n",
    "\n",
    "       \n",
    "        # Connector\n",
    "        self.connector_pool = nn.AdaptiveAvgPool1d(length_max - 1 )\n",
    "       # Down Sampling\n",
    "        cls_token = memory[:, :1, :] # (bsz, 1, 1024)\n",
    "        patch_tokens = memory[:, 1:, :] # (bsz, 576, 1024)\n",
    "        # パッチ部分を 576 -> 96 に圧縮\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 1024, 576)\n",
    "        patch_tokens = self.connector_pool(patch_tokens)\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 96, 1024)\n",
    "        # CLSと結合して合計 97 トークンにする\n",
    "        memory = torch.cat([cls_token, patch_tokens], dim=1) # (bsz, 97, 1024)\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding( dim_embedding )\n",
    "\n",
    "        model_id = \"google-bert/bert-large-uncased\"\n",
    "        self.bert = BertModel.from_pretrained( model_id )\n",
    "\n",
    "        ## 単語出力分布計算\n",
    "        self.ln_outputs = nn.LayerNorm( dim_embedding )\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "\n",
    "        crf_low_rank = 32\n",
    "        crf_beam_size = 256\n",
    "        self.crf_beam_size = crf_beam_size\n",
    "        top_dropout = 0.0\n",
    "        tgt_padding_idx = tokenizer.pad_token_id\n",
    "        print( \"initialize self.toplayer\" )\n",
    "        if ref_t:\n",
    "            num_samples = 1\n",
    "        self.ref_t = ref_t\n",
    "        self.toplayer = TopLayer( vocab_size, dim_embedding, crf_low_rank, crf_beam_size, top_dropout, \n",
    "                                  tgt_padding_idx, crf_coef = crf_coef, temp=temp, num_samples=num_samples, ref_t = ref_t )\n",
    "\n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.use_repeat_logits_half = use_repeat_logits_half\n",
    "\n",
    "\n",
    "    def mlp_connector(self, memory ):\n",
    "\n",
    "        cls_token = memory[:, :1, :] # (bsz, 1, 1024)\n",
    "        patch_tokens = memory[:, 1:, :] # (bsz, 576, 1024)\n",
    "\n",
    "        # パッチ部分を 576 -> 96 に圧縮\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 1024, 576)\n",
    "        patch_tokens = self.connector_pool(patch_tokens)\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 96, 1024)\n",
    "\n",
    "        # CLSと結合して合計 97 トークンにする\n",
    "        memory = torch.cat([cls_token, patch_tokens], dim=1) # (bsz, 97, 1024)\n",
    "\n",
    "        memory = self.connector_ln( memory )\n",
    "        memory = self.connector_linear1( memory )\n",
    "        memory = self.connector_gleu( memory )\n",
    "        memory = self.connector_linear2( memory )\n",
    "        \n",
    "        return memory\n",
    "\n",
    "    def forward(self, images: torch.Tensor, targets: torch.Tensor, top_indices = None ):\n",
    "\n",
    "        self.device = images.device\n",
    "        \n",
    "        memory = self.clip_model( images ).last_hidden_state\n",
    "        memory = self.mlp_connector( memory )\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        emissions = self.linear( outputs )\n",
    "\n",
    "        if self.use_repeat_logits_half == True:\n",
    "            emissions = repeat_logits_half( emissions )\n",
    "\n",
    "        if not self.ref_t:\n",
    "            finalized_scores, finalized_tokens, top_probs, top_indices, crf_loss, sampled_beam_idx, b_finalized_tokens  = \\\n",
    "                self.toplayer.crf_layer._compute_many_values(emissions, targets, top_indices)\n",
    "            return finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "                 crf_loss, emissions, sampled_beam_idx, b_finalized_tokens\n",
    "        else:\n",
    "            top_probs = self.toplayer.crf_layer._compute_many_values(emissions, targets, top_indices)\n",
    "            return top_probs\n",
    "\n",
    "    def repeat_logits_half(self, emissions ):\n",
    "        \n",
    "        penalty = 1.2\n",
    "        scores, preds = torch.max( emissions, 2 )\n",
    "        masks = emissions == scores[:,:,None]\n",
    "        masks = masks.permute( 1, 0, 2 )\n",
    "        new_mask = torch.zeros( (  masks.size(1), masks.size(2)), device = emissions.device, dtype=torch.bool )\n",
    "        new_masks = torch.zeros( ( masks.size(0), masks.size(1), masks.size(2)), device = emissions.device, dtype=torch.bool )\n",
    "        for i, mask in enumerate( masks ):\n",
    "            new_mask = torch.logical_or( mask,  new_mask  )\n",
    "            new_masks[i] = new_mask\n",
    "        new_masks = new_masks.transpose(0,1)\n",
    "        first_true_mask = ( new_masks.int().cumsum(dim = 1 ) == 1 ) & new_masks\n",
    "        new_masks = new_masks & ( ~first_true_mask )\n",
    "\n",
    "        p_masks = emissions > 0\n",
    "        m_masks = emissions < 0\n",
    "        p_new_masks = p_masks & new_masks\n",
    "        m_new_masks = p_masks & new_masks\n",
    "        emissions2 = emissions.clone()\n",
    "        emissions2[p_new_masks] = emissions[p_new_masks] / penalty\n",
    "        emissions2[m_new_masks] = emissions2[m_new_masks] * penalty\n",
    "\n",
    "        return emissions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path: str, img_directory: str, transforms, transforms2, tokenizer, length_max = None ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_directory = img_directory\n",
    "        self.transforms = transforms\n",
    "        self.transforms2 = transforms2 \n",
    "        # TODO: fix to original data\n",
    "        #画像の前処理\n",
    "        self.img_file = []\n",
    "        self.tokens = []\n",
    "        #vocab_size = len( tokenizer )\n",
    "        #c1 = torch.zeros( ( vocab_size ) )\n",
    "        #c2 = torch.zeros( ( vocab_size, vocab_size ) )\n",
    "        if length_max == None:\n",
    "            self.length_max = 0\n",
    "        else:\n",
    "            self.length_max = length_max\n",
    "        length_sum = 0\n",
    "\n",
    "        with open( file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for i, line_data in enumerate( data ):\n",
    "            if i % 100000 == 0:\n",
    "                print( \"i:\", i )\n",
    "            self.img_file.append( line_data['img_file'] )\n",
    "            id_tokens = line_data['id_tokens']\n",
    "            id_tokens.append( eos_token_id )\n",
    "            id_tokens.append( eos_token_id )\n",
    "            length_sum += len( id_tokens )\n",
    "            if length_max != None:\n",
    "                id_tokens = torch.tensor( id_tokens )[:self.length_max]\n",
    "            else:\n",
    "                if self.length_max < len( id_tokens ):\n",
    "                    self.length_max = len( id_tokens )\n",
    "                id_tokens = torch.tensor( id_tokens )\n",
    "            self.tokens.append( id_tokens )\n",
    "        # w1, w2 を作る時は length_max = None　でお願いします。\n",
    "        #    for i2 in range( len(id_tokens) ):\n",
    "        #        if i2 == len( id_tokens ) - 1:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #        else:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #            c2[id_tokens[i2], id_tokens[i2+1] ] += 1\n",
    "        '''\n",
    "        c1avg = int( torch.sum( c1 ) / torch.sum( torch.ne( c1, 0 ).int()) )\n",
    "        c2avg = int( torch.sum( torch.sum( c2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( c2, 0 ).int() ) )\n",
    "\n",
    "        c1[0] = c1avg\n",
    "\n",
    "        c2[:,0] = c2avg\n",
    "        c2[0,:] = c2avg\n",
    "        \n",
    "        sumc1 = torch.sum( c1, dim = 0 )\n",
    "        sumc2 = torch.sum( torch.sum( c2, dim = 1 ), dim = 0 )\n",
    "\n",
    "        prob1 = c1 / sumc1\n",
    "        prob2 = c2 / sumc2\n",
    "\n",
    "        self.w1 = prob1 ** -0.4\n",
    "        self.w1 = torch.nan_to_num( self.w1, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg1 = torch.sum( self.w1, dim = 0 ) / torch.sum( torch.ne( self.w1, 0.0 ).int() )\n",
    "        self.w1 = self.w1 / avg1\n",
    "\n",
    "        self.w2 = prob2 ** -0.4\n",
    "        self.w2 = torch.nan_to_num( self.w2, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg2 = torch.sum( torch.sum( self.w2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( self.w2, 0.0 ).int() )\n",
    "        self.w2 = self.w2 / avg2\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_unigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w1, f )\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_bigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w2, f )\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_unigram.pkl\", 'rb') as f:\n",
    "        #    self.w1 = pickle.load(f)\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_bigram.pkl\", 'rb') as f:\n",
    "        #    self.w2 = pickle.load(f)\n",
    "        \n",
    "        if length_max == None:\n",
    "            print( \"length max:\", self.length_max )\n",
    "            print( \"avg length:\", length_sum / len( self.tokens ) )\n",
    "    \n",
    "    # ここで取り出すデータを指定している\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        tokens = self.tokens[index]\n",
    "        img_file = self.img_file[index] + \".jpg\"\n",
    "        img_path = os.path.join( self.img_directory, img_file ) #index番目の画像のパスを取得\n",
    "        img = Image.open(img_path) #PIL形式で画像を読み込み\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        img1 = self.transforms(img)\n",
    "        img2 = self.transforms2(img)\n",
    "        \n",
    "        return img1, img2, tokens\n",
    "\n",
    "    # この method がないと DataLoader を呼び出す際にエラーを吐かれる\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def length_max(self):\n",
    "        return self.length_max\n",
    "\n",
    "    #def w1(self):\n",
    "    #    return self.w1\n",
    "\n",
    "    #def w2(self):\n",
    "    #    return self.w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, str]]], pad_index, length_max ):\n",
    "    imgs1, imgs2, tokens = zip(*batch)\n",
    "\n",
    "    max_length = length_max\n",
    "    #max_length = 0\n",
    "    #for target in tokens:\n",
    "    #    if max_length < len( target ):\n",
    "    #        max_length = len( target )\n",
    "    \n",
    "    targets = []\n",
    "    lengths = []\n",
    "    for target in tokens:\n",
    "        pad_len = max_length - len( target ) \n",
    "        #print( \"target:\", target )\n",
    "        input2= F.pad( target, (0, pad_len), mode='constant', value = pad_index)\n",
    "        targets.append( input2 )\n",
    "        lengths.append( len( target ) )\n",
    "    \n",
    "    imgs1 = torch.stack( imgs1, dim = 0 )\n",
    "    imgs2 = torch.stack( imgs2, dim = 0 )\n",
    "    targets = torch.stack( targets, dim = 0 )\n",
    "    lengths = torch.tensor( lengths, requires_grad = False  )\n",
    "\n",
    "    return imgs1, imgs2, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4x-PO05mCS-"
   },
   "source": [
    "###学習におけるハイパーパラメータやオプションの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQES3A8OG-V_"
   },
   "outputs": [],
   "source": [
    "class ConfigTrain(object):\n",
    "    '''\n",
    "    ハイパーパラメータ、システム共通変数の設定\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        # ハイパーパラメータ\n",
    "        self.img_size = 336\n",
    "        self.dim_embedding = 1024   # 埋め込み層の次元\n",
    "        self.length_max = 97\n",
    "        #self.lr = 5e-5            # 学習率\n",
    "        #self.lr = 2e-5            # 学習率\n",
    "        #self.lr_clip = 2e-7\n",
    "        #self.lr_bert = 2e-5            # 学習率\n",
    "        #self.lr_others = 1e-4\n",
    "        self.lr_clip = 0.0\n",
    "        self.lr_con = 1.66e-11\n",
    "        self.lr_bert = 2.19e-9            # 学習率\n",
    "        self.lr_cri = 7.20e-6\n",
    "        self.lr_others = 5.82e-8\n",
    "        #self.clip_thresh_clip = 1\n",
    "        self.clip_thresh_con = 3.3e-4\n",
    "        self.clip_thresh_bert = 9e-3            # 学習率\n",
    "        self.clip_thresh_cri = 8.60e-7\n",
    "        self.clip_thresh_others = 3.3e-4\n",
    "        #self.lr_top = 1e-4\n",
    "        #self.lr = 5e-6            # 学習率\n",
    "        self.dropout = 0.0         # dropout確率\n",
    "        #self.batch_size = 160       # ミニバッチ数\n",
    "        #self.batch_size = 128       # ミニバッチ数\n",
    "        self.batch_size = 120       # ミニバッチ数\n",
    "        #self.batch_size = 104\n",
    "        #self.batch_size = 96       # ミニバッチ数\n",
    "        #self.batch_size = 80       # ミニバッチ数\n",
    "        #self.batch_size = 64\n",
    "        #self.batch_size = 48\n",
    "        #self.batch_size = 40       # ミニバッチ数\n",
    "        #self.batch_size = 32       # ミニバッチ数\n",
    "        #self.batch_size = 24       # ミニバッチ数\n",
    "        #self.batch_size = 16       # ミニバッチ数\n",
    "        #self.batch_size = 8       # ミニバッチ数\n",
    "        self.num_epochs = 1       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        self.use_amp = True\n",
    "        #self.use_amp = False\n",
    "        self.use_saved_pth = True\n",
    "        #self.use_saved_pth = False\n",
    "        self.vocab_size = len( tokenizer )\n",
    "        self.weight_decay = 0.0232\n",
    "        self.betas = (0.9, 0.999 )\n",
    "        self.warmup = 0.1\n",
    "        self.metric = \"special\" # bleu, meteor, wer, rouge\n",
    "        self.decode_t = \"no-pad\"\n",
    "        self.reward_t = \"ord+rep+len+unr\"\n",
    "        #self.reward_t = \"ordinary\"\n",
    "        self.clip_range = 0.235\n",
    "        self.clip_grad_threshold = 1.37\n",
    "        self.ord_coef = 1.0\n",
    "        self.cider_coef = 1.0\n",
    "        self.rouge_coef = 2.53\n",
    "        self.clip_coef = 1.65\n",
    "        self.bert_coef = 6.68\n",
    "        self.rep_coef = 5.84\n",
    "        self.repeat_thresh = ( 3,2,2,2,2 )\n",
    "        self.repeat_weight = ( 1,1,1,1,1 )\n",
    "        self.len_coef = 4.17\n",
    "        self.unr_coef = 3.79\n",
    "        self.policy_coef = 1.0\n",
    "        self.crf_coef = 0.205\n",
    "        self.ce_coef = 0.661\n",
    "        self.ent_coef = 0.00269\n",
    "        self.cri_coef = 0.0 # モンテカルロ法\n",
    "        self.gae_coef = 0.0 # GAE\n",
    "        self.kl_coef = 0.0401\n",
    "        self.target_kl = 8.0\n",
    "        self.buffer_kl = 1.2\n",
    "        self.kl_max = 0.1\n",
    "        self.kl_min = 0.1\n",
    "        self.gamma = 0.972\n",
    "        self.lam = 0.974\n",
    "        self.use_repeat_logits_half = False\n",
    "        self.use_ce_bert = True\n",
    "        self.display_include_coef = True\n",
    "        self.use_adaptive_KL = False\n",
    "        self.temp = 1.0\n",
    "        self.num_samples = 8\n",
    "        self.residual_samples = 3\n",
    "\n",
    "        \n",
    "        # パスの設定\n",
    "        self.img_directory = '/mnt/ssd2/v7/img'\n",
    "        self.anno_file = '/mnt/ssd2/v7/data.pkl'\n",
    "        self.save_directory = './model'\n",
    "        #self.PATH = \"model/model_ar_hfgpt2_v7_curr.pth\"\n",
    "        #self.PATH = \"../test/model/model_bert_large_NAR_PAD_curr.pth\"\n",
    "        self.PATH = \"../test/pre_train_crf/model/model_bert_large_NAR_PAD_sft2_final.pth\"\n",
    "        #self.PATH = \"model/model_bert_large_NAR_PAD_curr.pth\"\n",
    "        #self.PATH = \"model/model_bert_mask_curr.pth\"\n",
    "\n",
    "        # 検証に使う学習セット内のデータの割合\n",
    "        self.test_ratio = 0.1\n",
    "        self.val_ratio = 0.1\n",
    "        #self.val_ratio = 0.0020\n",
    "        #self.test_ratio = 0.0020\n",
    "        \n",
    "        # 学習に使うデバイス\n",
    "        #self.device = 'cuda'\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = 'cpu'\n",
    "        \n",
    "        # データローダーに使うCPUプロセスの数\n",
    "        #self.num_workers = 4\n",
    "        #self.num_workers = 0 if self.device == torch.device('cpu') else 10\n",
    "        #self.num_workers = 0 if self.device == torch.device('cpu') else 4\n",
    "        self.num_workers = 0\n",
    "        \n",
    "        # 移動平均で計算する損失の値の数\n",
    "        self.moving_avg = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--NNWCwZI5qS"
   },
   "source": [
    "### 学習を行う関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "config = ConfigTrain()\n",
    "\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "a_token_id = tokenizer.encode( \"a\" )[1]\n",
    "the_token_id = tokenizer.encode( \"the\" )[1]\n",
    "and_token_id = tokenizer.encode( \"and\" )[1]\n",
    "in_token_id = tokenizer.encode( \"in\" )[1]\n",
    "we_token_id = tokenizer.encode( \"we\" )[1]\n",
    "i_token_id = tokenizer.encode( \"i\" )[1]\n",
    "he_token_id = tokenizer.encode( \"he\" )[1]\n",
    "she_token_id = tokenizer.encode( \"she\" )[1]\n",
    "it_token_id = tokenizer.encode( \"it\" )[1]\n",
    "they_token_id = tokenizer.encode( \"they\" )[1]\n",
    "period_token_id = tokenizer.encode( \".\" )[1]\n",
    "comma_token_id = tokenizer.encode( \",\" )[1]\n",
    "dbl_token_id = tokenizer.encode( '\"' )[1]\n",
    "sgl_token_id = tokenizer.encode( \"'\" )[1]\n",
    "\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "# モデル出力用のディレクトリを作成\n",
    "os.makedirs(config.save_directory, exist_ok=True)\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((336, 336)),\n",
    "    v2.AutoAugment(),\n",
    "    #v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    ## Coco データセット 2017 train の平均と標準偏差\n",
    "    #v2.Normalize((0.456,0.427,0.401),(0.224,0.219,0.231) )\n",
    "    # ImageNetデータセットの平均と標準偏差\n",
    "    #v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    # Clip Model の config から引用。\n",
    "    v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "transforms2 = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    #v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "# v7 データセット\n",
    "train_dataset = MyDataset( file_path=config.anno_file,\n",
    "                           img_directory = config.img_directory,\n",
    "                           transforms=transforms, transforms2 = transforms2,\n",
    "                           tokenizer=tokenizer, length_max = config.length_max)\n",
    "\n",
    "# Subset samplerの生成\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, config.test_ratio, config.val_ratio )\n",
    "    \n",
    "# 学習時にランダムにサンプルするためのサンプラー\n",
    "train_sampler = SubsetRandomSampler(train_set)\n",
    "\n",
    "# DataLoaderを生成\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id, config.length_max)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=train_sampler,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=val_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    #batch_size=config.batch_size,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=test_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print( \"config.device:\", config.device )\n",
    "print( \"学習セット数:\",len( train_loader ) )\n",
    "print( \"評価セット数:\",len( val_loader ))\n",
    "print( \"テストセット数:\",len( test_loader ))\n",
    "print( \"use_amp:\", config.use_amp )\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "\n",
    "# モデルの定義\n",
    "model = CaptioningTransformer( config.img_size,\n",
    "    config.dim_embedding, config.length_max, config.vocab_size,\n",
    "    tokenizer, config.dropout, pad_token_id = tokenizer.pad_token_id,\n",
    "    use_repeat_logits_half = config.use_repeat_logits_half,\n",
    "    crf_coef = config.crf_coef, temp=config.temp, num_samples=config.num_samples )\n",
    "model.to(config.device)\n",
    "\n",
    "ref_model = CaptioningTransformer( config.img_size,\n",
    "    config.dim_embedding, config.length_max, config.vocab_size,\n",
    "    tokenizer, config.dropout, pad_token_id = tokenizer.pad_token_id,\n",
    "    use_repeat_logits_half = config.use_repeat_logits_half,\n",
    "    crf_coef = config.crf_coef, temp=config.temp, num_samples=config.num_samples, ref_t = True )\n",
    "ref_model.to(config.device)\n",
    "\n",
    "compute_reward = ComputeReward(reward_t = config.reward_t, decode_t = config.decode_t, device = config.device, \n",
    "                               repeat_thresh = config.repeat_thresh, \n",
    "                          repeat_weight = config.repeat_weight, cider_coef = config.cider_coef, rouge_coef = config.rouge_coef, \n",
    "                          clip_coef = config.clip_coef, bert_coef = config.bert_coef, use_amp = config.use_amp )\n",
    "\n",
    "\n",
    "# 最適化手法の定義\n",
    "# Optimizerの生成, clipとそうでないモジュールとの\n",
    "# パラメータで異なる学習率を適用\n",
    "#params_clip = []\n",
    "params_con = []\n",
    "params_bert = []\n",
    "params_others = []\n",
    "params_cri = []\n",
    "for name, parameter in model.named_parameters():\n",
    "    if parameter.requires_grad:\n",
    "        if 'clip_model' in name:\n",
    "            #params_clip.append(parameter)\n",
    "            parameter.requires_grad = False\n",
    "        elif 'connector' in name:\n",
    "            params_con.append(parameter)\n",
    "        elif 'bert' in name and 'critical' not in name:\n",
    "            params_bert.append(parameter)\n",
    "        elif 'critical' in name:\n",
    "            params_cri.append(parameter)\n",
    "        else:\n",
    "            params_others.append(parameter)\n",
    "param_groups = [\n",
    "    #{'params': params_clip, 'lr': config.lr_clip},\n",
    "    {'params': params_con, 'lr': config.lr_con},\n",
    "    {'params': params_bert, 'lr': config.lr_bert},\n",
    "    {'params': params_cri, 'lr': config.lr_cri},\n",
    "    {'params': params_others, 'lr': config.lr_others}]\n",
    "\n",
    "optimizer = torch.optim.AdamW( param_groups, weight_decay = config.weight_decay, betas=config.betas )\n",
    "thresh_groups = {\n",
    "#    'clip': config.clip_thresh_clip,\n",
    "    'con': config.clip_thresh_con,\n",
    "    'bert': config.clip_thresh_bert,\n",
    "    'cri': config.clip_thresh_cri,\n",
    "    'others':config.clip_thresh_others\n",
    "}\n",
    "\n",
    "# 全ステップ数\n",
    "num_global_steps = len( train_loader ) * config.num_epochs\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "print( \"warmup:\", config.warmup )\n",
    "num_warmup_steps = num_global_steps * config.warmup\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps )   \n",
    "\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "print( \"PATH:\", config.PATH )\n",
    "print( \"exist saved_pth:\", os.path.isfile(config.PATH) ) \n",
    "use_saved_pth = config.use_saved_pth\n",
    "if use_saved_pth and os.path.isfile(config.PATH):\n",
    "    checkpoint = torch.load(config.PATH, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict = False)\n",
    "    print( \"model parameters were loaded\")\n",
    "    ref_model.load_state_dict(checkpoint['model_state_dict'], strict = False)\n",
    "    ref_model.eval() # 必須：DropoutやBatchNormを無効化\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False # 必須：メモリ節約と誤学習防止\n",
    "    ref_model = ref_model.to(config.device )\n",
    "    print( \"ref_model parameters were loaded\")\n",
    "\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "    #for state in optimizer.state.values():\n",
    "        #for k, v in state.items():\n",
    "            #if isinstance(v, torch.Tensor):\n",
    "                #state[k] = v.to(device)\n",
    "    #begin_epoch = checkpoint['epoch']\n",
    "    #loss = checkpoint['loss']\n",
    "    #global_step = checkpoint['global_step']    \n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "else:\n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "\n",
    "file_param = 10\n",
    "print( \"begin_epoch:\", begin_epoch )\n",
    "print( \"global_step:\", global_step )\n",
    "print( \"file_param:\", file_param )\n",
    "\n",
    "def get_nearest_multiple(a, b):\n",
    "    \"\"\"\n",
    "    a に最も近い b の倍数を求める\n",
    "    \"\"\"\n",
    "    # a/b を四捨五入して、それに b を掛ける\n",
    "    # round() は .5 の場合、偶数側に丸める性質があるため、\n",
    "    # 厳密な四捨五入が必要な場合は整数演算を使用する\n",
    "    return round(a / b) * b\n",
    "\n",
    "len_tr_loader = len( train_loader )\n",
    "#train_param = len_tr_loader // 30\n",
    "train_param = len_tr_loader // 100\n",
    "#train_param = len_tr_loader // 3000\n",
    "len_val_loader = len( val_loader )\n",
    "#train_param = len_val_loader // 3\n",
    "train_param = get_nearest_multiple( train_param, file_param )\n",
    "#train_param = 1\n",
    "val_param = len_val_loader // 3\n",
    "print( \"train_param:\", train_param )\n",
    "print( \"val_param:\", val_param )\n",
    "\n",
    "print( \"epochs:\", config.num_epochs )\n",
    "print( \"batch_size:\", config.batch_size )\n",
    "print( \"lr_clip:\", config.lr_clip )\n",
    "print( \"lr_con:\", config.lr_con )\n",
    "print( \"lr_bert:\", config.lr_bert )\n",
    "print( \"lr_cri:\", config.lr_cri )\n",
    "print( \"lr_others:\", config.lr_others )\n",
    "if config.clip_grad_threshold == 0.0:\n",
    "    print( 'clip_thresh_con:', config.clip_thresh_con )\n",
    "    print( 'clip_thresh_bert:', config.clip_thresh_bert )\n",
    "    print( 'clip_thresh_cri:', config.clip_thresh_cri )\n",
    "    print( 'clip_thresh_others:',config.clip_thresh_others )\n",
    "print( \"weight_decay:\", config.weight_decay )\n",
    "print( \"betas:\", config.betas )\n",
    "print( \"reward_type:\", config.reward_t )\n",
    "print( \"decode_type:\", config.decode_t )\n",
    "print( \"clip_range ppo clip:\", config.clip_range )\n",
    "print( \"clip_grad_threshold gradient norm:\", config.clip_grad_threshold)\n",
    "print( \"ord_coef:\", config.ord_coef )\n",
    "print( \"cider_coef:\", config.cider_coef )\n",
    "print( \"rouge_coef:\", config.rouge_coef )\n",
    "print( \"clip_coef:\", config.clip_coef )\n",
    "print( \"rep_coef:\", config.rep_coef )\n",
    "print( \"repeat_thresh:\", config.repeat_thresh )\n",
    "print( \"repeat_weight:\", config.repeat_weight )\n",
    "print( \"len_coef:\", config.len_coef )\n",
    "print( \"unr_coef:\", config.unr_coef )\n",
    "print( \"policy_coef:\", config.policy_coef )\n",
    "print( \"crf_coef:\", config.crf_coef )\n",
    "print( \"ce_coef:\", config.ce_coef )\n",
    "print( \"ent_coef:\", config.ent_coef )\n",
    "#print( \"cri_coef:\", config.cri_coef )\n",
    "print( \"gae_coef:\", config.gae_coef )\n",
    "print( \"kl_coef:\", config.kl_coef )\n",
    "print( \"target_kl:\", config.target_kl )\n",
    "print( \"buffer_kl:\", config.buffer_kl )\n",
    "print( \"kl_max:\", config.kl_max )\n",
    "print( \"kl_min:\", config.kl_min )\n",
    "print( \"gamma:\", config.gamma )\n",
    "print( \"lambda:\", config.lam )\n",
    "print( \"use_repeat_logits_half:\", config.use_repeat_logits_half )\n",
    "print( \"use_ce_bert:\", config.use_ce_bert )\n",
    "print( \"display_include_coef:\", config.display_include_coef )\n",
    "print( \"temp:\", config.temp )\n",
    "print( \"num_samples:\", config.num_samples )\n",
    "print( \"residual_samples:\", config.residual_samples )\n",
    "\n",
    "# 学習経過の書き込み\n",
    "now = datetime.datetime.now()\n",
    "train_loss_file = '{}/MyOriginal_train_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(train_loss_file, 'a') as f:\n",
    "    print(f'{len_tr_loader}', file=f) \n",
    "print( \"train_loss_file:\", train_loss_file )\n",
    "val_loss_file = '{}/MyOriginal_val_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(val_loss_file, 'a') as f:\n",
    "    print(f'{len_val_loader}', file=f) \n",
    "norm_file = '{}/norm_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "# 学習\n",
    "val_loss_best = float('inf')\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# AMP用のスケーラー\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "eps = 1e-8\n",
    "last_sample_log_probs = 0\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "def entropy_func(probs):\n",
    "    input_probs = torch.clamp( probs, eps )\n",
    "    log_probs = torch.log( input_probs )\n",
    "    p_log_p = probs * log_probs\n",
    "    return - p_log_p.sum(-1)\n",
    "\n",
    "def baseline( probs, targets, top_k ):\n",
    "    probs_k, preds_k = torch.topk( probs, dim = 2, k = top_k )\n",
    "    renorm_probs_k = probs_k / torch.sum( probs_k, dim = 2 )[:,:, None]\n",
    "    base_ = torch.stack( [ probs_k[:,:,k] * compue_reward._compute_reward(preds_k[:,:,k], targets, sources = None ) \\\n",
    "                        for k in range( top_k ) ], dim = 0 )\n",
    "    base = torch.sum( base_, dim = 0 )\n",
    "    \n",
    "    return base\n",
    "\n",
    "def custom_gradient_clipping(clip_params, gpt2_params, cri_params, others_params, \n",
    "                             clip_threshold, gpt2_threshold, cri_threshold, others_threshold):\n",
    "    # エンコーダーの勾配クリッピング\n",
    "    if clip_params:\n",
    "        # torch.nn.utils.clip_grad_norm_ は、与えられたパラメータのリストに勾配クリッピングを適用する\n",
    "        torch.nn.utils.clip_grad_norm_(clip_params, clip_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if gpt2_params:\n",
    "        torch.nn.utils.clip_grad_norm_(gpt2_params, gpt2_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if cri_params:\n",
    "        torch.nn.utils.clip_grad_norm_(cri_params, cri_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if gpt2_params:\n",
    "        torch.nn.utils.clip_grad_norm_(others_params, others_threshold)\n",
    "\n",
    "def my_index( list1, target ):\n",
    "    if target in list1:\n",
    "        return list1.index( target )\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    with tqdm(train_loader) as pbar:\n",
    "    #with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[エポック {epoch + 1}]')\n",
    "\n",
    "        # 学習モードに設定\n",
    "        model.train()\n",
    "\n",
    "        train_losses = deque()\n",
    "        train_policys = deque()\n",
    "        train_entropies = deque()\n",
    "        train_critics = deque()\n",
    "        train_kl_divs = deque()\n",
    "        train_rewards = deque()\n",
    "        train_rewards2 = deque()\n",
    "        train_ord = deque()\n",
    "        train_repeat = deque()\n",
    "        train_length = deque()\n",
    "        train_adv = deque()\n",
    "        train_errors = deque()\n",
    "        train_bleus = deque()\n",
    "        train_crfs = deque()\n",
    "        train_ces = deque()\n",
    "        train_clips = deque()\n",
    "        train_unrs = deque()\n",
    "        train_berts = deque()\n",
    "\n",
    "        start = time.time()\n",
    "        for n_batch, (imgs, imgs2, captions, caption_lengths) in enumerate( pbar ):\n",
    "            #print( \"captions[0]:\", captions[0] )\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            imgs2 = imgs2.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "            \n",
    "            if imgs.size(0) != config.batch_size:\n",
    "                print( f\"bsz {imgs.size(0)} is not batch_size {config.batch_size}. skip\")\n",
    "                continue\n",
    "\n",
    "            #if imgs.size(0) != config.batch_size:\n",
    "            #    print( f\"bsz {imgs.size(0)} is not batch_size {config.batch_size}. skip\")\n",
    "            #    continue\n",
    "            \n",
    "            #optimizer.zero_grad()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            #start_time0 = time.time()\n",
    "            \n",
    "            # 最後の単語から次を予測する必要はないため最後の単語を除外\n",
    "            with autocast(str(config.device),enabled=config.use_amp):\n",
    "                #start_time = time.time()\n",
    "                finalized_scores, finalized_tokens, top_probss, top_indices, \\\n",
    "                crf_loss, bert_logits, sampled_beam_idxs, b_finalized_tokens  = \\\n",
    "                model( imgs, captions, top_indices = None )\n",
    "                #end_time = time.time()\n",
    "                #print( \"model time:\", end_time - start_time )\n",
    "                #preds = finalized_tokens\n",
    "                bsz, seq_len, beam, N = top_probss.size()\n",
    "                top_probs = top_probss[:,:,:,0]\n",
    "                sampled_beam_idx = sampled_beam_idxs[:,:,0].unsqueeze( -1 ) \n",
    "                if config.use_ce_bert == False:\n",
    "                    ce_tensor = torch.full((bsz, seq_len, vocab_size), float(eps), device=config.device)\n",
    "                    ce_tensor = torch.scatter( ce_tensor, 2, top_indices, top_probs )\n",
    "                    ce_tensor = torch.clamp( ce_tensor, eps )\n",
    "                    log_ce_tensor = torch.log( ce_tensor )\n",
    "                hypo_ids = finalized_tokens\n",
    "                with torch.no_grad():\n",
    "                    preds = b_finalized_tokens[:,:,0]\n",
    "                    b2_finalized_tokens = b_finalized_tokens[:,:,:].permute( 2, 0, 1 ).reshape( N * bsz, seq_len )\n",
    "                    b2_imgs2 = imgs2[None].expand( N, -1, -1, -1, -1 ).reshape( N * bsz, 3, 224, 224 )\n",
    "                    b2_captions = captions[None].expand(N,bsz,seq_len).reshape( N * bsz, seq_len)\n",
    "                    b_ord, b_ord2, b_repeat, b_length, b_unr \\\n",
    "                        = compute_reward( b2_finalized_tokens, b2_captions, b2_imgs2 )\n",
    "                    b_ord = b_ord.view( N, bsz, seq_len )\n",
    "                    b_ord2 = b_ord2.view( N, bsz, seq_len )\n",
    "                    b_repeat = b_repeat.view( N , bsz, seq_len )\n",
    "                    b_length = b_length.view( N, bsz, seq_len )\n",
    "                    b_unr = b_unr.view( N , bsz, seq_len ) \n",
    "                with torch.no_grad():\n",
    "                    ref_captions = preds\n",
    "                    #start_time = time.time()\n",
    "                    ref_top_probs = ref_model( imgs, ref_captions, top_indices = top_indices )\n",
    "                    #end_time = time.time()\n",
    "                    #print( \"ref_model time:\", end_time - start_time )\n",
    "                    \n",
    "                # 1. Policy側の対数確率（学習対象）\n",
    "                tmp = torch.clamp( top_probs, min = eps )\n",
    "                top_log_probs = torch.log( tmp )\n",
    "                policy_lp = torch.gather(top_log_probs, -1, sampled_beam_idx).squeeze(-1) # lp は log_prob の略と思われる。 bsz * seq_len\n",
    "                \n",
    "                # 2. Reference側の対数確率（固定）\n",
    "                tmp = torch.clamp( ref_top_probs, min = eps )\n",
    "                ref_top_log_probs = torch.log(tmp)\n",
    "                ref_lp = torch.gather(ref_top_log_probs, -1, sampled_beam_idx).squeeze(-1)\n",
    "                \n",
    "                if config.decode_t == \"no-pad\":\n",
    "                    lengths = []\n",
    "                    for pred in preds:\n",
    "                        length = my_index( pred.tolist(), eos_token_id )\n",
    "                        if length != -1:\n",
    "                            lengths.append( length )\n",
    "                        else:\n",
    "                            lengths.append( 0 )\n",
    "                    lengths = torch.tensor( lengths, device = config.device )[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "                    arange1 = torch.arange( 0, preds.size(1), device = config.device )\n",
    "                    arange1 = arange1[None,:].expand( preds.size(0), -1 )\n",
    "                    \n",
    "                    masks = arange1 < ( lengths + 2 )\n",
    "                \n",
    "                    kl_divs = ( policy_lp - ref_lp ) * masks.float()\n",
    "                else:\n",
    "                    kl_divs = policy_lp - ref_lp \n",
    "                \n",
    "                b_rewards = config.ord_coef * b_ord + config.rep_coef * b_repeat + config.len_coef * b_length \\\n",
    "                    + config.unr_coef * b_unr\n",
    "                rewards = torch.mean( b_rewards, dim = 0 )\n",
    "                rewards2 = torch.mean( (b_ord2 + b_repeat + b_length + b_unr ), dim = 0 )\n",
    "\n",
    "                \n",
    "                tmp = torch.clamp( top_probss, eps )\n",
    "                top_log_probss = torch.log( tmp )\n",
    "                sample_log_probs = torch.gather( top_log_probss, 2, sampled_beam_idxs[:,:,None,:] )[:,:,0,:]\n",
    "                sample_log_probs = sample_log_probs.permute(2,0,1)\n",
    "                if global_step == 0:\n",
    "                    last_sample_log_probs = sample_log_probs.detach()\n",
    "\n",
    "                  \n",
    "                # 1. Advantageの計算とTop-K選別\n",
    "                with torch.no_grad():\n",
    "                    mean = b_rewards.mean(dim=0, keepdim=True)\n",
    "                    std = b_rewards.std(dim=0, keepdim=True)\n",
    "                    advantages_norm = (b_rewards - mean) / (std + eps)\n",
    "\n",
    "                \n",
    "                ## 2. 確率比 (Importance Ratio) の計算\n",
    "                ## r_t(θ) = π_θ(a|s) / π_old(a|s)\n",
    "                _, sample_idx = torch.topk( torch.abs( advantages_norm ), config.residual_samples, dim = 0 ) # 3, b, seq_len\n",
    "                topk_advantages_norm = torch.gather( advantages_norm, 0, sample_idx )\n",
    "                ratio = torch.exp(sample_log_probs - last_sample_log_probs)\n",
    "                topk_ratio = torch.gather( ratio, 0, sample_idx) # re_sample, b, seq_len\n",
    "\n",
    "                # 2. 通常の目的関数とクリップされた目的関数\n",
    "                surr1 = topk_ratio * topk_advantages_norm\n",
    "                surr2 = torch.clamp(topk_ratio, 1.0 - config.clip_range, 1.0 + config.clip_range) * topk_advantages_norm\n",
    "                \n",
    "                # 3. 損失の計算 (最大化したいのでマイナスを付ける)\n",
    "                # GRPOの論文式に基づき、min(surr1, surr2) を最大化する\n",
    "                loss_per_sample = -torch.min(surr1, surr2)\n",
    "\n",
    "                # 4. 最終的な Policy Loss\n",
    "                policy_loss = loss_per_sample.mean()\n",
    "\n",
    "                entropy = entropy_func(top_probs)\n",
    "                entropy_loss = - torch.mean(entropy)\n",
    "\n",
    "                kl_per_sample = torch.sum(kl_divs, dim=1) / (torch.sum(masks, dim=1) + 1e-8)\n",
    "                kl_div_loss = torch.mean(kl_per_sample)\n",
    "                if config.use_adaptive_KL:\n",
    "                    if n_batch % 100 == 0:\n",
    "                        if kl_div_loss < config.target_kl / config.buffer_kl:\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                            print(f\"Update kl (low KL): {config.kl_coef} -> {config.kl_coef / 2.0}\")\n",
    "                            config.kl_coef = max(config.kl_coef / 2.0, config.kl_min) # 下限 0.05\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                        elif kl_div_loss > config.target_kl * config.buffer_kl:\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                            print(f\"Update kl (high KL): {config.kl_coef} -> {config.kl_coef * 2.0}\")\n",
    "                            config.kl_coef = min(config.kl_coef * 2.0, config.kl_max)  # 上限 5.0\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                \n",
    "                loss =  config.policy_coef * policy_loss + config.ent_coef * entropy_loss + config.kl_coef * kl_div_loss\n",
    "\n",
    "                if config.crf_coef != 0.0:\n",
    "                    loss =  loss + config.crf_coef * crf_loss\n",
    "                \n",
    "                if config.ce_coef != 0.0:\n",
    "                    if config.use_ce_bert:\n",
    "                        ce_loss = nn.CrossEntropyLoss()( bert_logits.transpose(1,2), captions )\n",
    "                    else:\n",
    "                        ce_loss = nn.NLLLoss()( log_ce_tensor.view( bsz * seq_len, -1 ), captions.view( bsz * seq_len ) )\n",
    "                    loss =  loss + config.ce_coef * ce_loss\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    last_sample_log_probs = sample_log_probs.detach()                    \n",
    "\n",
    "            #start_time = time.time()\n",
    "            torch.cuda.synchronize()\n",
    "            scaler.scale(loss).backward()\n",
    "            #end_time = time.time()\n",
    "            #print( \"backward time:\", end_time - start_time )\n",
    "            loss_item = loss.item()\n",
    "            reward_item = rewards.mean().item()\n",
    "            reward2_item = rewards2.mean().item()\n",
    "            adv_item = advantages_norm.mean().item()\n",
    "            if config.display_include_coef:\n",
    "                policy_item = config.policy_coef * policy_loss.item()\n",
    "                entropy_item = config.ent_coef * entropy_loss.item()\n",
    "                #critic_item = config.gae_coef * gae_loss.item()\n",
    "                critic_item = 0\n",
    "                kl_div_item = config.kl_coef * kl_div_loss.item()\n",
    "                ord_item = config.ord_coef * b_ord.mean().item()\n",
    "                repeat_item = config.rep_coef * b_repeat.mean().item()\n",
    "                length_item = config.len_coef * b_length.mean().item()\n",
    "                if config.crf_coef != 0.0:\n",
    "                    crf_item = config.crf_coef * crf_loss.mean().item()\n",
    "                else:\n",
    "                    crf_item = 0.0\n",
    "                if config.ce_coef != 0.0:\n",
    "                    ce_item = config.ce_coef * ce_loss.mean().item()\n",
    "                else:\n",
    "                    ce_item = 0.0\n",
    "                unr_item = config.unr_coef * b_unr.mean().item()\n",
    "            else:\n",
    "                policy_item = policy_loss.item()\n",
    "                entropy_item = entropy_loss.item()\n",
    "                #critic_item = gae_loss.item()\n",
    "                critic_item = 0\n",
    "                kl_div_item = kl_div_loss.item()\n",
    "                ord_item = b_ord.mean().item()\n",
    "                repeat_item = b_repeat.mean().item()\n",
    "                length_item = b_length.mean().item()\n",
    "                if config.crf_coef != 0.0:\n",
    "                    crf_item = crf_loss.mean().item()\n",
    "                else:\n",
    "                    crf_item = 0.0\n",
    "                if config.ce_coef != 0.0:\n",
    "                    ce_item = ce_loss.mean().item()\n",
    "                else:\n",
    "                    ce_item = 0.0\n",
    "                unr_item = b_unr.mean().item()\n",
    "            del loss, rewards, b_ord, b_repeat, b_length\n",
    "            del policy_loss, entropy_loss, kl_div_loss, topk_advantages_norm, #mean_advantages_norm\n",
    "            del sample_log_probs, advantages_norm, ratio, topk_ratio, surr1, surr2\n",
    "            del finalized_scores, finalized_tokens\n",
    "            del kl_divs, masks, kl_per_sample\n",
    "            torch.cuda.empty_cache()\n",
    "            scaler.unscale_(optimizer)\n",
    "            if config.clip_grad_threshold != 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(\\\n",
    "                   model.parameters(),\n",
    "                   config.clip_grad_threshold)\n",
    "            else:\n",
    "                custom_gradient_clipping(\n",
    "                    #params_clip, \n",
    "                    params_con, \n",
    "                    params_bert,\n",
    "                    params_cri,\n",
    "                    params_others,\n",
    "                    #thresh_groups['clip'], \n",
    "                    thresh_groups['con'], \n",
    "                    thresh_groups['bert'], \n",
    "                    thresh_groups['cri'], \n",
    "                    thresh_groups['others'], \n",
    "                )\n",
    "          \n",
    "            # オプティマイザにより，パラメータを更新する\n",
    "            #for i, params in enumerate(model.parameters()):\n",
    "            #    params.grad = grad[i]\n",
    "            #print( \"model parameters i:\",i )\n",
    "            #norm0 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[0].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            #norm1 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[12].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            #norm0 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[0].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            #norm1 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[12].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            norm0 = 0\n",
    "            norm1 = 0\n",
    "            norm2 = torch.norm( model.bert.encoder.layer[11].attention.self.query.weight.grad, p = 2 ).item()\n",
    "            norm3 = torch.norm( model.bert.encoder.layer[23].attention.self.query.weight.grad, p = 2 ).item()\n",
    "            norm_mean = torch.mean( torch.stack ( [ torch.norm( param.grad, p = 2 ) \\\n",
    "                                                  for param in model.parameters() if param.grad is not None ] , dim = 0 ) ).item()\n",
    "            #total_norm = torch.nn.utils.clip_grad_norm_(params_bert, thresh_groups['bert']).item()\n",
    "            #print( norm0, norm1, norm2, norm3, norm_mean )\n",
    "            with open(norm_file, 'a') as f:\n",
    "                print( \"epcoch:\", epoch, \", step:\", global_step, \", norm0:\", norm0, \", norm1:\", norm1, \", norm2:\", norm2, \\\n",
    "                       \", norm3:\", norm3, \", norm_mean:\", norm_mean, file=f  )\n",
    "                f.flush()\n",
    "\n",
    "            #optimizer.step()\n",
    "            #start_time = time.time()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()            \n",
    "            scheduler.step()\n",
    "            #end_time = time.time()\n",
    "            #print( \"step time:\", end_time - start_time )\n",
    "            \n",
    "            if global_step % file_param == 0:\n",
    "                #start_time = time.time()\n",
    "                #start_time = time.time()\n",
    "                hypo_sentence1 = []\n",
    "                ref_sentence1 = []\n",
    "                if config.decode_t == 'no-endoftext':\n",
    "                    preds_str = [tokenizer.decode(\n",
    "                        [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                        ) for pred in hypo_ids]\n",
    "                    samps_str = [tokenizer.decode(\n",
    "                        [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                        ) for pred in preds]\n",
    "                    targets_str = [tokenizer.decode(\n",
    "                        [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                        ) for target in captions]\n",
    "                elif config.decode_t == 'no-pad':\n",
    "                    hypo_ids1 = copy.deepcopy(hypo_ids)  # deep copy\n",
    "                    captions1 = copy.deepcopy(captions)\n",
    "                    preds1 = copy.deepcopy(preds)\n",
    "                    hypo_ids1[hypo_ids1 == eos_token_id] = pad_token_id\n",
    "                    decoded = tokenizer.batch_decode(hypo_ids1, skip_special_tokens=False)\n",
    "                    preds_str = [s.replace(\"[PAD]\", \"\").replace(\"[unused1]\", \"\").strip() for s in decoded]            \n",
    "                    preds_str2 = tokenizer.batch_decode(hypo_ids1, skip_special_tokens=True)\n",
    "                    captions1[captions1 == eos_token_id] = pad_token_id\n",
    "                    decoded = tokenizer.batch_decode(captions1, skip_special_tokens=False)\n",
    "                    targets_str = [s.replace(\"[PAD]\", \"\").replace(\"[unused1]\", \"\").strip() for s in decoded]   \n",
    "                    preds1[preds1 == eos_token_id] = pad_token_id\n",
    "                    decoded = tokenizer.batch_decode(preds1, skip_special_tokens=False)\n",
    "                    samps_str = [s.replace(\"[PAD]\", \"\").replace(\"[unused1]\", \"\").strip() for s in decoded]   \n",
    "                else:\n",
    "                    preds_str = [tokenizer.decode(pred) for pred in hypo_ids]\n",
    "                    targets_str = [tokenizer.decode(target) for target in captions]\n",
    "                pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "                target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "                with torch.no_grad():\n",
    "                    avg_bleu, scores = compute_reward.scorer.compute_score(target_dict, pred_dict) # cider の計算\n",
    "                    rouge_scores = [compute_reward.rougeL.score(target, pred)['rougeL'][0] for pred, target in zip(preds_str, targets_str)]\n",
    "                    avg_error = sum( rouge_scores ) / len( rouge_scores )\n",
    "                with autocast(str(config.device),enabled=config.use_amp):\n",
    "                    with torch.no_grad():\n",
    "                        #reward_clip = compute_reward_with_metrics( preds_str2, imgs2 )\n",
    "                        processed = compute_reward.metric.processor(text=preds_str2, images=imgs2, return_tensors=\"pt\", padding=True, \\\n",
    "                                                          truncation=True, max_length=77, do_resize=False, do_rescale=False ).to(config.device)\n",
    "                        outputs = compute_reward.metric.model(**processed)\n",
    "                        # 特徴量の正規化\n",
    "                        image_features = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                        text_features = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                        individual_scores = torch.clamp( (image_features.to(config.device) * \\\n",
    "                                                          text_features.to(config.device)).sum(axis=-1), min=0)\n",
    "                        clip_scores = individual_scores[:,None].expand( -1, seq_len )\n",
    "                        clip_score = clip_scores.mean()\n",
    "                        #bert_scores = compute_reward.bert.compute(predictions=preds_str, references=targets_str, model_type=model_name, \\\n",
    "                        #                                  lang='en',  device=config.device)['f1']\n",
    "                        model_name = 'distilbert-base-uncased' \n",
    "                        bert_scores = compute_reward.bert.compute(\n",
    "                            predictions=preds_str, \n",
    "                            references=targets_str,\n",
    "                            model_type=model_name,\n",
    "                            use_fast_tokenizer=True, \n",
    "                            lang='en', \n",
    "                            device=config.device,\n",
    "                            batch_size=config.batch_size,  # メモリ許容範囲で大きく設定\n",
    "                            rescale_with_baseline=False\n",
    "                            )['f1']\n",
    "                        bert_score = sum( bert_scores ) / len( bert_scores )\n",
    "                if  global_step % train_param == 0:\n",
    "                    hypo_sentence1 = [preds_str[0]]\n",
    "                    samp_sentence1 = [samps_str[0]]\n",
    "                    ref_sentence1 = [targets_str[0]]\n",
    "\n",
    "                # 学習時の損失をログに書き込み\n",
    "                #エポック内の平均\n",
    "                train_losses.append(loss_item)\n",
    "                train_policys.append(policy_item)\n",
    "                train_entropies.append(entropy_item)\n",
    "                train_critics.append(critic_item)\n",
    "                train_kl_divs.append(kl_div_item)\n",
    "                train_rewards.append(reward_item)\n",
    "                train_rewards2.append(reward2_item)\n",
    "                train_ord.append(ord_item)\n",
    "                train_repeat.append(repeat_item)\n",
    "                train_length.append(length_item)\n",
    "                train_adv.append(adv_item)\n",
    "                train_errors.append( avg_error )\n",
    "                train_bleus.append( avg_bleu )\n",
    "                train_crfs.append( crf_item )\n",
    "                train_ces.append( ce_item )\n",
    "                train_clips.append( clip_score )\n",
    "                train_unrs.append( unr_item )\n",
    "                train_berts.append( bert_score )\n",
    "                if len(train_losses) > config.moving_avg:\n",
    "                    train_losses.popleft()\n",
    "                    train_policys.popleft()\n",
    "                    train_entropies.popleft()\n",
    "                    train_critics.popleft()\n",
    "                    train_kl_divs.popleft()\n",
    "                    train_rewards.popleft()\n",
    "                    train_rewards2.popleft()\n",
    "                    train_ord.popleft()\n",
    "                    train_repeat.popleft()\n",
    "                    train_length.popleft()\n",
    "                    train_adv.popleft()\n",
    "                    train_errors.popleft()\n",
    "                    train_bleus.popleft()\n",
    "                    train_crfs.popleft()\n",
    "                    train_ces.popleft()\n",
    "                    train_clips.popleft()\n",
    "                    train_unrs.popleft()\n",
    "                    train_berts.popleft()\n",
    "                mean_loss = torch.Tensor(train_losses).mean().item()\n",
    "                mean_policy = torch.Tensor(train_policys).mean().item()\n",
    "                mean_entropy = torch.Tensor(train_entropies).mean().item()\n",
    "                mean_critic = torch.Tensor(train_critics).mean().item()\n",
    "                mean_kl_div = torch.Tensor(train_kl_divs).mean().item()\n",
    "                mean_reward = torch.Tensor(train_rewards).mean().item()\n",
    "                mean_reward2 = torch.Tensor(train_rewards2).mean().item()\n",
    "                mean_ord = torch.Tensor(train_ord).mean().item()\n",
    "                mean_repeat = torch.Tensor(train_repeat).mean().item()\n",
    "                mean_length = torch.Tensor(train_length).mean().item()\n",
    "                mean_adv = torch.Tensor(train_adv).mean().item()\n",
    "                mean_error = torch.Tensor(train_errors).mean().item()\n",
    "                mean_bleu = torch.Tensor(train_bleus).mean().item()\n",
    "                mean_crf = torch.Tensor(train_crfs).mean().item()\n",
    "                mean_ce = torch.Tensor(train_ces).mean().item()\n",
    "                mean_clip = torch.Tensor(train_clips).mean().item()\n",
    "                mean_unr = torch.Tensor(train_unrs).mean().item()\n",
    "                mean_bert = torch.Tensor(train_berts).mean().item()\n",
    "                #print( \"mean_reward2:\", mean_reward2 ) \n",
    "                pbar.set_postfix({\n",
    "                    'loss': mean_loss,\n",
    "                    'policy': mean_policy,\n",
    "                    'entropy': mean_entropy,\n",
    "                    'gae': mean_critic,\n",
    "                    'kl_div': mean_kl_div,\n",
    "                    'reward': mean_reward,\n",
    "                    'reward2': mean_reward2,\n",
    "                    'ord': mean_ord,\n",
    "                    'repeat': mean_repeat,\n",
    "                    'length': mean_length,\n",
    "                    'adv': mean_adv,\n",
    "                    'rougeL': mean_error,\n",
    "                    'cider': mean_bleu,\n",
    "                    'crf': mean_crf,\n",
    "                    'ce': mean_ce,\n",
    "                    'clip': mean_clip,\n",
    "                    'unr': mean_unr,\n",
    "                    'bert': mean_bert,\n",
    "                })\n",
    "                with open(train_loss_file, 'a') as f:\n",
    "                    print(f' {global_step}, {mean_loss}, {mean_policy}, {mean_entropy}, {mean_critic}, {mean_kl_div}, {mean_reward}, ' \\\n",
    "                          f'{mean_ord}, {mean_repeat}, {mean_length}, {mean_adv}, {mean_error}, {mean_bleu}, {mean_crf}, {mean_ce}, '\\\n",
    "                          f'{mean_clip}, {mean_unr}, {mean_bert}', file=f)\n",
    "                print_flag = 1\n",
    "                for ( hypo_se, ref_se, samp_se ) in zip( hypo_sentence1, ref_sentence1, samp_sentence1 ):\n",
    "                    if print_flag == 1:\n",
    "                        print( \"lr con   :\", optimizer.param_groups[0][\"lr\"] )\n",
    "                        print( \"lr bert  :\", optimizer.param_groups[1][\"lr\"] )\n",
    "                        print( \"lr cri   :\", optimizer.param_groups[2][\"lr\"] )\n",
    "                        print( \"lr others:\", optimizer.param_groups[3][\"lr\"] )\n",
    "                        print_flag = 0\n",
    "                    print(f'Train epoch = {global_step/len_tr_loader}, loss = {mean_loss}, policy = {mean_policy}, '\\\n",
    "                          f'entropy_loss = {mean_entropy}, gae = {mean_critic}, kl_div = {mean_kl_div}, reward = {mean_reward}, '\\\n",
    "                          f'ord = {mean_ord}, repeat = {mean_repeat}, length = {mean_length}, adv = {mean_adv}, '\\\n",
    "                          f'rougeL = {mean_error}, cider = {mean_bleu}, clip = {mean_clip}, crf = {mean_crf}, ce = {mean_ce}, '\\\n",
    "                          f'unr = {mean_unr}, bert = {mean_bert}' )\n",
    "                    print( \"refe:\", ref_se )\n",
    "                    print( \"hypo:\", hypo_se )\n",
    "                    print( \"samp:\", samp_se )\n",
    "                #end_time = time.time()\n",
    "                #print( \"display time:\", end_time - start_time )\n",
    "            \n",
    "            global_step += 1\n",
    "            #end_time0 = time.time()\n",
    "            #print( \"all train time:\", end_time0 - start_time0 )\n",
    "        end = time.time()\n",
    "        print( \"time:\",end - start )\n",
    "        #print(prof.key_averages().table(sort_by = \"cuda_time\", row_limit = 30))\n",
    "        #print(prof.key_averages().table(sort_by = \"cpu_time\", row_limit = 30))\n",
    "    #各値を表示\n",
    "    print(f'Train loss: {mean_loss}')\n",
    "    print(f'Train policy: {mean_policy}')\n",
    "    print(f'Train entropy: {mean_entropy}')\n",
    "    print(f'Train gae: {mean_critic}')\n",
    "    print(f'Train kl_div: {mean_kl_div}')\n",
    "    print(f'Train reward: {mean_reward}')\n",
    "    print(f'Train reward2: {mean_reward2}')\n",
    "    print(f'Train ord: {mean_ord}')\n",
    "    print(f'Train repeat: {mean_repeat}')\n",
    "    print(f'Train pad: {mean_length}')\n",
    "    print(f'Train adv: {mean_adv}')\n",
    "    print(f'Train clip: {mean_clip}')\n",
    "    print(f'Train rougeL: {mean_error}')        \n",
    "    print(f'Train cider: {mean_bleu}')\n",
    "    print(f'Train crf: {mean_crf}')        \n",
    "    print(f'Train ce: {mean_ce}')\n",
    "    print(f'Train unr: {mean_unr}')        \n",
    "    print(f'Train bert: {mean_bert}')\n",
    "    \n",
    "    # 検証\n",
    "    with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[検証]')\n",
    "    \n",
    "        # 評価モード\n",
    "        model.eval()\n",
    "    \n",
    "        #val_losses = deque()\n",
    "        #val_rewards = deque()\n",
    "        val_errors = deque()\n",
    "        val_bleus = deque()\n",
    "        val_clips = deque()\n",
    "        val_berts = deque()\n",
    "        for n_batch, (imgs, imgs2, captions, caption_lengths) in enumerate( pbar ):\n",
    "    \n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            imgs2 = imgs2.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "                critical_value, crf_loss, bert_logits, sampled_beam_idx  = \\\n",
    "                model( imgs, captions, top_indices = None )\n",
    "                hypo_ids = finalized_tokens\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "               \n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            n2 = 0\n",
    "            if config.decode_t == 'no-endoftext':\n",
    "                preds_str = [tokenizer.decode(\n",
    "                    [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                    ) for pred in hypo_ids]\n",
    "                samps_str = [tokenizer.decode(\n",
    "                    [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                    ) for pred in preds]\n",
    "                targets_str = [tokenizer.decode(\n",
    "                    [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                    ) for target in captions]\n",
    "            elif config.decode_t == 'no-pad':\n",
    "                hypo_ids1 = copy.deepcopy(hypo_ids)  # deep copy\n",
    "                captions1 = copy.deepcopy(captions)\n",
    "                preds1 = copy.deepcopy(preds)\n",
    "                hypo_ids1[hypo_ids1 == eos_token_id] = pad_token_id\n",
    "                decoded = tokenizer.batch_decode(hypo_ids1, skip_special_tokens=False)\n",
    "                preds_str = [s.replace(\"[PAD]\", \"\").replace(\"[unused1]\", \"\").strip() for s in decoded]            \n",
    "                preds_str2 = tokenizer.batch_decode(hypo_ids1, skip_special_tokens=True)\n",
    "                captions1[captions1 == eos_token_id] = pad_token_id\n",
    "                decoded = tokenizer.batch_decode(captions1, skip_special_tokens=False)\n",
    "                targets_str = [s.replace(\"[PAD]\", \"\").replace(\"[unused1]\", \"\").strip() for s in decoded]   \n",
    "            else:\n",
    "                preds_str = [tokenizer.decode(pred) for pred in hypo_ids]\n",
    "                targets_str = [tokenizer.decode(target) for target in captions]\n",
    "            pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "            target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "            with torch.no_grad():\n",
    "                avg_bleu, scores = compute_reward.scorer.compute_score(target_dict, pred_dict) # cider の計算\n",
    "                rouge_scores = [compute_reward.rougeL.score(target, pred)['rougeL'][0] for pred, target in zip(preds_str, targets_str)]\n",
    "                avg_error = sum( rouge_scores ) / len( rouge_scores )\n",
    "            with autocast(str(config.device),enabled=config.use_amp):\n",
    "                with torch.no_grad():\n",
    "                    #reward_clip = compute_reward_with_metrics( preds_str2, imgs2 )\n",
    "                    processed = compute_reward.metric.processor(text=preds_str2, images=imgs2, return_tensors=\"pt\", padding=True, \\\n",
    "                                                      truncation=True, max_length=77, do_resize=False, do_rescale=False ).to(config.device)\n",
    "                    outputs = compute_reward.metric.model(**processed)\n",
    "                    # 特徴量の正規化\n",
    "                    image_features = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                    text_features = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                    individual_scores = torch.clamp( (image_features.to(config.device) * \\\n",
    "                                                      text_features.to(config.device)).sum(axis=-1), min=0)\n",
    "                    clip_scores = individual_scores[:,None].expand( -1, seq_len ) \n",
    "                    clip_score = clip_scores.mean()\n",
    "                    #bert_scores = compute_reward.bert.compute(predictions=preds_str, references=targets_str, model_type=model_name, \\\n",
    "                    #                                  lang='en',  device=config.device)['f1']\n",
    "                    bert_scores = compute_reward.bert.compute(\n",
    "                        predictions=preds_str, \n",
    "                        references=targets_str,\n",
    "                        model_type=model_name,\n",
    "                        use_fast_tokenizer=True, \n",
    "                        lang='en', \n",
    "                        device=config.device,\n",
    "                        batch_size=config.batch_size,  # メモリ許容範囲で大きく設定\n",
    "                        rescale_with_baseline=False\n",
    "                        )['f1']\n",
    "                    bert_score = sum( bert_scores ) / len( bert_scores )\n",
    "            if  n_batch % val_param == 0:\n",
    "                hypo_sentence1 = [preds_str[0]]\n",
    "                ref_sentence1 = [targets_str[0]]\n",
    "    \n",
    "            val_errors.append( avg_error )\n",
    "            val_bleus.append( avg_bleu )\n",
    "            val_clips.append( clip_score )\n",
    "            val_berts.append( bert_score )\n",
    "            if len(val_errors) > config.moving_avg:\n",
    "                #val_losses.popleft()\n",
    "                #val_rewards.popleft()\n",
    "                val_errors.popleft()\n",
    "                val_bleus.popleft()\n",
    "                val_clips.popleft()\n",
    "                val_berts.popleft()\n",
    "             #mean_loss = torch.Tensor(val_losses).mean().item()\n",
    "            #mean_reward = torch.Tensor(val_rewards).mean().item()\n",
    "            mean_error = torch.Tensor(val_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(val_bleus).mean().item()\n",
    "            mean_clip = torch.Tensor(val_clips).mean().item()\n",
    "            mean_bert = torch.Tensor(val_berts).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                #'loss': mean_loss,\n",
    "                #'reward': mean_reward,\n",
    "                'rougeL': mean_error,\n",
    "                'CIDER': mean_bleu,\n",
    "                'clip': mean_clip,\n",
    "                'bert': mean_bert,\n",
    "            })\n",
    "            # Validation Lossをログに書き込み\n",
    "            with open(val_loss_file, 'a') as f:\n",
    "                print(f'{epoch}, {mean_error}, {mean_bleu}, {mean_clip}, {mean_bert}', file=f)\n",
    "            \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                print(f'Val epoch = {epoch}, rougeL = {mean_error}, cider = {mean_bleu}, clip = {mean_clip}, bert = {mean_bert}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "    \n",
    "    # Loss 表示\n",
    "    #print(f'Validation loss: {val_loss}')\n",
    "    #print(f'Validation loss: {val_reward}')\n",
    "    print(f'Validation rougeL: {mean_error}')\n",
    "    print(f'Validation cider: {mean_bleu}')\n",
    "    print(f'Validation clip: {mean_clip}')\n",
    "    print(f'Validation bert: {mean_bert}')\n",
    "    \n",
    "    ## より良い検証結果が得られた場合、モデルを保存\n",
    "            \n",
    "    # モデルを保存\n",
    "    torch.save({'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        #'loss': loss,\n",
    "        },\n",
    "        f'{config.save_directory}/model_rl_grpo_crf33_.pth')\n",
    "    ## モデルを保存\n",
    "        \n",
    "## モデルを保存\n",
    "#torch.save({'epoch': epoch,\n",
    "#    'global_step': global_step,\n",
    "#    'model_state_dict': model.state_dict(),\n",
    "#    'optimizer_state_dict': optimizer.state_dict(),\n",
    "#    #'scheduler_state_dict': scheduler.state_dict(),\n",
    "#    #'loss': loss,\n",
    "#    },\n",
    "#    f'{config.save_directory}/model_rl_grpo_crf24_final.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### モデルを保存\n",
    "torch.save({'epoch': epoch,\n",
    "    'global_step': global_step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    #'loss': loss,\n",
    "    },\n",
    "    f'{config.save_directory}/model_rl_grpo_crf33_.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10ac0991cf0d4346a2dd950c4e69de74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc134416b78b4e16ab143f7b3c9b4db1",
      "placeholder": "​",
      "style": "IPY_MODEL_3e3ad9c31b8244d9987cb5d09ac80909",
      "value": " 230M/230M [00:03&lt;00:00, 57.2MB/s]"
     }
    },
    "189ddc1a5e674235a5a4c9461ec37bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89ed549065394bad9c015c612b04c1d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f9994c1870c348b0ab983404decb722e",
      "value": "100%"
     }
    },
    "2030796b20a24135908446e8a05b2447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e3ad9c31b8244d9987cb5d09ac80909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741ba7eec1f94e6889152972dbf220c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89ed549065394bad9c015c612b04c1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8d4c88aa714714b864351862297512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_741ba7eec1f94e6889152972dbf220c0",
      "max": 241669177,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be3b477f459a4fdd80b055ef88cf8b52",
      "value": 241669177
     }
    },
    "be3b477f459a4fdd80b055ef88cf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc134416b78b4e16ab143f7b3c9b4db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1785c740b7449584ceec0225c64c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_189ddc1a5e674235a5a4c9461ec37bb7",
       "IPY_MODEL_af8d4c88aa714714b864351862297512",
       "IPY_MODEL_10ac0991cf0d4346a2dd950c4e69de74"
      ],
      "layout": "IPY_MODEL_2030796b20a24135908446e8a05b2447"
     }
    },
    "f9994c1870c348b0ab983404decb722e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
