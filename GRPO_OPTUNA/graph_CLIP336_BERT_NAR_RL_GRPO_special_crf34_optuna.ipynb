{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278d1c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "data1 = '20260212_074027'\n",
    "\n",
    "with open('./model/MyOriginal_train_loss_' + data1 + '.csv',encoding='utf-8')as f:\n",
    "    line1_all = f.readlines()\n",
    "\n",
    "with open('./model/MyOriginal_val_loss_' + data1 + '.csv',encoding='utf-8')as f:\n",
    "    line2_all = f.readlines()\n",
    "\n",
    "i = 0\n",
    "x = []\n",
    "loss = []\n",
    "actor = []\n",
    "entropy = []\n",
    "critic = []\n",
    "kl_div = []\n",
    "reward = []\n",
    "ord1 = []\n",
    "rep = []\n",
    "pad = []\n",
    "adv = []\n",
    "error = []\n",
    "bleu = []\n",
    "crf = []\n",
    "ce = []\n",
    "clip = []\n",
    "unr = []\n",
    "bert = []\n",
    "v_i = 0\n",
    "v_x = []\n",
    "v_loss = []\n",
    "v_reward = []\n",
    "v_error = []\n",
    "v_bleu = []\n",
    "v_clip = []\n",
    "v_bert = []\n",
    "\n",
    "#line2 = line2_all[0].replace( \"\\n\", \"\")\n",
    "#len_tr_loader = int(line2)\n",
    "\n",
    "for i, line1 in enumerate( line1_all ):\n",
    "    #print( line1 )\n",
    "    if i == 0:\n",
    "        line1 = line1.replace( \"\\n\", \"\")\n",
    "        len_tr_loader = int(line1)\n",
    "    else:\n",
    "        line1_split = line1.split(\",\")\n",
    "        #i += 1\n",
    "        #x.append( i / len_tr_loader )\n",
    "        #        print(f'{epoch}, {mean_loss}, {mean_policy}, {mean_entropy}, {mean_critic}, {mean_kl_div}, {mean_reward}, {mean_ord}, \n",
    "        #        {mean_repeat}, {mean_length}, {mean_adv}, {mean_error}, {mean_bleu}', file=f)\n",
    "        x.append( float(line1_split[0].split(' ')[1]) )\n",
    "        loss.append( float(line1_split[1].split(' ')[1]) )\n",
    "        actor.append( float(line1_split[2].split(' ')[1]) )\n",
    "        entropy.append( float(line1_split[3].split(' ')[1]) )\n",
    "        critic.append( float(line1_split[4].split(' ')[1]) )\n",
    "        kl_div.append( float(line1_split[5].split(' ')[1]) )\n",
    "        reward.append( float(line1_split[6].split(' ')[1]) )\n",
    "        ord1.append( float(line1_split[7].split(' ')[1]) )\n",
    "        rep.append( float(line1_split[8].split(' ')[1]) )\n",
    "        pad.append( float(line1_split[9].split(' ')[1]) )\n",
    "        #print( \"float(line1_split[9].split(' ')[1]):\", float(line1_split[9].split(' ')[1]) )\n",
    "        adv.append( float(line1_split[10].split(' ')[1]) )\n",
    "        error.append( float(line1_split[11].split(' ')[1]) )\n",
    "        bleu.append( float(line1_split[12].split( ' ' )[1]))\n",
    "        crf.append( float(line1_split[13].split(' ')[1]) )\n",
    "        ce.append( float(line1_split[14].split( ' ' )[1]))\n",
    "        clip.append( float(line1_split[15].split( ' ' )[1]))\n",
    "        unr.append( float(line1_split[16].split( ' ' )[1]))\n",
    "        bert.append( float(line1_split[17].split( ' ' )[1]))\n",
    "\n",
    "#print( \"x:\", x )\n",
    "\n",
    "#print( \"pad:\", pad )\n",
    "\n",
    "for i, line2 in enumerate( line2_all ):\n",
    "    if i == 0:\n",
    "        line2 = line2.replace( \"\\n\", \"\")\n",
    "        len_val_loader = int(line2)\n",
    "    else:\n",
    "        #print( \"line2:\", line2 )\n",
    "        line2_split = line2.split(\",\")\n",
    "        v_i += 1\n",
    "        v_x.append( v_i )\n",
    "        #v_loss.append( float(line2_split[1].split(' ')[1]) )\n",
    "        #v_reward.append( float(line2_split[2].split(' ')[1] ) )\n",
    "        v_error.append( float(line2_split[1].split(' ')[1] ) )\n",
    "        v_bleu.append( float(line2_split[2].split(' ')[1]))\n",
    "        v_clip.append( float(line2_split[3].split(' ')[1]))\n",
    "        v_bert.append( float(line2_split[4].split(' ')[1]))\n",
    "\n",
    "plt.plot( x, loss, label=\"Train loss\")\n",
    "#plt.plot( v_x, v_loss, label=\"Val Loss\" )\n",
    "plt.title( \"Loss\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'Loss')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "#plt.xlim( 0, 0.01 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, actor, label=\"Train policy loss\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"policy loss\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'policy loss')\n",
    "plt.legend()\n",
    "#plt.xlim( 0, 0.01 )\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, crf, label = 'Train crf')\n",
    "#plt.plot( v_x, v_error, label=\"Val WER\")\n",
    "plt.title( \"crf\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'crf loss')\n",
    "plt.legend()\n",
    "#plt.ylim( 60, 120)\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, ce, label = 'Train ce')\n",
    "#plt.plot( v_x, v_bleu, label=\"Val ce\")\n",
    "plt.title( \"ce\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'ce')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 40 )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#plt.plot( x, repeat, label=\"Train reward repeat repeat の数のマイナス。repeat が少ないほうが良いので - から　0に誓うほうが良い。\")\n",
    "plt.plot( x, entropy, label=\"Train entropy loss\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"entropy loss\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'entropy loss')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "#plt.plot( x, pad, label=\"Train reward pad targetのpad とのずれのマイナス。- から　0に近いほうが良い。\")\n",
    "plt.plot( x, critic, label=\"Train gae loss\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"gae loss\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'gae loss')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "#plt.plot( x, pad, label=\"Train reward pad targetのpad とのずれのマイナス。- から　0に近いほうが良い。\")\n",
    "plt.plot( x, kl_div, label=\"Train kl_div loss\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"kl_div loss\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'kl_div loss')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, reward, label=\"Train reward\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"reward\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'reward')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, ord1, label=\"Train reward ord\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward ord\" )\n",
    "plt.title( \"reward ord\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'reward ord')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, rep, label=\"Train reward repeat\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"reward repeat\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'reward repeat')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot( x, pad, label=\"Train reward length\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"reward length\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'reward length')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, unr, label=\"Train reward unr\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"reward unr\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'reward unr')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot( x, adv, label=\"Train adv\")\n",
    "#plt.plot( v_x, v_reward, label=\"Val reward\" )\n",
    "plt.title( \"adv\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'adv')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 20 )\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.plot( x, error, label = 'Train rougeL')\n",
    "plt.plot( v_x, v_error, label=\"Val rougeL\")\n",
    "plt.title( \"rougeL\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'rougeL')\n",
    "plt.legend()\n",
    "#plt.ylim( 60, 120)\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, bleu, label = 'Train cider')\n",
    "plt.plot( v_x, v_bleu, label=\"Val cider\")\n",
    "plt.title( \"cider\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'cider')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 40 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, clip, label = 'Train clip')\n",
    "plt.plot( v_x, v_clip, label=\"Val clip\")\n",
    "plt.title( \"clip\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'clip')\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 40 )\n",
    "plt.show()\n",
    "\n",
    "plt.plot( x, bert, label = 'Train bert')\n",
    "plt.plot( v_x, v_bert, label=\"Val bert\")\n",
    "plt.title( \"bert\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'bert' )\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 40 )\n",
    "plt.show()\n",
    "\n",
    "#print( rep )\n",
    "#print( pad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa3fae7-5968-4a7b-8583-f927a2b4721c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./model/norm_\" + data1 + \".csv\",encoding='utf-8')as f:\n",
    "    line_all = f.readlines()\n",
    "\n",
    "epochs = []\n",
    "steps = []\n",
    "norm0s = []\n",
    "norm1s = []\n",
    "norm2s = []\n",
    "norm3s = []\n",
    "norm_means = []\n",
    "\n",
    "for i, line in enumerate( line_all ):\n",
    "    line_split = line.split( \", \" )\n",
    "    #print( \"line_split:\", line_split )\n",
    "    steps.append( float( line_split[1].split(\": \")[1] ) )\n",
    "    #epochs.append( float( line_split[1].split(\": \")[1] ) / len_tr_loader )\n",
    "    norm0s.append( float( line_split[2].split(\": \")[1] ) )\n",
    "    norm1s.append( float( line_split[3].split(\": \")[1] ) )\n",
    "    norm2s.append( float( line_split[4].split(\": \")[1] ) )\n",
    "    norm3s.append( float( line_split[5].split(\": \")[1] ) )\n",
    "    norm_means.append( float( line_split[6].split(\": \")[1] ) )\n",
    "\n",
    "#print( norm0s )\n",
    "#print( norm1s )\n",
    "#print( norm_means )\n",
    "\n",
    "#plt.plot( steps, norm0s, label = 'norm0s')\n",
    "#plt.plot( steps, norm1s, label = 'norm1s')\n",
    "#plt.plot( steps, norm_means, label = 'norm_menas')\n",
    "plt.plot( steps, norm0s, label = 'norm0s')\n",
    "plt.plot( steps, norm1s, label = 'norm1s')\n",
    "plt.plot( steps, norm2s, label = 'norm2s')\n",
    "plt.plot( steps, norm3s, label = 'norm3s')\n",
    "#plt.plot( epochs, norm_means, label = 'norm_menas')\n",
    "plt.title( \"norm\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'norm')\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 2 )\n",
    "plt.show()\n",
    "\n",
    "#plt.plot( steps, norm0s, label = 'norm0s')\n",
    "plt.plot( steps, norm_means, label = 'total_norm')\n",
    "plt.title( \"gradients norm\")\n",
    "plt.xlabel( 'steps')\n",
    "plt.ylabel( 'gradients norm')\n",
    "plt.yscale( 'log' )\n",
    "plt.legend()\n",
    "#plt.ylim( 0, 1.0 )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92fd8da4-6d0b-4d5a-bb19-2edea0d7deaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "batch_size = 4\n",
    "seq_len = 100\n",
    "num_embedding = 30000\n",
    "low_rank = 32\n",
    "rank = low_rank\n",
    "beam = 256\n",
    "\n",
    "E1 = nn.Embedding(num_embedding, low_rank)\n",
    "E2 = nn.Embedding(num_embedding, low_rank)\n",
    "\n",
    "logits = torch.randn( ( batch_size, seq_len, num_embedding ) )\n",
    "beam_logits, beam_targets = torch.topk( logits, beam, dim = 2 )\n",
    "beam_emission_scores = beam_logits\n",
    "\n",
    "print( beam_targets.size() )\n",
    "\n",
    "beam_transition_score1 = E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step. \n",
    "print( \"beam_transition_score1 size:\", beam_transition_score1.size() )\n",
    "beam_transition_score2 = E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "beam_transition_matrix = torch.bmm(\n",
    "    beam_transition_score1.view(-1, beam, rank),\n",
    "    beam_transition_score2.view(-1, beam, rank).transpose(1, 2))\n",
    "beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "# compute the normalizer in the log-space\n",
    "scores = torch.zeros( ( batch_size, seq_len, beam ) )\n",
    "scores[:,0,:] = beam_emission_scores[:,0]\n",
    "for i in range(1, seq_len):\n",
    "    scores[:,i] = beam_emission_scores[:, i] + torch.sum(beam_transition_matrix[:, :, i, :], dim = 1 ) \n",
    "\n",
    "max_score, _ = torch.max( scores, dim = 2 )\n",
    "exp1 = torch.exp( scores - max_score[:,:,None].expand(batch_size,seq_len,beam) )\n",
    "sumof = torch.sum( exp1, dim = 2 )\n",
    "denominator1 = torch.log( sumof + 1e-8 ) + max_score\n",
    "\n",
    "beam_log_probs = scores - denominator1.view(batch_size, seq_len, 1)\n",
    "\n",
    "beam_probs = torch.exp(beam_log_probs)\n",
    "\n",
    "#outputs = torch.bmm(E1(targets[:, :-1]).view( -1, 256, 32 ), E2(targets[:, 1:]).view( -1, 256, 32).transpose(1,2) )\n",
    "\n",
    "print( beam_probs.size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba1ff0c-b9d1-4ee5-a70e-611e032d3720",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "eos_token_id = 3\n",
    "preds = torch.tensor([[1, 3, 3], [4, 3, 6], [7, 8, 9]])\n",
    "\n",
    "# 条件に一致する要素のインデックスを直接取得\n",
    "# coords_listは、座標のペアが各行に含まれるテンソルになる\n",
    "pred_idx = torch.where(preds == eos_token_id)\n",
    "\n",
    "#pred_lengths = torch.zeros( (preds.size(0), dtype=torch.float, device=self.device )\n",
    "pred_lengths = torch.zeros( (preds.size(0)), dtype=torch.float )\n",
    "\n",
    "for i in range( pred_idx[0].size(0)):\n",
    "    pred_lengths[pred_idx[0][i]] = pred_idx[1][i]\n",
    "    \n",
    "print( pred_lengths )\n",
    "print( pred_idx )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8746f922-a63f-4b8e-b455-60f8044f3c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "preds = torch.tensor([[1, 2, 3], [4, 3, 6], [7, 8, 9]])\n",
    "\n",
    "pred_index = preds == eos_token_id\n",
    "first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "print( first_index )\n",
    "arange_index = torch.arange( 0, first_index.size(1) )\n",
    "pred_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 )\n",
    "print( pred_lengths )\n",
    "\n",
    "\n",
    "# 出力例: (tensor([0, 1]), tensor([2, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a305135-8419-416d-b51c-506d180cef74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "a = torch.tensor( [[0.6, 0.2],[0.1, 0.3]] )\n",
    "\n",
    "print( a )\n",
    "print( a[:,0] )\n",
    "print( a[:,1] )\n",
    "\n",
    "b, c = a.max( dim = 0 )\n",
    "\n",
    "print( b )\n",
    "print( c )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b024d675-b2ff-4eed-8a78-0c59389ccf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "_score = torch.randn( (2, 256, 256 ) )\n",
    "\n",
    "_score_multinomial = []\n",
    "_index_multinomial = []\n",
    "\n",
    "for b in range( _score.size(0) ):\n",
    "    _score_multinomial0 = []\n",
    "    _index_multinomial0 = []\n",
    "    for i in range( _score.size(2) ):\n",
    "        _score2 = _score[b,:,i]\n",
    "        _score3 = F.softmax( _score2, dim = -1 )\n",
    "        _index2 = torch.multinomial( _score3, 1 )\n",
    "        _score5 = torch.gather( _score2, -1, _index2 ).squeeze(-1)\n",
    "        _index_multinomial0.append( _index2.squeeze(-1) )\n",
    "        _score_multinomial0.append( _score5 )\n",
    "    _index_multinomial0 = torch.stack( _index_multinomial0, dim = 0 )\n",
    "    _score_multinomial0 = torch.stack( _score_multinomial0, dim = 0 )\n",
    "    print( _index_multinomial0.size() )\n",
    "    print( _score_multinomial0.size() )\n",
    "    _index_multinomial.append( _index_multinomial0 )\n",
    "    _score_multinomial.append( _score_multinomial0 )\n",
    "\n",
    "_index_multinomial = torch.stack( _index_multinomial, dim = 0 )\n",
    "_score_multinomial = torch.stack( _score_multinomial, dim = 0 )\n",
    "\n",
    "print( _index_multinomial.size() )\n",
    "print( _score_multinomial.size() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f320f2-46b5-4fcd-9d7c-2c69f1caafe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "eps = 1e-4\n",
    "logits = torch.randn( (8, 97, 30000 ) )\n",
    "bsz, seq_len, vocab_size = logits.size()\n",
    "\n",
    "probs = torch.softmax( logits, dim = 2 )\n",
    "probs = torch.nan_to_num(probs, nan=1e-4)\n",
    "#log_probs = torch.log(probs + eps )\n",
    "\n",
    "## Apply softmax to get probabilities\n",
    "probs2 = torch.softmax(logits.view( bsz * seq_len, -1 ), dim=1).view( bsz, seq_len, -1 )  # (batch_size * context_len, vocab_size)\n",
    "probs2 = torch.nan_to_num(probs2, nan=1e-4)\n",
    "\n",
    "print( probs[0][0] )\n",
    "print( probs2[0][0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d06fa88-eaa6-4bb0-9bdf-1bfddf71a6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo 5, true to the algorithm of the paper https://aclanthology.org/2021.cl-4.29.pdf\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained( model_id )\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "\n",
    "print( \"sos_tokeni_d:\", sos_token_id )\n",
    "print( \"eos_token_id:\", eos_token_id )\n",
    "\n",
    "import math\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairseq.criterions import FairseqCriterion, register_criterion\n",
    "from fairseq.data import encoders\n",
    "from fairseq.dataclass import FairseqDataclass\n",
    "from dataclasses import dataclass, field\n",
    "from fairseq.logging import metrics\n",
    "from sacrebleu.metrics import BLEU\n",
    "from evaluate import load\n",
    "import jiwer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import wandb\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class RLCriterionConfig(FairseqDataclass):\n",
    "    sentence_level_metric: str = field(default=\"bleu\", metadata={\"help\": \"sentence level metric\"}) \n",
    "    sampling: str = field(default=\"multinomial\", metadata={\"help\": \"sampling method\"})\n",
    "    detokenization: bool = field(default=True,  metadata={\"help\": \"whether to detokenize the output\"})\n",
    "    wandb_project: str = field(default=\"nlp2-nanmt\", metadata={\"help\": \"wandb project\"})\n",
    "    wandb_run: Optional[str] = field(default=None, metadata={\"help\": \"wandb run name\"})\n",
    "    \n",
    "#@register_criterion(\"rl_loss\", dataclass=RLCriterionConfig)\n",
    "class RLCriterion(FairseqCriterion):\n",
    "    def __init__(self, task, sentence_level_metric=\"bleu\", sampling=\"multinomial\", wandb_project='nlp2-nanmt', wandb_run=None):\n",
    "        super().__init__(task)\n",
    "        self.metric = sentence_level_metric\n",
    "        #self.tokenizer = encoders.build_tokenizer(Namespace(tokenizer='moses'))\n",
    "        self.tokenizer = task\n",
    "        #self.tgt_dict = task.target_dictionary\n",
    "        #self.src_dict = task.source_dictionary\n",
    "        self.tgt_lang = \"en\"\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = torch.device(\"cpu\")\n",
    "        self.sampling = sampling\n",
    "        self.bleu = BLEU(effective_order=\"True\")\n",
    "        self.meteor = load('meteor')\n",
    "        #self.rouge = load('rouge')\n",
    "        # self.ter = load('ter')\n",
    "        #self.bert = load('bertscore')\n",
    "        #self.bleurt = load('bleurt', module_type='metric', checkpoint='bleurt-large-128')\n",
    "        if self.metric == \"comet\":\n",
    "          model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "          self.comet = load_from_checkpoint(model_path)\n",
    "        # init wandb project\n",
    "        #wandb.init(project=wandb_project)\n",
    "        #if wandb_run:\n",
    "        #    wandb.run.name = wandb_run\n",
    "        \n",
    "    def forward(self, model, sample, reduce=True):\n",
    "        \"\"\"Compute the loss for the given sample.\n",
    "        Returns a tuple with three elements:\n",
    "        1) the loss\n",
    "        2) the sample size, which is used as the denominator for the gradient\n",
    "        3) logging outputs to display while training\n",
    "        \"\"\"\n",
    "        nsentences, ntokens = sample[\"nsentences\"], sample[\"ntokens\"]\n",
    "\n",
    "        # B x T\n",
    "        src_tokens, src_lengths = (\n",
    "            sample[\"net_input\"][\"src_tokens\"],\n",
    "            sample[\"net_input\"][\"src_lengths\"],\n",
    "        )\n",
    "        tgt_tokens, prev_output_tokens = sample[\"target\"], sample[\"prev_target\"]\n",
    "\n",
    "        outputs = model(src_tokens, src_lengths, prev_output_tokens, tgt_tokens)\n",
    "        \n",
    "        #get loss only on tokens, not on lengths\n",
    "        outs = outputs[\"word_ins\"].get(\"out\", None)\n",
    "        masks = outputs[\"word_ins\"].get(\"mask\", None)\n",
    "\n",
    "        loss, reward = self._compute_loss(outs, tgt_tokens, src_tokens, masks)\n",
    "\n",
    "        # NOTE:\n",
    "        # we don't need to use sample_size as denominator for the gradient\n",
    "        # here sample_size is just used for logging\n",
    "        sample_size = 1\n",
    "        logging_output = {\n",
    "            \"loss\": loss.detach(),\n",
    "            \"nll_loss\": loss.detach(),\n",
    "            \"ntokens\": ntokens,\n",
    "            \"nsentences\": nsentences,\n",
    "            \"sample_size\": sample_size,\n",
    "            \"reward\": reward.detach()\n",
    "        }\n",
    "        \n",
    "        return loss, sample_size, logging_output\n",
    "\n",
    "    def decode(self, toks, ref=\"tgt\", escape_unk=False):\n",
    "        \"\"\"\n",
    "        Decode a tensor of token ids into a string.\n",
    "        \"\"\"\n",
    "        decode_dict = self.tgt_dict if ref == \"tgt\" else self.src_dict\n",
    "        with torch.no_grad():\n",
    "            s = decode_dict.string(\n",
    "                toks.int().cpu(),\n",
    "                \"@@ \",\n",
    "                # The default unknown string in fairseq is `<unk>`, but this is tokenized by sacrebleu as `< unk >`, inflating BLEU scores. Instead, we use a somewhat more verbose alternative that is unlikely to appear in the real reference, but doesn't get split into multiple tokens.\n",
    "                unk_string=(\n",
    "                    \"UNKNOWNTOKENINREF\" if escape_unk else \"UNKNOWNTOKENINHYP\"\n",
    "                ),\n",
    "            )\n",
    "            s = self.tokenizer.decode(s)\n",
    "        return s\n",
    "\n",
    "    def compute_reward(self, preds, targets, sources=None):\n",
    "        \"\"\"\n",
    "        Compute reward metric for a batch of prediction and target sentences\n",
    "        \"\"\"\n",
    "        \n",
    "        # detokenize (convert to str) preds & targets\n",
    "        #preds_str = [self.decode(pred) for pred in preds]\n",
    "        #targets_str = [self.decode(target) for target in targets]\n",
    "        #sources_str = [self.decode(source, ref=\"src\") for source in sources] if sources is not None else None\n",
    "        #preds_str = [self.tokenizer.decode(pred) for pred in preds]\n",
    "        #targets_str = [self.tokenizer.decode(target) for target in targets]\n",
    "        sources_str = [self.tokenizer.decode(source, ref=\"src\") for source in sources] if sources is not None else None\n",
    "        #preds_str = [self.tokenizer.decode(pred, skip_special_tokens = True) for pred in preds]\n",
    "        #targets_str = [self.tokenizer.decode(target, skip_special_tokens = True) for target in targets]\n",
    "        pad_token_id = tokenizer.pad_token_id\n",
    "        preds_str = [self.tokenizer.decode([token_id for token_id in pred if token_id != pad_token_id]) for pred in preds]\n",
    "        targets_str = [self.tokenizer.decode([token_id for token_id in target if token_id != pad_token_id]) for target in targets]\n",
    "        #print(f'1st target sent: {targets_str[0]}')\n",
    "        #print(f'1st pred sent: {preds_str[0]}')\n",
    "\n",
    "        # compute reward metric\n",
    "        seq_len = preds.shape[1]\n",
    "        if self.metric == \"bleu\":\n",
    "            reward = [[self.bleu.sentence_score(pred, [target]).score] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "        \n",
    "        elif self.metric == \"meteor\":\n",
    "            meteor_scores = [self.meteor.compute(predictions=[preds], references=[targets])['meteor'] for preds, targets in zip(preds_str, targets_str)]\n",
    "            reward = [[score] * seq_len for score in meteor_scores]\n",
    "            \n",
    "        elif self.metric == \"rouge\":\n",
    "            rouge_scores = self.rouge.compute(predictions=preds_str, references=targets_str, use_aggregator=False)['rougeL']\n",
    "            reward = [[score] * seq_len for score in rouge_scores]\n",
    "            \n",
    "        elif self.metric == \"wer\":\n",
    "            wer_scores = [jiwer.wer(target, pred) for pred, target in zip(targets_str, preds_str)]\n",
    "            reward = [[score] * seq_len for score in wer_scores]\n",
    "            \n",
    "        elif self.metric == \"bert\":\n",
    "            bert_scores = self.bert.compute(predictions=preds_str, references=targets_str, lang='en')['f1']\n",
    "            reward = [[score] * seq_len for score in bert_scores]\n",
    "\n",
    "        elif self.metric == \"bleurt\":\n",
    "            bleurt_scores = self.bleurt.compute(predictions=preds_str, references=targets_str)['scores']\n",
    "            reward = [[score] * seq_len for score in bleurt_scores]\n",
    "            \n",
    "        elif self.metric == \"comet\":\n",
    "            data = [{\"src\": source, \"mt\": pred, \"ref\": target} for source, pred, target in zip(sources_str, preds_str, targets_str)]\n",
    "            reward = self.comet.predict(data, batch_size=8, gpus=1)['scores']\n",
    "            reward = [[score] * seq_len for score in reward]\n",
    "        else:\n",
    "            raise ValueError(f\"metric {self.metric} not supported\")\n",
    "        reward = torch.tensor(reward).to(self.device)\n",
    "        return reward\n",
    "\n",
    "    def compute_pad_reward( self, preds, targets ):\n",
    "\n",
    "        #def differentiable_argamx( logits, tau ):\n",
    "\n",
    "        #    tmp = F.gumbel_softmax( logits, tau, hard=True )\n",
    "        #    tmp1 = torch.arange( 0, logits.size(2) )[None,None] * tmp\n",
    "        #    tokens = torch.sum( tmp1, dim = 2 )\n",
    "\n",
    "        #    return tokens\n",
    "\n",
    "        #tokens = differentiable_argamx( logits, tau ) #logits から token を算出。微分可能 B * T\n",
    "        tmp = torch.abs( preds - tokenizer.pad_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        pad_preds = F.sigmoid( 10 - 100 * tmp ) # pad のところだけ 1, あとは 0。 B * T\n",
    "        #length_preds = preds.size(1) - torch.sum( pad_preds, dim = 1 )\n",
    "\n",
    "        tmp = torch.abs( targets - tokenizer.pad_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        pad_targets = F.sigmoid( 10 - 100 * tmp ) # pad のところだけ 1, あとは 0。 B * T\n",
    "        #length_targets = targets.size(1) - torch.sum( pad_targets, dim = 1 )\n",
    "        \n",
    "        #print( \"length_preds:\", length_preds )\n",
    "        #print( \"length_targets:\", length_targets )\n",
    "        \n",
    "        #reward = - nn.MSELoss( reduction = 'none' )( length_preds, length_targets ) # pad の数を算出。 B\n",
    "        reward = - nn.MSELoss( reduction = 'none' )( pad_preds, pad_targets )\n",
    "        \n",
    "        return reward    \n",
    "\n",
    "    def compute_embedding_reward(self, logits, cap_embed ):\n",
    "\n",
    "        logits = F.normalize( logits, p = 2, dim = 1 )\n",
    "        cap_embed = F.normalize( cap_embed, p = 2, dim = 1 )\n",
    "\n",
    "        #reward = torch.sum( logits * cap_embed, dim = 2 )\n",
    "        reward = - torch.mean( nn.MSELoss(reduction='none')( logits, cap_embed ), dim = 2 )\n",
    "        \n",
    "        return reward\n",
    "\n",
    "    \n",
    "    #def _compute_loss(self, outputs, targets, sources, masks=None):\n",
    "    def _compute_loss(self, outputs, outputs1024, targets, sources, masks=None):\n",
    "        \"\"\"\n",
    "        outputs: batch x len x d_model\n",
    "        targets: batch x len\n",
    "        sources: batch x len\n",
    "        masks:   batch x len\n",
    "        \"\"\"\n",
    "        self.device = outputs.device\n",
    "        \n",
    "        # input to device\n",
    "        outputs = outputs.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "        \n",
    "        bsz, seq_len, vocab_size = outputs.size()\n",
    "\n",
    "        reward = []\n",
    "        policy_loss = 0\n",
    "        itern = 2\n",
    "        eps = 1e-4\n",
    "        #preds = torch.argmax( outputs, dim = 2 )\n",
    "        # Sample from the distribution\n",
    "        #preds_itern = torch.multinomial(probs2, num_samples=itern).view( bsz, seq_len, -1 )  # (B * T * num_samples)\n",
    "        probs = F.softmax( outputs, dim = 2 )\n",
    "        probs2 = probs.clone().view( bsz * seq_len, -1 )\n",
    "        input_probs = torch.clamp( probs, min = eps )\n",
    "        log_probs = torch.log( input_probs )\n",
    "        print( \"log_probs size:\", log_probs.size() )\n",
    "        preds = torch.multinomial(probs2, num_samples=1, replacement = True ).view( bsz, seq_len )  # (B * T * num_samples)\n",
    "        #Vitern = preds_itern\n",
    "        V = preds # bsz\n",
    "        Vy = targets # bsz\n",
    "        #reulst_itern = [　list( (set(Vitern[:,:,m) - set(Vy) ) ) for n in range( itern ) ] # (Vitern にあって Vy にない要素)だから不定のlist　の itern_list の list\n",
    "        #w_itern = [ random.choice( result_itern[n] ) for n in range( itern ) ] # 1の要素のリストの itern_list \n",
    "        #idx_w_itern = [ index( Vitern[b,:,n].tollist(), w_itern[b,n] ) for n in range( itern ) for b in range( bsz ) ]\n",
    "        #idx_w_itern = torch.tensor( idx_w_itern ) # bsz * itern\n",
    "        #reulst = list( (set(V[b]) - set(Vy[b]] for v in range( bsz )[ )# (Vitern にあって Vy にない要素)だから不定のlist\n",
    "        #idx_result = [ [index( V[b:,:].tolist(), r ) for r in result[b] ] for b in range( bsz )] # 個数 * bsz のリスト　リストのリストにならないかも\n",
    "        #w = random.choice( result ) # 1の要素\n",
    "        ##w_itern = torch.tensor( w_itern )\n",
    "        #idx_w = [ index( V[b,:].tollist(), w[b] ) for b in range( bsz ) ]\n",
    "        #idx_w = torch.tensor( idx_w_itern ) # bsz\n",
    "        def make_hashable(item):\n",
    "            \"\"\"ネストされたリストを再帰的にタプルに変換する\"\"\"\n",
    "            if isinstance(item, list):\n",
    "                return tuple(make_hashable(sub_item) for sub_item in item)\n",
    "            else:\n",
    "                return item\n",
    "        \n",
    "        result = [ list(set(make_hashable(V[b].tolist())) - set(make_hashable(Vy[b].tolist()))) for b in range(bsz)]\n",
    "        print( \"result len:\", len( result ) )\n",
    "        w = torch.tensor( [random.choice( result[b] ) for b in range( bsz )] )# 1の要素\n",
    "        print( \"w:\", w )\n",
    "        print( \"w size:\", w.size() )\n",
    "        print( \" V size:\", V.size() )\n",
    "        mask = V == w[:,None]\n",
    "        mask = torch.tensor( mask )\n",
    "        #_, idx_w = torch.where( mask )\n",
    "        _, idx_w = torch.where( mask )\n",
    "        print( idx_w )\n",
    "\n",
    "        ryt = torch.zeros( ( vocab_size ) )\n",
    "        print( \"ryt size:\", ryt.size() )\n",
    "        for t in range( seq_len ):\n",
    "            print( \"t: \", t )\n",
    "            Y = []\n",
    "            for yt in Vy.transpose( 0, 1 ):\n",
    "                #print( \"yt size:\", yt.size() )\n",
    "                Y.append( yt )\n",
    "                reward_t_sum = 0\n",
    "                for n in range( itern ):\n",
    "                    preds_t = torch.multinomial(probs2, num_samples=1, replacement = True ).view( bsz, seq_len )  # (B * T * num_samples)\n",
    "                    preds_t[:,t] = yt[:] # B  preds_t は B * seq_len\n",
    "                    reward_t_itern = self.compute_reward(preds_t, targets, sources)[:,0] # B\n",
    "                    reward_t_sum = reward_t_sum + reward_t_itern # B\n",
    "                reward_t = reward_t_sum / itern # B\n",
    "                for b in range( bsz ):\n",
    "                    #print( \"yt[b]:\", yt[b] )\n",
    "                    ryt[yt[b].long()] = reward_t[b]\n",
    "            reward_t_sum2 = 0\n",
    "            for n in range( itern ):\n",
    "                preds_t2 = torch.multinomial(probs2, num_samples=1, replacement = True ).view( bsz, seq_len )  # (B * T * num_samples)\n",
    "                #print( \"preds_t2[:,t].size():\", preds_t2[:,t].size() )\n",
    "                #print( \"w.size():\", w.size() ) \n",
    "                preds_t2[:,t] = w[:]   # bsz* 1 を bsz * n に broadcast\n",
    "                reward_t_itern2 = self.compute_reward(preds_t2, targets, sources)[:,0] # B\n",
    "                reward_t_sum2 = reward_t_sum2 + reward_t_itern2 # B\n",
    "            reward_t_w = reward_t_sum2 / itern\n",
    "            #ここいらへんから怪しい\n",
    "            for b in range( bsz ):\n",
    "                for yt2 in result[b]:\n",
    "                    ryt[yt2] = reward_t_w[b]\n",
    "            #for b in range( bsz ):\n",
    "            for yt in V.transpose(0,1 ):\n",
    "                #print( \"yt:\", yt )\n",
    "                sample_log_probs_yt = torch.gather( log_probs[:,t,:], -1, yt[:,None] )[:,0]\n",
    "                for b in range( bsz ):\n",
    "                    policy_loss = policy_loss -  sample_log_probs_yt[b] * ryt[yt[b].long()]\n",
    "\n",
    "        \n",
    "            ## apply mask\n",
    "            #if masks is not None:\n",
    "            #    #masks = masks.to(self.device)\n",
    "            #    #probs, targets = probs[masks], targets[masks]\n",
    "            #    ## outputs, targets = outputs[masks], targets[masks]\n",
    "            #    #reward, preds = reward[masks], preds[masks]\n",
    "\n",
    "        policy_loss = torch.mean( policy_loss, dim = 0 )\n",
    "        #reward = torch.stack( reward, dim = 0 ).mean()\n",
    "        reward = ryt.mean()\n",
    "        return policy_loss, reward\n",
    "    \n",
    "    @staticmethod\n",
    "    def reduce_metrics(logging_outputs) -> None:\n",
    "        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n",
    "        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n",
    "        reward_sum= sum(log.get(\"reward\", 0) for log in logging_outputs)\n",
    "        ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n",
    "        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n",
    "\n",
    "        # we divide by log(2) to convert the loss from base e to base 2\n",
    "        metrics.log_scalar(\n",
    "            \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n",
    "        )\n",
    "        metrics.log_scalar(\"reward\", reward_sum / sample_size, sample_size, round=3)\n",
    "        \n",
    "        # log to wandb\n",
    "        wandb.log({\n",
    "            \"loss\": loss_sum / sample_size / math.log(2),\n",
    "            \"reward\": reward_sum / sample_size,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45285b0-7122-4061-ab88-16be2372e638",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlcriterion = RLCriterion(task=tokenizer, sentence_level_metric=\"bleu\" )\n",
    "\n",
    "outputs = torch.randn(  (2, 97, len(tokenizer) ), requires_grad = True )\n",
    "targets = torch.randint( 0, len(tokenizer), size=(2, 97 ) )\n",
    "\n",
    "#loss, reward, pred = rlcriterion._compute_loss( outputs, outputs, targets, sources = None )\n",
    "loss, reward  = rlcriterion._compute_loss( outputs, outputs, targets, sources = None )\n",
    "\n",
    "print( \"loss:\", loss )\n",
    "print( \"reward:\", reward )\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2ba766-23ed-452f-9086-969aef6b58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "a = [1,2,3,4]\n",
    "b = [1,2,3,5]\n",
    "\n",
    "c = set( a ) - set( b )\n",
    "\n",
    "print( c )\n",
    "\n",
    "bsz = 4\n",
    "\n",
    "V = torch.tensor( [[1,2,3,4],[2,3,4,5],[3,4,5,6],[4,5,6,7]] )\n",
    "Vy = torch.tensor([[1,2,6,5],[2,3,4,6],[3,4,5,7],[4,5,6,8]] )\n",
    "\n",
    "print( \"V[0]:\", set(V[0].tolist()) )\n",
    "print( \"Vy[0]:\", set(Vy[0].tolist() ) )\n",
    "\n",
    "a = set(V[0].tolist())\n",
    "b = set(Vy[0].tolist())\n",
    "\n",
    "print( \"a:\", a )\n",
    "print( \"b:\", b )\n",
    "\n",
    "c =  a - b\n",
    "print( \"c:\", c )\n",
    "\n",
    "result = [ list(set(V[b].tolist()) - set(Vy[b].tolist() ) ) for b in range( bsz ) ] #  不定 の bsz_list\n",
    "w = torch.tensor( [ random.choice( result[b] ) for b in range( bsz ) ] )# 1の要素\n",
    "print( result )\n",
    "#idx_result = [ [( V[b:].tolist().index( r2 ) ) for r2 in result[b] ] for b in range( bsz )] # 個数 * bsz のリスト　リストのリストにならないかも\n",
    "#print( \"V size:\", V.size() )\n",
    "#print( \"result size:\", result.size() )\n",
    "\n",
    "mask = V == w\n",
    "\n",
    "mask = torch.tensor( mask )\n",
    "\n",
    "print( \"mask:\", mask )\n",
    "\n",
    "_, idx_w = torch.where( mask )\n",
    "print( \"idx_result:\", idx_result )\n",
    "\n",
    "\n",
    "\n",
    "#idx_w = [ index( V[b,:].tollist(), w[b] ) for b in range( bsz ) ]\n",
    "#idx_w = torch.tensor( idx_w_itern ) # bsz\n",
    "\n",
    "\n",
    "\n",
    "#reulst = list( (set(V) - set(Vy) ) )# (Vitern にあって Vy にない要素)だから不定のlist\n",
    "#idx_result = [ [index( V[b:,:].tolist(), r ) for r in result[b] ] for b in range( bsz )] # 個数 * bsz のリスト　リストのリストにならないかも\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22caf565-a1f7-493b-998b-162ea0570b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "outputs = torch.randn( ( 8, 97, 30000 ) )\n",
    "\n",
    "bsz, seq_len, vocab_size = outputs.size()\n",
    "itern = 4\n",
    "probs = F.softmax( outputs, dim = 2 )\n",
    "probs2 = probs.view( bsz * seq_len, -1 )\n",
    "\n",
    "preds_itern = torch.multinomial(probs2, num_samples=itern * seq_len, replacement = True).view( bsz, seq_len, -1 )  # (batch_size, context_len)\n",
    "#reward_t_itern = torch.stack( [ torch.stack( [(compute_reward(preds_itern[:,:,n * seq_len + t ], targets, sources)[:,0] - b ) \\\n",
    "#    for t in range( seq_len ) ], dim = 0 ) for n in range( itern ) ], dim = 0 ) # itern * seq_len * b\n",
    "reward_t_itern = torch.stack( [ torch.stack( [ torch.tensor( n * seq_len + t  )  \\\n",
    "    for t in range( seq_len ) ], dim = 0 ) for n in range( itern ) ], dim = 0 ) # itern * seq_len * b\n",
    "print( \"size of reward_t_itern:\", reward_t_itern.size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f8e2aa-192b-4bee-943a-8510e7459958",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.stack( [torch.stack([  torch.tensor( n * seq_len + t ) for t in range( seq_len ) ], dim = 0 ) for n in range( itern ) ], dim = 0 )\n",
    "print( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead9291f-f2b2-4837-b9f1-86a3959a0e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# algo 2 + 3, rapid\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained( model_id )\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "\n",
    "print( \"sos_tokeni_d:\", sos_token_id )\n",
    "print( \"eos_token_id:\", eos_token_id )\n",
    "\n",
    "import math\n",
    "from argparse import Namespace\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from fairseq.criterions import FairseqCriterion, register_criterion\n",
    "from fairseq.data import encoders\n",
    "from fairseq.dataclass import FairseqDataclass\n",
    "from dataclasses import dataclass, field\n",
    "from fairseq.logging import metrics\n",
    "from sacrebleu.metrics import BLEU\n",
    "from evaluate import load\n",
    "import jiwer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "import wandb\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class RLCriterionConfig(FairseqDataclass):\n",
    "    sentence_level_metric: str = field(default=\"bleu\", metadata={\"help\": \"sentence level metric\"}) \n",
    "    sampling: str = field(default=\"multinomial\", metadata={\"help\": \"sampling method\"})\n",
    "    detokenization: bool = field(default=True,  metadata={\"help\": \"whether to detokenize the output\"})\n",
    "    wandb_project: str = field(default=\"nlp2-nanmt\", metadata={\"help\": \"wandb project\"})\n",
    "    wandb_run: Optional[str] = field(default=None, metadata={\"help\": \"wandb run name\"})\n",
    "    \n",
    "#@register_criterion(\"rl_loss\", dataclass=RLCriterionConfig)\n",
    "class RLCriterion(FairseqCriterion):\n",
    "    def __init__(self, task, sentence_level_metric=\"bleu\", sampling=\"multinomial\", b = 30.0, itern = 10,  wandb_project='nlp2-nanmt', wandb_run=None):\n",
    "        super().__init__(task)\n",
    "        self.metric = sentence_level_metric\n",
    "        #self.tokenizer = encoders.build_tokenizer(Namespace(tokenizer='moses'))\n",
    "        self.tokenizer = task\n",
    "        #self.tgt_dict = task.target_dictionary\n",
    "        #self.src_dict = task.source_dictionary\n",
    "        self.tgt_lang = \"en\"\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.sampling = sampling\n",
    "        self.bleu = BLEU(effective_order=\"True\")\n",
    "        self.meteor = load('meteor')\n",
    "        #self.rouge = load('rouge')\n",
    "        # self.ter = load('ter')\n",
    "        #self.bert = load('bertscore')\n",
    "        #self.bleurt = load('bleurt', module_type='metric', checkpoint='bleurt-large-128')\n",
    "        if self.metric == \"comet\":\n",
    "          model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "          self.comet = load_from_checkpoint(model_path)\n",
    "        self.b = b\n",
    "        self.itern = itern\n",
    "        # init wandb project\n",
    "        #wandb.init(project=wandb_project)\n",
    "        #if wandb_run:\n",
    "        #    wandb.run.name = wandb_run\n",
    "        \n",
    "    def forward(self, model, sample, reduce=True):\n",
    "        \"\"\"Compute the loss for the given sample.\n",
    "        Returns a tuple with three elements:\n",
    "        1) the loss\n",
    "        2) the sample size, which is used as the denominator for the gradient\n",
    "        3) logging outputs to display while training\n",
    "        \"\"\"\n",
    "        nsentences, ntokens = sample[\"nsentences\"], sample[\"ntokens\"]\n",
    "\n",
    "        # B x T\n",
    "        src_tokens, src_lengths = (\n",
    "            sample[\"net_input\"][\"src_tokens\"],\n",
    "            sample[\"net_input\"][\"src_lengths\"],\n",
    "        )\n",
    "        tgt_tokens, prev_output_tokens = sample[\"target\"], sample[\"prev_target\"]\n",
    "\n",
    "        outputs = model(src_tokens, src_lengths, prev_output_tokens, tgt_tokens)\n",
    "        \n",
    "        #get loss only on tokens, not on lengths\n",
    "        outs = outputs[\"word_ins\"].get(\"out\", None)\n",
    "        masks = outputs[\"word_ins\"].get(\"mask\", None)\n",
    "\n",
    "        loss, reward = self._compute_loss(outs, tgt_tokens, src_tokens, masks)\n",
    "\n",
    "        # NOTE:\n",
    "        # we don't need to use sample_size as denominator for the gradient\n",
    "        # here sample_size is just used for logging\n",
    "        sample_size = 1\n",
    "        logging_output = {\n",
    "            \"loss\": loss.detach(),\n",
    "            \"nll_loss\": loss.detach(),\n",
    "            \"ntokens\": ntokens,\n",
    "            \"nsentences\": nsentences,\n",
    "            \"sample_size\": sample_size,\n",
    "            \"reward\": reward.detach()\n",
    "        }\n",
    "        \n",
    "        return loss, sample_size, logging_output\n",
    "\n",
    "    def decode(self, toks, ref=\"tgt\", escape_unk=False):\n",
    "        \"\"\"\n",
    "        Decode a tensor of token ids into a string.\n",
    "        \"\"\"\n",
    "        decode_dict = self.tgt_dict if ref == \"tgt\" else self.src_dict\n",
    "        with torch.no_grad():\n",
    "            s = decode_dict.string(\n",
    "                toks.int().cpu(),\n",
    "                \"@@ \",\n",
    "                # The default unknown string in fairseq is `<unk>`, but this is tokenized by sacrebleu as `< unk >`, inflating BLEU scores. Instead, we use a somewhat more verbose alternative that is unlikely to appear in the real reference, but doesn't get split into multiple tokens.\n",
    "                unk_string=(\n",
    "                    \"UNKNOWNTOKENINREF\" if escape_unk else \"UNKNOWNTOKENINHYP\"\n",
    "                ),\n",
    "            )\n",
    "            s = self.tokenizer.decode(s)\n",
    "        return s\n",
    "\n",
    "    def compute_reward(self, preds, targets, sources=None):\n",
    "        \"\"\"\n",
    "        Compute reward metric for a batch of prediction and target sentences\n",
    "        \"\"\"\n",
    "        \n",
    "        # detokenize (convert to str) preds & targets\n",
    "        #preds_str = [self.decode(pred) for pred in preds]\n",
    "        #targets_str = [self.decode(target) for target in targets]\n",
    "        #sources_str = [self.decode(source, ref=\"src\") for source in sources] if sources is not None else None\n",
    "        preds_str = [self.tokenizer.decode(pred) for pred in preds]\n",
    "        targets_str = [self.tokenizer.decode(target) for target in targets]\n",
    "        sources_str = [self.tokenizer.decode(source, ref=\"src\") for source in sources] if sources is not None else None\n",
    "        #preds_str = [self.tokenizer.decode(pred, skip_special_tokens = True) for pred in preds]\n",
    "        #targets_str = [self.tokenizer.decode(target, skip_special_tokens = True) for target in targets]\n",
    "        #pad_token_id = tokenizer.pad_token_id\n",
    "        #preds_str = [self.tokenizer.decode([token_id for token_id in pred if token_id != pad_token_id]) for pred in preds]\n",
    "        #targets_str = [self.tokenizer.decode([token_id for token_id in target if token_id != pad_token_id]) for target in targets]\n",
    "        #print(f'1st target sent: {targets_str[0]}')\n",
    "        #print(f'1st pred sent: {preds_str[0]}')\n",
    "\n",
    "        # compute reward metric\n",
    "        seq_len = preds.shape[1]\n",
    "        if self.metric == \"bleu\":\n",
    "            reward = [[self.bleu.sentence_score(pred, [target]).score] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward = [[self.bleu.sentence_score(pred, [target]).score] for pred, target in zip(preds_str, targets_str)]\n",
    "            #print( \"raw reward:\", reward )\n",
    "        \n",
    "        elif self.metric == \"meteor\":\n",
    "            meteor_scores = [self.meteor.compute(predictions=[preds], references=[targets])['meteor'] for preds, targets in zip(preds_str, targets_str)]\n",
    "            reward = [[score] * seq_len for score in meteor_scores]\n",
    "            \n",
    "        elif self.metric == \"rouge\":\n",
    "            rouge_scores = self.rouge.compute(predictions=preds_str, references=targets_str, use_aggregator=False)['rougeL']\n",
    "            reward = [[score] * seq_len for score in rouge_scores]\n",
    "            \n",
    "        elif self.metric == \"wer\":\n",
    "            wer_scores = [jiwer.wer(target, pred) for pred, target in zip(targets_str, preds_str)]\n",
    "            reward = [[score] * seq_len for score in wer_scores]\n",
    "            \n",
    "        elif self.metric == \"bert\":\n",
    "            bert_scores = self.bert.compute(predictions=preds_str, references=targets_str, lang='en')['f1']\n",
    "            reward = [[score] * seq_len for score in bert_scores]\n",
    "\n",
    "        elif self.metric == \"bleurt\":\n",
    "            bleurt_scores = self.bleurt.compute(predictions=preds_str, references=targets_str)['scores']\n",
    "            reward = [[score] * seq_len for score in bleurt_scores]\n",
    "            \n",
    "        elif self.metric == \"comet\":\n",
    "            data = [{\"src\": source, \"mt\": pred, \"ref\": target} for source, pred, target in zip(sources_str, preds_str, targets_str)]\n",
    "            reward = self.comet.predict(data, batch_size=8, gpus=1)['scores']\n",
    "            reward = [[score] * seq_len for score in reward]\n",
    "        else:\n",
    "            raise ValueError(f\"metric {self.metric} not supported\")\n",
    "        reward = torch.tensor(reward).to(self.device)\n",
    "        return reward\n",
    "\n",
    "    #def _compute_loss(self, outputs, targets, sources, masks=None):\n",
    "    def _compute_loss(self, outputs, outputs1024, targets, sources, masks=None):\n",
    "        \"\"\"\n",
    "        outputs: batch x len x d_model\n",
    "        targets: batch x len\n",
    "        sources: batch x len\n",
    "        masks:   batch x len\n",
    "        \"\"\"\n",
    "        self.device = outputs.device\n",
    "        \n",
    "        # input to device\n",
    "        outputs = outputs.to(self.device)\n",
    "        targets = targets.to(self.device)\n",
    "        \n",
    "        bsz, seq_len, vocab_size = outputs.size()\n",
    "\n",
    "        eps = 1e-4\n",
    "        probs = torch.softmax( outputs, dim = 2 )\n",
    "        probs2 = probs.view( bsz * seq_len, -1 )\n",
    "        log_probs = torch.log(probs + eps )\n",
    "\n",
    "        # Sample from the distribution\n",
    "        #for _ in range( seq_len ):\n",
    "        #    preds_itern = torch.multinomial(probs2, num_samples=itern, replacement = True).view( bsz, seq_len, -1 )  # (batch_size, context_len)\n",
    "        #    reward_t_itern = [ (self.compute_reward(preds_itern[:,:,n], targets, sources)[:,0] - b ) for n in range( itern ) ] # [ bsz ] list itern\n",
    "        #    reward_t_itern = torch.stack( reward_t_itern, dim = 0 ) # itern list * B\n",
    "        #    tmp = torch.mean( reward_t_itern, dim = 0 ) # itern についての平均 bsz\n",
    "        #    reward_t.append( tmp )\n",
    "        preds_itern = torch.multinomial(probs2, num_samples=self.itern * seq_len, replacement = True).view( bsz, seq_len, -1 )  # (batch_size, context_len)\n",
    "        preds_t = torch.multinomial( probs.view(-1, vocab_size ), num_samples = 1, replacement=True).view(bsz, seq_len, 1 )\n",
    "        preds_itern[:,:,:] = preds_t # bsz*seq_len*1 を bsz*seq_len*seq_len に broadcast\n",
    "        reward_t_itern = torch.stack( [ torch.stack( [(self.compute_reward(preds_itern[:,:,n * seq_len + t ], targets, sources)[:,0] - self.b ) \\\n",
    "                for t in range( seq_len ) ], dim = 0 ) for n in range( self.itern ) ], dim = 0 ) # itern * seq_len * b\n",
    "        reward_t = torch.mean( reward_t_itern, dim = 0 ) # torch.tensor([ seq_len, bsz ]) itern について平均\n",
    "        reward_t = reward_t.transpose(0,1) # bsz * seq_len\n",
    "        one_hot = F.one_hot( preds_itern[:,:,0], num_classes = vocab_size )\n",
    "        sample_log_probs = torch.sum( log_probs * one_hot, dim = 2 )\n",
    "        loss = torch.mean( - sample_log_probs * reward_t, dim = 1  ) #  bsz * seq_len かける bsz * seq_len = bsz * seq_len を seq_len　で平均。\n",
    "        \n",
    "            ## apply mask\n",
    "            #if masks is not None:\n",
    "                #masks = masks.to(self.device)\n",
    "                #probs, targets = probs[masks], targets[masks]\n",
    "                ## outputs, targets = outputs[masks], targets[masks]\n",
    "                #reward, preds = reward[masks], preds[masks]\n",
    "\n",
    "        reward = torch.mean( reward_t, dim =  1 ) # bsz\n",
    "        #print( \"reward:\", reward )\n",
    "        loss, reward = loss.mean(), reward.mean()\n",
    "        return loss, reward\n",
    "    \n",
    "    @staticmethod\n",
    "    def reduce_metrics(logging_outputs) -> None:\n",
    "        \"\"\"Aggregate logging outputs from data parallel training.\"\"\"\n",
    "        loss_sum = sum(log.get(\"loss\", 0) for log in logging_outputs)\n",
    "        reward_sum= sum(log.get(\"reward\", 0) for log in logging_outputs)\n",
    "        ntokens = sum(log.get(\"ntokens\", 0) for log in logging_outputs)\n",
    "        sample_size = sum(log.get(\"sample_size\", 0) for log in logging_outputs)\n",
    "\n",
    "        # we divide by log(2) to convert the loss from base e to base 2\n",
    "        metrics.log_scalar(\n",
    "            \"loss\", loss_sum / sample_size / math.log(2), sample_size, round=3\n",
    "        )\n",
    "        metrics.log_scalar(\"reward\", reward_sum / sample_size, sample_size, round=3)\n",
    "        \n",
    "        # log to wandb\n",
    "        wandb.log({\n",
    "            \"loss\": loss_sum / sample_size / math.log(2),\n",
    "            \"reward\": reward_sum / sample_size,\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda77c3c-dd37-451a-a490-205f507b2309",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlcriterion = RLCriterion(task=tokenizer, sentence_level_metric=\"bleu\" )\n",
    "\n",
    "outputs = torch.randn(  (2, 97, len(tokenizer) ), requires_grad = True )\n",
    "targets = torch.randint( 0, len(tokenizer), size=(2, 97 ) )\n",
    "\n",
    "#loss, reward, pred = rlcriterion._compute_loss( outputs, outputs, targets, sources = None )\n",
    "loss, reward  = rlcriterion._compute_loss( outputs, outputs, targets, sources = None )\n",
    "\n",
    "print( \"loss:\", loss )\n",
    "print( \"reward:\", reward )\n",
    "\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d283b9d-4666-47bb-b3d0-f7dd6d92ec09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "a = nn.Parameter( torch.randn( ( 1,2 ), requires_grad = True ) )\n",
    "fca = nn.Linear( 2, 2 )\n",
    "b = nn.Parameter( torch.randn( ( 1,2 ), requires_grad = False ) )\n",
    "fcb = nn.Linear( 2, 2 )\n",
    "c= nn.Parameter( torch.randn( ( 1,2 ), requires_grad = True ) )\n",
    "fcc = nn.Linear( 2, 2 )\n",
    "\n",
    "d = torch.sum( fca(a) * ( fcb(b) + fcc(c) ) )\n",
    "\n",
    "d.backward()\n",
    "\n",
    "print( d.grad )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b09b666d-317b-4b43-b4b6-e961fdbdab04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Test, self).__init__()\n",
    "        self.fca = nn.Linear( 2, 2 )\n",
    "        self.fcb = nn.Linear( 2, 2 )\n",
    "        self.fcc = nn.Linear( 2, 2 )\n",
    "    \n",
    "    def forward(self, a, b, c ):\n",
    "        a = self.fca( a )\n",
    "        b = self.fcb( b ).detach()\n",
    "        c = self.fcc( c )\n",
    "        return a,  b + c\n",
    "\n",
    "a = torch.randn( ( 1,2 ), requires_grad = True )\n",
    "b = torch.randn( ( 1,2 ), requires_grad = False )\n",
    "c = torch.randn( ( 1,2 ), requires_grad = True )\n",
    "\n",
    "test = Test()\n",
    "\n",
    "sample_log_probs, reward = test( a, b, c )\n",
    "\n",
    "loss = (sample_log_probs * reward).mean()\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "for name,param in test.named_parameters():\n",
    "    print(name)\n",
    "    #print(param)\n",
    "    print(param.grad )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e85804b-f07d-40b7-994e-ec483585a2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained( model_id )\n",
    "bert = BertModel.from_pretrained( model_id )\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "print( \"sos_tokeni_d:\", sos_token_id )\n",
    "print( \"eos_token_id:\", eos_token_id )\n",
    "\n",
    "c = 3\n",
    "\n",
    "def calc_cnt_repeat( preds ):\n",
    "    # preds は、推論結果\n",
    "    # c window size\n",
    "    # tau temprature of gumbel softmax func.\n",
    "\n",
    "    B, T = preds.size()\n",
    "\n",
    "    tokens = preds\n",
    "    \n",
    "    cnt = torch.zeros( (B, T),  device=preds.device, dtype=torch.float16 )\n",
    "    for i in range( T ):\n",
    "        cntj = torch.zeros( (B),  device=preds.device, dtype=torch.float16 )\n",
    "        num_j = 0\n",
    "        for j in range( max( 0, i - c ), min(  T, i + c ) ):\n",
    "            if j != i:\n",
    "                tmp = ((tokens[:,i] != tokens[:,j] ).to(torch.float) ) * 10 \\\n",
    "                    +  ((tokens[:,i] == eos_token_id).to(torch.float) )  * 10 \\\n",
    "                    +  ((tokens[:,i] == tokenizer.pad_token_id).to(torch.float) )  * 10\n",
    "                        # tokens[:,i] と tokens[:,j] が同じで、tokens[:,i] が　eos、pad でなければ 0 その他は 10 以上の整数 \n",
    "                tmp = F.sigmoid( 10 - 100 * tmp ) # tokens[:,i]とtokens[:,j] が同じで eos pad ではないところだけ 1, あとは 0.\n",
    "                #if torch.any( tmp != 0 ):\n",
    "                #    print( \"tmp:\", tmp )\n",
    "                cntj = cntj + tmp # repeat の数。[B]  jについて足しこんでいる。\n",
    "                #if torch.any( cntj != 0 ):\n",
    "                #    print( \"cntj:\", cntj )\n",
    "                num_j = num_j + 1\n",
    "        cnt[:,i] = cntj / num_j # repeat の数。[B] i について足しこんでいる。\n",
    "        \n",
    "    #mask = cnt != 0\n",
    "    #print( \"cnt mask:\", cnt[mask] )\n",
    "    #return - torch.mean( cnt, dim = 1 ) # \n",
    "    return - cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5b7bb0-cdb2-4f59-a576-4beafa547625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "preds = torch.tensor( [[1000,1000,1000,1000,2000,300,4000],[1000,1000,1000,1000,1000,300,4000]] )\n",
    "\n",
    "reward_repeat = calc_cnt_repeat( preds )\n",
    "\n",
    "print( reward_repeat )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67a215-7bbe-4c7a-9ab6-ea5eebad2265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.autograd\n",
    "\n",
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import datetime\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "import gc\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import skimage.transform\n",
    "from collections import deque\n",
    "from typing import Sequence, Dict, Tuple, Union, Optional\n",
    "#from argparse import Namespace\n",
    "from dataclasses import dataclass, field\n",
    "from sacrebleu.metrics import BLEU\n",
    "from evaluate import load\n",
    "import jiwer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "#from timm.scheduler import CosineLRScheduler\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "\n",
    "#from transformers import AutoImageProcessor, AutoModel, AutoProcessor, CLIPVisionModel\n",
    "#from transformers import AutoTokenizer, CLIPVisionModel, AutoModelForCausalLM\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "from evaluate import load\n",
    "\n",
    "import util\n",
    "import levenshtein\n",
    "from nltk import bleu_score\n",
    "import ssl\n",
    "from torch.amp import autocast, GradScaler\n",
    "from collections import OrderedDict\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import json\n",
    "import collections\n",
    "#logging.getLogger('rouge_score.rouge_scorer').setLevel(logging.WARNING)\n",
    "#logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8b39e-3903-4572-8353-4335ed08319a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, dim=1):\n",
    "    return torch.logsumexp(x.float(), dim=dim).type_as(x)\n",
    "\n",
    "class DynamicCRF(nn.Module):\n",
    "    def __init__(self, num_embedding, low_rank=32, beam_size=64):\n",
    "        super().__init__()\n",
    "\n",
    "        #low_rank = num_embedding\n",
    "        self.E1 = nn.Embedding(num_embedding, low_rank)\n",
    "        self.E2 = nn.Embedding(num_embedding, low_rank)\n",
    "\n",
    "        self.vocb = num_embedding\n",
    "        self.rank = low_rank\n",
    "        self.beam = beam_size\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"vocab_size={}, low_rank={}, beam_size={}\".format(\n",
    "            self.vocb, self.rank, self.beam)\n",
    "\n",
    "    def forward(self, emissions, top_logits, top_indices, targets, masks, beam=None):\n",
    "        numerator = self._compute_score(emissions, targets, masks)\n",
    "        denominator = self._compute_normalizer(emissions, targets, masks, beam )\n",
    "        beam_probs = self._compute_normalizer2(top_logits, top_indices, targets, masks, beam)\n",
    "\n",
    "        #return numerator - denominator, beam_probs, all_preds, sample_log_probs, sampled_beam_idx\n",
    "        #return numerator - denominator, beam_probs, sampled_beam_idx\n",
    "        return numerator - denominator, beam_probs\n",
    "    \n",
    "    def forward_decoder(self, emissions, masks=None, beam=None):\n",
    "        return self._viterbi_decode(emissions, masks, beam)\n",
    "\n",
    "    def _compute_score(self, emissions, targets, masks=None):\n",
    "        batch_size, seq_len = targets.size()\n",
    "\n",
    "        emission_scores = emissions.gather(2, targets[:, :, None])[:, :, 0]  # B x T\n",
    "        transition_scores = (self.E1(targets[:, :-1]) * self.E2(targets[:, 1:])).sum(2)\n",
    "       \n",
    "        scores = emission_scores\n",
    "        scores[:, 1:] += transition_scores\n",
    "        \n",
    "        if masks is not None:\n",
    "            scores = scores * masks.type_as(scores)\n",
    "\n",
    "        return scores.sum(-1)\n",
    "        \n",
    "    def _compute_normalizer(self, emissions, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        if targets is not None:\n",
    "            #_emissions = emissions.scatter(2, targets[:, :, None], np.float('inf'))\n",
    "            _emissions = emissions.scatter(2, targets[:, :, None], float('inf'))\n",
    "            beam_targets = _emissions.topk(beam, 2)[1]\n",
    "            beam_emission_scores = emissions.gather(2, beam_targets)\n",
    "        else:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step.\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        for i in range(1, seq_len):\n",
    "            next_score = score[:, :, None] + beam_transition_matrix[:, i-1]\n",
    "            next_score = logsumexp(next_score, dim=1) + beam_emission_scores[:, i]\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i:i+1], next_score, score)\n",
    "            else:\n",
    "                score = next_score\n",
    "\n",
    "        return logsumexp(score, dim=1)\n",
    "    '''\n",
    "    def _compute_normalizer2(self, top_logits, top_indices, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = top_logits.size()[:2]\n",
    "        beam_emission_scores, beam_targets = top_logits, top_indices\n",
    "        beam_probs = F.softmax( top_logits, dim = 2 )\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step.\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        ## compute the normalizer in the log-space\n",
    "        scores = torch.zeros( ( batch_size, seq_len, self.beam ), dtype=torch.float, device = beam_transition_matrix.device )\n",
    "        scores[:,0,:] = beam_emission_scores[:,0]\n",
    "        for i in range(1, seq_len):\n",
    "            scores[:,i] = beam_emission_scores[:, i] + torch.sum(beam_transition_matrix[:, :, i, :], dim = 1 ) \n",
    "        \n",
    "        denominator1 = torch.logsumexp(scores, dim=2).type_as(scores)\n",
    "        \n",
    "        beam_log_probs = scores - denominator1.view(batch_size, seq_len, 1)\n",
    "\n",
    "        beam_probs = torch.exp(beam_log_probs)\n",
    "\n",
    "        return beam_probs\n",
    "    '''\n",
    "    def _compute_normalizer2(self, top_logits, top_indices, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = top_logits.size()[:2]\n",
    "        beam_emission_scores, beam_targets = top_logits, top_indices\n",
    "        beam_probs = F.softmax( top_logits, dim = 2 )\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step.\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        traj_scores = []\n",
    "        '''\n",
    "        # compute the normalizer in the log-space\n",
    "        #score = beam_emission_scores[:, 0]  # B x K\n",
    "        _score1 = beam_emission_scores[:, 0]  # B x K\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(_score1)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score1, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score = _score1 + beam_emission_scores[:, i] # bsz, beam\n",
    "\n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            #    score, index = _score, _index\n",
    "            score = _score\n",
    "\n",
    "        '''\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "\n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            #    score, index = _score, _index\n",
    "            score = _score\n",
    "        \n",
    "        \n",
    "        all_scores = traj_scores\n",
    "        all_scores.append( score )\n",
    "        all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 )\n",
    "\n",
    "        #print( \"size of all_scores:\", all_scores.size() )\n",
    "        \n",
    "        denominator1 = torch.logsumexp(all_scores, dim=2).type_as(all_scores)\n",
    "        \n",
    "        beam_log_probs = all_scores - denominator1.view(batch_size, seq_len, 1)\n",
    "\n",
    "        beam_probs = torch.exp(beam_log_probs)\n",
    "\n",
    "        return beam_probs\n",
    "    '''\n",
    "    def _viterbi_decode(self, emissions, masks=None, beam=None):\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        #print( \"score size:\", score.size() )\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return finalized_scores, finalized_tokens\n",
    "\n",
    "   \n",
    "    '''\n",
    "    def _viterbi_decode(self, emissions, masks=None, beam=None):\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        #print( \"score size:\", score.size() )\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "        all_scores = traj_scores\n",
    "        #all_scores.append( score )\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens0 = finalized_tokens\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        captions = finalized_tokens0\n",
    "\n",
    "        #print( \"captions size:\", captions.size() )\n",
    "\n",
    "        penalty = 1.2\n",
    "\n",
    "        all_scores2 = torch.stack( all_scores, dim = 0 ).transpose(0,1)\n",
    "        \n",
    "        #print( \"all_scores2 size:\", all_scores2.size() )\n",
    "\n",
    "        '''\n",
    "        for j in range( captions.size(1) - 1 ):\n",
    "            #print( \"j:\", j )\n",
    "            logits = all_scores2[:,j]\n",
    "            if penalty != 1.0:\n",
    "                for i in range(captions.shape[0]): # batchごとにループ\n",
    "                    for token_id in set(captions[i].tolist()): # 既に出現したユニークなトークン\n",
    "                        if logits[i, token_id] > 0:\n",
    "                            logits[i, token_id] /= penalty\n",
    "                        else:\n",
    "                            logits[i, token_id] *= penalty\n",
    "                    all_scores2[i,j] = logits[i]\n",
    "        '''\n",
    "\n",
    "        print( \"all_scores2 size:\", all_scores2.size() )\n",
    "        print( \"finalized_tokens0[:,:-1,None].expand( -1, -1, 1) size:\", finalized_tokens0[:,:-1,None].expand( -1, -1, 1).size() )\n",
    "        print( \"finalized_tokens0[:,:-1,None].expand( -1, -1, 1):\", finalized_tokens0[:,:-1,None].expand( -1, -1, 1) )\n",
    "        all_scores3 = torch.gather( all_scores2, 2, finalized_tokens0[:,:-1,None].expand( -1, -1, 1) )\n",
    "        scores2 = all_scores3 / penalty\n",
    "        all_scores4 = torch.scatter( all_scores2, 2, finalized_tokens0[:,:-1,None].expand( -1, -1, 1), scores2 )        \n",
    "\n",
    "\n",
    "        ## now running the back-tracing and find the best\n",
    "        all_scores = all_scores4.transpose(0,1)\n",
    "\n",
    "        finalized_tokens2, finalized_scores2 = [], []\n",
    "\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens2.append(best_index[:, None])\n",
    "        finalized_scores2.append(best_score[:, None])\n",
    "\n",
    "        #previous_index size: torch.Size([8, 1])\n",
    "        #finalized_scores2[-1] size: torch.Size([8, 1])\n",
    "        #idx size: torch.Size([])\n",
    "\n",
    "        for i, (idx, scs) in enumerate( zip(reversed(traj_tokens), reversed(all_scores))):        \n",
    "        #for i, idx, scs in enumerate(reversed(traj_tokens), reversed(all_scores)):\n",
    "            previous_index = finalized_tokens2[-1]\n",
    "            finalized_tokens2.append(idx.gather(1, previous_index))\n",
    "            finalized_scores2.append(scs.gather(1, previous_index))            \n",
    "            \n",
    "            #print( \"previous_index size:\", previous_index.size() )\n",
    "            #finalized_scores2.append(scs.gather(1, previous_index))\n",
    "            #print( \"finalized_scores2[-1] size:\", finalized_scores2[-1].size() )\n",
    "            #_, idx = finalized_scores2[-1].max( dim = 1  )\n",
    "            #if i == 0:\n",
    "            #    idx = idx[:,None]\n",
    "            #print( \"idx size:\", idx.size() )\n",
    "            #print( \"previous_index size:\", previous_index.size() )\n",
    "            #print( \"previous_index:\", previous_index )\n",
    "            #finalized_tokens2.append(idx.gather(0, previous_index))\n",
    "            \n",
    "            \n",
    "        finalized_tokens2.reverse()\n",
    "        finalized_tokens2 = torch.cat(finalized_tokens2, 1)\n",
    "        finalized_tokens2 = beam_targets.gather(2, finalized_tokens2[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores2.reverse()\n",
    "        finalized_scores2 = torch.cat(finalized_scores2, 1)\n",
    "        finalized_scores2[:, 1:] = finalized_scores2[:, 1:] - finalized_scores2[:, :-1]\n",
    "\n",
    "        #return finalized_scores, finalized_tokens, finalized_scores2, finalized_tokens2\n",
    "        return finalized_scores2, finalized_tokens2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485f4ed7-441f-4966-8b38-1ff113f9368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, crf_low_rank, crf_beam_size, dropout, padding_idx):\n",
    "        super(TopLayer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.padding_idx = padding_idx\n",
    "        print( \"in TopLayer:\" )\n",
    "        self.crf_layer = DynamicCRF(num_embedding = vocab_size, low_rank = crf_low_rank, \n",
    "                                    beam_size = crf_beam_size)\n",
    "\n",
    "        #self.one_more_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        #self.tgt_word_prj = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "        ## gae 学習用\n",
    "        self.linear_critical = nn.Linear(crf_beam_size, 1 )\n",
    "\n",
    "    #def forward(self, src_representation, src_input, tgt_input, is_training):\n",
    "    def forward(self, src_representation, top_logits, top_indices, src_input, tgt_input, is_training ):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        #assert src_input.size() == tgt_input.size()\n",
    "\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        #seqlen, bsz = src_input.size()\n",
    "        seqlen, bsz = src_input.shape[:2]\n",
    "\n",
    "        src_representation = F.dropout(src_representation, p=self.dropout, training=is_training)\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "\n",
    "        src = src_representation\n",
    "\n",
    "        #emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "        emissions = src_representation\n",
    "        #log_probs = torch.log_softmax(emissions, -1)\n",
    "        #assert log_probs.size() == torch.Size([seqlen, bsz, self.vocab_size])\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz x src_len x vocab_size]\n",
    "        #emission_mask = ~tgt_input.eq(self.padding_idx) # [bsz x src_len] #pad のところは 0 padでないところが 1\n",
    "        emission_mask = torch.ones_like( tgt_input, dtype=torch.bool ) #全部　pad でないとして 1\n",
    "        batch_crf_loss, top_probs = self.crf_layer(emissions, top_logits, top_indices, tgt_input, emission_mask) # [bsz]\n",
    "        critical_value = self.linear_critical( top_probs )\n",
    "        #critical_value = torch.zeros( ( 1,1,1) )\n",
    "        batch_crf_loss = - batch_crf_loss\n",
    "        assert batch_crf_loss.size() == torch.Size([bsz])\n",
    "        return batch_crf_loss, top_probs, critical_value[:,:,0]\n",
    "\n",
    "    def decoding(self, src_representation, src_input):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        seqlen, bsz = src_input.size()\n",
    "\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "        src = src_representation\n",
    "\n",
    "        emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        _, finalized_tokens = self.crf_layer.forward_decoder(emissions)\n",
    "        assert finalized_tokens.size() == torch.Size([bsz, seqlen])\n",
    "        return finalized_tokens\n",
    "\n",
    "    def length_ratio_decoding(self, src_representation, src_input, length_ratio):\n",
    "        '''\n",
    "            src_representation : 1 x seqlen x embed_dim\n",
    "            src_input : 1 x seqlen\n",
    "        '''\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        seqlen, bsz = src_input.size()\n",
    "\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "        src = src_representation\n",
    "\n",
    "        emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        valid_len = int(seqlen * length_ratio) + 1\n",
    "        valid_emissions = emissions[:, :valid_len+1,:]\n",
    "        _, finalized_tokens = self.crf_layer.forward_decoder(valid_emissions)\n",
    "        return finalized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933b19f5-35e9-4e9b-9bdb-d65bd36b9729",
   "metadata": {},
   "outputs": [],
   "source": [
    "crf_low_rank = 32\n",
    "crf_beam_size = 256\n",
    "top_dropout = 0.0\n",
    "tgt_padding_idx = 0\n",
    "vocab_size = 30000\n",
    "dim_embedding = 1024\n",
    "toplayer = TopLayer( vocab_size, dim_embedding, crf_low_rank, crf_beam_size, top_dropout, tgt_padding_idx )\n",
    "\n",
    "#images = torch.randint( 0, 255, size = (10,3,256,256) )\n",
    "#logits, critical_values = model( imgs )\n",
    "logits = torch.randn( ( 8, 20, vocab_size), requires_grad = True )\n",
    "src_representation = logits\n",
    "#src_input = imgs\n",
    "src_input = logits\n",
    "tgt_input = torch.randint( 0, vocab_size, ( 8, 97 ) )\n",
    "\n",
    "top_logits, top_indices = torch.topk( logits, 256, 2 )\n",
    "\n",
    "#def forward(self, src_representation, top_logits, top_indices, src_input, tgt_input, is_training, sample_t):\n",
    "#train_batch_crf_loss, top_probs, critical_value = toplayer( src_representation, \\\n",
    "#                                                             top_logits, top_indices, src_input, tgt_input, \\\n",
    "#                                                             is_training = True )\n",
    "finalized_scores2, finalized_tokens2 = toplayer.crf_layer.forward_decoder(logits)\n",
    "\n",
    "print( \"finalized_scores2 size:\", finalized_scores2.size() )\n",
    "print( \"finalized_tokens2 size:\", finalized_tokens2.size() )\n",
    "\n",
    "#print( \"size of train_batch_crf_loss:\", train_batch_crf_loss.size() )\n",
    "#print( \"size of top_probs:\", top_probs.size() )\n",
    "#print( \"size of critical_value:\", critical_value.size() )\n",
    "\n",
    "'''\n",
    "#a, b, c, d, e = model.toplayer( src_representation, \\\n",
    "#                                                             top_logits, top_indices, src_input, tgt_input, \\\n",
    "#                                                             is_training = True, sample_t = 'normal' )\n",
    "#print( outputs )\n",
    "print( \"size of top_probs:\", top_probs.size() )\n",
    "print( \"grad_fn of top_probs:\", top_probs.grad_fn )\n",
    "top_probs.mean().backward()\n",
    "#print( \"size of preds:\", preds.size() )\n",
    "#print( \"size of sample_log_probs:\", sample_log_probs.size() )\n",
    "print( \"size of sampled_beam_idx:\", sampled_beam_idx.size() )\n",
    "\n",
    "train_batch_crf_loss, top_probs, critical_value = toplayer( src_representation, \\\n",
    "                                                             top_logits, top_indices, src_input, tgt_input, \\\n",
    "                                                             is_training = True, sample_t = 'greedy' )\n",
    "\n",
    "print( \"size of train_batch_crf_loss:\", train_batch_crf_loss.size()\n",
    "print( \"size of top_probs:\", top_probs.size() )\n",
    "print( \"size of critical_value:\", critical_value.size() )\n",
    "#print( \"size of sample_log_probs:\", sample_log_probs.size() )\n",
    "#print( \"size of sampled_beam_idx:\", sampled_beam_idx.size() )\n",
    "\n",
    "#print( \"topk_probs:\", topk_probs )\n",
    "masks_nan = torch.isnan( topk_probs)\n",
    "print( \"topk_probs nan:\", topk_probs[masks_nan])\n",
    "masks_inf = torch.isinf( topk_probs )\n",
    "print( \"topk_probs inf:\", topk_probs[masks_inf] )\n",
    "masks_zeroika = topk_probs <= 0.0\n",
    "print( \"topk_probs zero:\", topk_probs[masks_zeroika] )\n",
    "\n",
    "rlcriterion = RLCriterion(task=tokenizer, algo = \"1r\", reward_t = \"ordinary\", decode_t = \"no-pad\", \n",
    "                          device = torch.device(\"cpu\"), sentence_level_metric=\"bleu\", b_top_k = 0, \n",
    "                          itern=10, topk = 10 )\n",
    "\n",
    "sample_log_probs, reward_ord, reward_repeat, reward_length, probs, preds = rlcriterion( topk_probs, topk_indicies, captions, b = 0.0, sources = None )\n",
    "\n",
    "print( \"sample_log_probs:\", sample_log_probs.size() )\n",
    "print( \"probs:\", probs )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe1f87-f886-44b3-90bd-6fd9bd662bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "emissions = torch.randn( (2, 3, 5) )\n",
    "#probs = F.softmax( logits,  dim = 2 )\n",
    "#preds = torch.argmax( logits, dim = 2 )\n",
    "\n",
    "#top_logits, top_indices = torch.topk( logits, 3, 2 )\n",
    "#top_probs = F.softmax( top_logits, 2 )\n",
    "#top_preds = torch.argmax( top_probs, 2 )\n",
    "#preds2 = torch.gather( top_indices, 2, top_preds.unsqueeze(-1) ).squeeze( -1 )\n",
    "\n",
    "print( \"emissions:\", emissions )\n",
    "\n",
    "#preds = torch.argmax( emissions, dim = 2 )\n",
    "\n",
    "penalty = 1.2\n",
    "\n",
    "scores, preds = torch.max( emissions, 2 )\n",
    "\n",
    "print( \"scores size:\", scores.size() )\n",
    "print( \"preds size:\", preds.size() )\n",
    "\n",
    "masks = emissions == scores[:,:,None]\n",
    "\n",
    "print( \"masks size:\", masks.size() )\n",
    "\n",
    "print( \"masks[0]:\", masks[0] )\n",
    "\n",
    "\n",
    "print( \"preds[0]:\", preds[0] )\n",
    "print( \"masks[0,:]:\",masks[0,:] )\n",
    "                       #s,b, v\n",
    "masks = masks.permute( 1, 0, 2 )\n",
    "new_mask = torch.zeros( (  masks.size(1), masks.size(2)) ).bool()\n",
    "new_masks = torch.zeros( ( masks.size(0), masks.size(1), masks.size(2)) ).bool()\n",
    "for i, mask in enumerate( masks ):\n",
    "    new_mask = torch.logical_or( mask,  new_mask  )\n",
    "    print( \"new_mask:\", new_mask )\n",
    "    print( \"new_mask size:\", new_mask.size() )\n",
    "    new_masks[i] = new_mask\n",
    "\n",
    "new_masks = new_masks.transpose(0,1)\n",
    "\n",
    "print( \"new_masks size:\", new_masks.size() )\n",
    "\n",
    "first_true_mask = ( new_masks.int().cumsum(dim = 1 ) == 1 ) & new_masks\n",
    "\n",
    "new_masks = new_masks & ( ~first_true_mask )\n",
    "\n",
    "#new_masks = new_masks[:,None,:].expand( ( -1, masks.size(0), -1 ) )\n",
    "\n",
    "p_masks = emissions > 0\n",
    "m_masks = emissions < 0\n",
    "p_new_masks = p_masks & new_masks\n",
    "m_new_masks = p_masks & new_masks\n",
    "\n",
    "half = emissions[p_new_masks] / penalty\n",
    "emissions[p_new_masks] = half\n",
    "\n",
    "twice = emissions[m_new_masks] * penalty\n",
    "emissions[m_new_masks] = twice\n",
    "\n",
    "#emissions2 = emissions.clone()\n",
    "#emissions2[new_masks] = half\n",
    "\n",
    "print( \"new_masks[1,:,1]:\", new_masks[1,:,0] )\n",
    "\n",
    "print( emissions[1,0,0] )\n",
    "#print( emissions2[1,0,0] )\n",
    "print( emissions[1,1,0] )\n",
    "#print( emissions2[1,1,0] )\n",
    "\n",
    "#print( \"new_emissions:\", emissions )\n",
    "\n",
    "#beam_emission_scores, beam_targets = emissions.topk(3, 2)\n",
    "\n",
    "#print( \"beam_emission_scores:\", beam_emission_scores )\n",
    "#print( \"beam_targets:\", beam_targets )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c736f83e-137d-4ab5-850d-27259a3685c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print( emissions[1,0,0] )\n",
    "print( emissions2[1,0,0] )\n",
    "print( emissions[1,1,0] )\n",
    "print( emissions2[1,1,0] )\n",
    "print( emissions[1,2,0] )\n",
    "print( emissions2[1,2,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060bc9b2-9913-4af2-8e84-e9ddb49527af",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "space_token_id = tokenizer.encode( [ \" \" ] )[1]\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "def calc_cnt_repeat( preds ):\n",
    "\n",
    "    B, T = preds.size()\n",
    "\n",
    "    repeat_count = torch.zeros( ( B, vocab_size ), device = preds.device, dtype=torch.float )\n",
    "    print( \"3 preds:\", preds )\n",
    "    for i, pred in enumerate( preds ):\n",
    "        repeat_count0 = torch.bincount( pred )\n",
    "        repeat_count[i,:len(repeat_count0)] = repeat_count0 \n",
    "\n",
    "    print( \"ini repeat_count:\", repeat_count )\n",
    "    masks = repeat_count != 0\n",
    "    print( \"not 0 repepat_count:\", repeat_count[masks] )\n",
    "    #thresh_masks = repeat_count >= 2\n",
    "    #print( torch.sum( repeat_count * thresh_masks ) )\n",
    "    repeat_count[:,pad_token_id] = 0\n",
    "    #print( repeat_count )\n",
    "    #pos = repeat_count[0].tolist().index( 1  )\n",
    "    #pos = [i for i, x in enumerate(repeat_count[masks].tolist()) if isinstance(2, float)]\n",
    "    #print( \" 2 pos:\", pos )\n",
    "    #thresh_masks = repeat_count >= 2\n",
    "    #print( torch.sum( repeat_count * thresh_masks ) )\n",
    "    #print( \"repeat_count[:,eos_token_id]:\", repeat_count[:,eos_token_id] )\n",
    "    print( \"eos_token_id:\",eos_token_id )\n",
    "    repeat_count[:,eos_token_id] = 0\n",
    "    repeat_count[:,cls_token_id] = 0\n",
    "    repeat_count[:,sep_token_id] = 0\n",
    "    #print( \"repeat_count[:,eos_token_id]:\", repeat_count[:,eos_token_id] )\n",
    "    #print( repeat_count )\n",
    "    #thresh_masks = repeat_count >= 2\n",
    "    #print( torch.sum( repeat_count * thresh_masks ) )\n",
    "    #repeat_count[:,space_token_id] = 0\n",
    "    thresh_masks = repeat_count >= 2\n",
    "    print( torch.sum( repeat_count * thresh_masks ) )\n",
    "    thresh_masks = repeat_count >= 2\n",
    "    repeat_count = repeat_count * thresh_masks\n",
    "\n",
    "    repeat = torch.sum( repeat_count, dim = 1 )\n",
    "    \n",
    "    return - repeat[:,None].expand( -1, T ).float()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69658cc3-14f0-4a20-a2a2-fe93e1eb6f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = \"This This This is is\"\n",
    "\n",
    "pred = tokenizer.encode( caption )\n",
    "print( \"1 pred:\", pred )\n",
    "pred.append( 2 )\n",
    "pred.append( 2 )\n",
    "pred.append( 0 )\n",
    "pred.append( 0 )\n",
    "pred.append( 0 )\n",
    "print( \"2 pred:\", pred )\n",
    "outputs = calc_cnt_repeat( torch.tensor([pred]) )\n",
    "\n",
    "print( outputs[0,0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6240db4-4e8f-4bed-af1b-32a00c4adac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded = tokenizer.decode( [101] )\n",
    "print( decoded )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be8161a-7cdc-4f49-872e-35e22832ffe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = torch.randn( ( 2, 10, 30000 ) )\n",
    "bsz, seq_len, vocab_size = logits.size() \n",
    "probs = F.softmax( logits, 2 )\n",
    "preds = torch.argmax( logits, 2 )\n",
    "\n",
    "print( preds )\n",
    "\n",
    "top_logits, top_indices = torch.topk( logits, 256, 2 )\n",
    "\n",
    "tensor = torch.full((bsz, seq_len, vocab_size), float('-inf'))\n",
    "\n",
    "tensor = torch.scatter( tensor, 2, top_indices, top_logits )\n",
    "\n",
    "preds2 = torch.argmax( tensor, 2 )\n",
    "\n",
    "print( preds2 )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f95b00b-61d4-49ac-84b1-d85b5d39c002",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def count_ngram_frequency( token_list, n ):\n",
    "    ngrams1 = [token_list[i:i+n] for i in range(len(token_list) - n + 1)]\n",
    "    print( \"ngrams1:\", ngrams1 )\n",
    "    ngrams = []\n",
    "    for i in range( len( token_list) - n + 1 ):\n",
    "        narabi = \"\"\n",
    "        for j in range( i, i + n ):\n",
    "            narabi = narabi + str(token_list[j] )\n",
    "        ngrams.append( narabi )\n",
    "        print( ngrams )\n",
    "    print( \"ngrams:\", ngrams )\n",
    "    \n",
    "    frequency = Counter( ngrams )\n",
    "\n",
    "    return frequency\n",
    "\n",
    "token_list = [1,2,3,2,3,2,3,4,1,2,3,2,3]\n",
    "#token_list = \"1232323412323\"\n",
    "n = 3\n",
    "ngram_counts = count_ngram_frequency( token_list, n )\n",
    "\n",
    "print( f\"token_list: {token_list}\" )\n",
    "print( f\"N: {n}\" )\n",
    "print( \"N-gramの重複回数:\" )\n",
    "for ngram, count in ngram_counts.items():\n",
    "    print( f\"{ngram}: {count}回\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501e60b-b066-4b4d-9159-1f2f5383b3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# 例\n",
    "c = Counter(['a', 'b', 'b', 'c', 'c', 'c'])\n",
    "\n",
    "for element in c:\n",
    "    print(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044f3795-1c48-4427-99b7-21456f1c6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#tokens = [\"私\", \"は\", \"猫\", \"が\", \"好き\", \"です\", \"私\", \"は\", \"猫\", \"が\"]\n",
    "tokens = [1,2,3,2,3,2,3,4,1,2,3,2,3]\n",
    "n = 2  # bigramの場合\n",
    "\n",
    "# n-gramの生成\n",
    "ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "\n",
    "# 重複回数のカウント\n",
    "counts = Counter(ngrams)\n",
    "\n",
    "print(counts)\n",
    "for count in counts.values():\n",
    "    print( count )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f558e32f-4ca9-411f-9bbb-676524655a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ngram_repeat(  preds ):\n",
    "\n",
    "    bsz, seq_len = preds.size()\n",
    "        \n",
    "    ngram_cnt = torch.zeros( (bsz), device = preds.device, dtype = torch.float )\n",
    "    for n in range( 2, 4 ):\n",
    "        for i, pred in enumerate( preds ):\n",
    "            pred = pred.tolist()\n",
    "            print( \"pred:\", pred )\n",
    "            ngrams = zip(*[pred[i:] for i in range(n)])\n",
    "            counts = Counter(ngrams)\n",
    "            print( \"counts:\", counts )\n",
    "            count_sum = 0\n",
    "            for count in counts.values():\n",
    "                print( count )\n",
    "                if count >= 2:\n",
    "                    count_sum = count_sum + count\n",
    "            print( \"count_sum:\", count_sum )\n",
    "            ngram_cnt[i] = ngram_cnt[i] + count_sum\n",
    "\n",
    "    return - ngram_cnt[:,None].expand( -1, seq_len )\n",
    "\n",
    "\n",
    "tokens = torch.tensor( [[1,2,3,2,3,2,3,4,1,2,3,2,3]] )\n",
    "\n",
    "a = calc_ngram_repeat( tokens )\n",
    "print( a )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bba58e-eee8-4158-8654-348b3662d1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_ngram_repeat_fast( preds):\n",
    "    bsz, seq_len = preds.size()\n",
    "    # 合計を保持するテンソル (GPU)\n",
    "    ngram_cnt = torch.zeros(bsz, device=preds.device, dtype=torch.float)\n",
    "    \n",
    "    # n-gramの範囲 (2から3)\n",
    "    for n in range(2, 4):\n",
    "        if seq_len < n:\n",
    "            continue\n",
    "            \n",
    "        # 1. すべての n-gram を一括抽出\n",
    "        # shape: (bsz, seq_len - n + 1, n)\n",
    "        ngrams = preds.unfold(dimension=1, size=n, step=1)\n",
    "        \n",
    "        # 2. n-gram 同士を比較するために次元を拡張\n",
    "        # left:  (bsz, num_ngrams, 1, n)\n",
    "        # right: (bsz, 1, num_ngrams, n)\n",
    "        left = ngrams.unsqueeze(2)\n",
    "        right = ngrams.unsqueeze(1)\n",
    "        \n",
    "        # 3. ブロードキャストを利用して全ペアの一致を確認\n",
    "        # match_matrix: (bsz, num_ngrams, num_ngrams)\n",
    "        # 各 n-gram が他のどの位置の n-gram と一致するかを bool で保持\n",
    "        match_matrix = (left == right).all(dim=-1)\n",
    "        \n",
    "        # 4. 各 n-gram の出現回数をカウント\n",
    "        # 各行を足すと、その n-gram がシーケンス内に何回現れたかが出る\n",
    "        counts = match_matrix.sum(dim=-1) # (bsz, num_ngrams)\n",
    "        \n",
    "        # 5. 閾値 (repeat_thresh) 以上のカウントのみを合計\n",
    "        # 重複してカウントしないよう、各一致ペアを適切に処理\n",
    "        # 元のロジック(Counter)に合わせる場合:\n",
    "        # 閾値以上の要素を抽出し、その出現回数を合計する\n",
    "        # ただし、単純に sum(counts >= thresh) だと重複が発生するため、\n",
    "        # 非ゼロ要素の数で調整するか、ユニークな n-gram ごとに判定する必要があります。\n",
    "        \n",
    "        # 下記は元の Counter ロジックをベクトル化した近似（厳密なユニーク処理）:\n",
    "        for b in range(bsz):\n",
    "            # 行ごとにユニークな n-gram とそのカウントを取得\n",
    "            unique_ngrams, counts_per_ngram = torch.unique(ngrams[b], dim=0, return_counts=True)\n",
    "            # 閾値以上のカウントを合計\n",
    "            mask = counts_per_ngram >= 2\n",
    "            ngram_cnt[b] += counts_per_ngram[mask].sum().float()\n",
    "\n",
    "    return - ngram_cnt[:, None].expand(-1, seq_len)\n",
    "\n",
    "tokens = torch.tensor( [[1,2,3,2,3,2,3,4,1,2,3,2,3]] )\n",
    "\n",
    "a = calc_ngram_repeat_fast( tokens )\n",
    "print( a )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd84de9-8573-4e47-9cde-5debf4bd9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones( (2, 3 ) )\n",
    "a = torch.tensor( [[0.0,0,0],[0,0,10000]] )\n",
    "b = a.mean()\n",
    "\n",
    "print( b )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee62e125-a738-4b3e-a05e-7da5e059e8f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993acc2e-383b-42f6-9243-b7eb86014cea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
