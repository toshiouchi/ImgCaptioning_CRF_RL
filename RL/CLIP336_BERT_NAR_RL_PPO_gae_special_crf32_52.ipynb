{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl7yR7P3CTTj"
   },
   "source": [
    "### ライブラリの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7IQqCE7mCeD"
   },
   "source": [
    "###モジュールのインポートとGoogleドライブのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLEMbaPJOs9v",
    "outputId": "f47ad633-eb58-49cd-8893-914d53aa4475"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.autograd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import datetime\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "import gc\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import skimage.transform\n",
    "from collections import deque\n",
    "from typing import Sequence, Dict, Tuple, Union, Optional\n",
    "#from argparse import Namespace\n",
    "from dataclasses import dataclass, field\n",
    "from sacrebleu.metrics import BLEU\n",
    "from evaluate import load\n",
    "import jiwer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "#from timm.scheduler import CosineLRScheduler\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "\n",
    "#from transformers import AutoImageProcessor, AutoModel, AutoProcessor, CLIPVisionModel\n",
    "#from transformers import AutoTokenizer, CLIPVisionModel, AutoModelForCausalLM\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "from evaluate import load\n",
    "\n",
    "import util\n",
    "import levenshtein\n",
    "from nltk import bleu_score\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "import ssl\n",
    "from torch.amp import autocast, GradScaler\n",
    "from collections import OrderedDict\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import json\n",
    "import collections\n",
    "from collections import Counter\n",
    "import plotly\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "fn = bleu_score.SmoothingFunction().method7\n",
    "import time\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "#logging.getLogger('rouge_score.rouge_scorer').setLevel(logging.WARNING)\n",
    "#logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5jwqxtGH7JR"
   },
   "source": [
    "### 位置エンコーディングの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja_g99AUIJTF"
   },
   "source": [
    "### Transformerデコーダの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siyThdM4Icll"
   },
   "source": [
    "### CaptioningTransformerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "a_token_id = tokenizer.encode( [ \"a\" ] )[1]\n",
    "the_token_id = tokenizer.encode( [ \"the\" ] )[1]\n",
    "and_token_id = tokenizer.encode( [ \"and\" ] )[1]\n",
    "in_token_id = tokenizer.encode( [ \"in\" ] )[1]\n",
    "we_token_id = tokenizer.encode( [ \"we\" ] )[1]\n",
    "i_token_id = tokenizer.encode( [ \"i\" ] )[1]\n",
    "he_token_id = tokenizer.encode( [ \"he\" ] )[1]\n",
    "she_token_id = tokenizer.encode( [ \"she\" ] )[1]\n",
    "it_token_id = tokenizer.encode( [ \"it\" ] )[1]\n",
    "they_token_id = tokenizer.encode( [ \"they\" ] )[1]\n",
    "period_token_id = tokenizer.encode( [ \".\" ] )[1]\n",
    "comma_token_id = tokenizer.encode( [ \",\" ] )[1]\n",
    "dbl_token_id = tokenizer.encode( [ '\"' ] )[1]\n",
    "sgl_token_id = tokenizer.encode( [ \"'\" ] )[1]\n",
    "\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "class ComputeReward(nn.Module):\n",
    "    def __init__(self, device, reward_t = 'ordinary', decode_t = 'ordinary', sentence_level_metric=\"bleu\", \n",
    "                 repeat_thresh = [4,2,2,2], repeat_weight = [0.5, 1, 1, 2], cider_coef = 1.0, rouge_coef = 1.0, clip_coef = 2.0, \n",
    "                 bert_coef = 1.0, use_amp = True ):\n",
    "        super().__init__()\n",
    "        self.metric = sentence_level_metric\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tgt_lang = \"en\"\n",
    "        self.device = device\n",
    "\n",
    "        if sentence_level_metric =='special':\n",
    "            #self.bleu = BLEU(effective_order=\"True\")\n",
    "            self.scorer = Cider()\n",
    "            #self.meteor = load('meteor')\n",
    "            self.rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "            self.bert = load('bertscore')\n",
    "            self.metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "            for param in self.metric.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.spider = Spice()\n",
    "        elif sentence_level_metric == 'bleu':\n",
    "            self.bleu = BLEU(effective_order=\"True\")\n",
    "        elif sentence_level_metric == 'meteor':\n",
    "            self.meteor = load('meteor')\n",
    "        elif sentence_level_metric == 'rouge':\n",
    "            self.rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        elif sentence_level_metric == 'cider':\n",
    "            self.scorer = Cider() \n",
    "        elif sentence_level_metric == 'ter':\n",
    "            #self.ter = load('ter')\n",
    "            pass\n",
    "        elif sentence_level_metric == 'bert':\n",
    "            #self.bert = load('bertscore')\n",
    "            pass\n",
    "        elif sentence_level_metric == 'bleurt':\n",
    "            #self.bleurt = load('bleurt', module_type='metric', checkpoint='bleurt-large-128')\n",
    "            pass\n",
    "        elif self.metric == \"comet\":\n",
    "            model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "            self.comet = load_from_checkpoint(model_path)\n",
    "        self.reward_t = reward_t\n",
    "        self.repeat_thresh = repeat_thresh\n",
    "        self.repeat_weight = repeat_weight\n",
    "        self.decode_t = decode_t\n",
    "        self.cider_coef = cider_coef\n",
    "        self.rouge_coef = rouge_coef\n",
    "        self.clip_coef = clip_coef\n",
    "        self.bert_coef = bert_coef\n",
    "        self.use_amp = use_amp\n",
    "    \n",
    "    def _compute_reward_ord(self, preds, targets, imgs2, sources=None):\n",
    "        \"\"\"\n",
    "        Compute reward metric for a batch of prediction and target sentences\n",
    "        \"\"\"\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        # detokenize (convert to str) preds & targets\n",
    "        if self.decode_t == 'no-endoftext':\n",
    "            preds_str = [self.tokenizer.decode(\n",
    "                [pred[i] for i in range( 1,  len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                ) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(\n",
    "                [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                ) for target in targets]\n",
    "        elif self.decode_t == 'no-pad':\n",
    "            preds_str = [self.tokenizer.decode(\n",
    "                [ i for i in pred \\\n",
    "                 if i != pad_token_id \\\n",
    "                 and i != eos_token_id ] \\\n",
    "                ) for pred in preds]\n",
    "            preds_str2 = [self.tokenizer.decode(\n",
    "                [ i for i in pred \\\n",
    "                 if  i != eos_token_id  ] \\\n",
    "                 , skip_special_tokens = True ) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(\n",
    "                [ i for i in target \\\n",
    "                if i != pad_token_id \\\n",
    "                 and i != eos_token_id ]       \n",
    "                ) for target in targets]\n",
    "        else:\n",
    "            preds_str = [self.tokenizer.decode(pred) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(target) for target in targets]\n",
    "        sources_str = [self.tokenizer.decode(source, ref=\"src\") for source in sources] if sources is not None else None\n",
    "        #print( \"preds size:\", preds.size() )\n",
    "        #print( \"targets size:\", targets.size() )\n",
    "        \n",
    "        #print(f'1st target sent: {targets_str[0]}')\n",
    "        #print(f'1st pred sent: {preds_str[0]}')\n",
    "\n",
    "        # compute reward metric\n",
    "        seq_len = preds.shape[1]\n",
    "\n",
    "        if self.metric == 'special':\n",
    "            #reward_bleu = [[self.bleu.sentence_score(pred, [target]).score] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward_bleu = [[bleu_score.sentence_bleu( target, pred, smoothing_function=fn)] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward_bleu = torch.tensor(reward_bleu).to(self.device) / 100.0\n",
    "            #wer_scores = [jiwer.wer(target, pred) for pred, target in zip(targets_str, preds_str)]\n",
    "            #reward = [[score] * seq_len for score in wer_scores]\n",
    "            #reward_wer = - torch.tensor( reward ).to(self.device)\n",
    "            #start_time = time.time()\n",
    "            pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "            target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "            score, scores = self.scorer.compute_score(target_dict, pred_dict)\n",
    "            reward_cider = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            #end_time = time.time()\n",
    "            #print( \"cider time:\", end_time - start_time )\n",
    "            #start_time = time.time()\n",
    "            reward_rouge = [[self.rougeL.score(target, pred)['rougeL'][0]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            reward_rouge = torch.tensor( reward_rouge ).to( self.device )\n",
    "            #end_time = time.time()\n",
    "            #print( \"rouge time:\", end_time - start_time )\n",
    "            #start_time = time.time()\n",
    "            with autocast(str(self.device),enabled=self.use_amp):\n",
    "                with torch.no_grad():\n",
    "                    #clip_scores = [[self.metric( img2, pred).detach()] * seq_len for img2, pred in zip( imgs2, preds_str2 )]\n",
    "                    #tmp = self.metric( imgs2, preds_str2 )\n",
    "                    #print( \"tmp size:\", tmp.size() )\n",
    "                    #clip_scores = [[self.metric( imgs2, preds_str2 ).detach()] * seq_len ]\n",
    "                    processed = self.metric.processor(text=preds_str2, images=imgs2, return_tensors=\"pt\", padding=True, \\\n",
    "                                                      truncation=True, max_length=77 ).to(self.device)\n",
    "                    outputs = self.metric.model(**processed)\n",
    "                    # 特徴量の正規化\n",
    "                    image_features = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                    text_features = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                    # 3. コサイン類似度を一括計算 (100倍してマイナスをカットするのが CLIPScore の定義)\n",
    "                    # 各画像ペアの個別スコア (Batch Size,) が得られる\n",
    "                    individual_scores = torch.clamp( (image_features * text_features).sum(axis=-1), min=0)\n",
    "                    #print( \"individual_scores size:\", individual_scores.size() )\n",
    "                    clip_scores = individual_scores[:,None].expand( -1, seq_len ).to( self.device ) / 100.0\n",
    "                    #print( \"clip_scores size:\", clip_scores.size() )\n",
    "                    reward_clip = clip_scores\n",
    "                    #end_time = time.time()\n",
    "                    #print( \"clip time:\", end_time - start_time )\n",
    "                    #start_time = time.time()\n",
    "                    bert_scores = self.bert.compute(predictions=preds_str, references=targets_str, use_fast_tokenizer=True, \\\n",
    "                                            model_type=model_name, lang='en',  device=self.device)['f1']\n",
    "                    reward_bert = torch.tensor( bert_scores )[:,None].expand( -1, seq_len ).to( self.device )\n",
    "                    #end_time = time.time()\n",
    "                    #print( \"bert time:\", end_time - start_time )\n",
    "            #image = torch.randint(255, (3, 224, 224), generator=torch.Generator().manual_seed(42))\n",
    "            #score = metric(image, \"a photo of a cat\")\n",
    "            #average_score, scores = self.spice.compute_score(target_dict, pred_dict)\n",
    "            #print( \"average_score:\", average_score ) \n",
    "            #reward_spice = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            #meteor_scores = [self.meteor.compute(predictions=[preds], references=[targets])['meteor'] for preds, targets in zip(preds_str, targets_str)]\n",
    "            #reward_meteor = [[score] * seq_len for score in meteor_scores]\n",
    "            #reward_meteor = torch.tensor( reward_meteor ).to( self.device )\n",
    "            #reward = reward_bleu + reward_cider\n",
    "            #print( \"reward_bleu:\", reward_bleu )\n",
    "            #print( \"reward_cider:\", reward_cider )\n",
    "            #print( \"reward_meteor:\", reward_meteor )\n",
    "            #reward = reward_meteor + reward_cider\n",
    "            #print( \"self.bert_coef:\", self.bert_coef )\n",
    "            #print( \"reward_bert:\", reward_bert )\n",
    "            #reward = self.clip_coef * reward_clip + self.bert_coef * reward_bert\n",
    "            reward = self.cider_coef * reward_cider + self.rouge_coef * reward_rouge \\\n",
    "                + self.clip_coef * reward_clip + self.bert_coef * reward_bert\n",
    "            reward2 = reward_cider + reward_rouge + reward_bert + reward_clip\n",
    "            #score, scores = self.spider.compute_score(gts, res)\n",
    "            #reward_rouge = [[self.spider.compute_score(target, pred)[1]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward2 = reward_rouge + reward_bert + reward_clip\n",
    "            #reward = self.rouge_coef * reward_rouge + self.cider_coef * reward_cider + self.clip_coef * reward_clip \\\n",
    "            #    + self.bert_coef * reward_bert\n",
    "            #reward = reward_bleu + reward_cider\n",
    "            #reward = reward_bleu + reward_cider + reward_meteor\n",
    "            #reward = reward_wer + reward_cider\n",
    "            #reward = reward_bleu + reward_wer + reward_cider\n",
    "            #reward = reward_bleu + reward_wer + reward_cider + reward_meteor\n",
    "            #print( \"wer:\", reward_wer )\n",
    "            #print( \"cider:\", reward_cider )\n",
    "        elif self.metric == \"bleu\":\n",
    "            reward = [[self.bleu.sentence_score(pred, [target]).score] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "        \n",
    "        elif self.metric == \"meteor\":\n",
    "            meteor_scores = [self.meteor.compute(predictions=[preds], references=[targets])['meteor'] for preds, targets in zip(preds_str, targets_str)]\n",
    "            reward = [[score] * seq_len for score in meteor_scores]\n",
    "            \n",
    "        elif self.metric == \"rouge\":\n",
    "            #rouge_scores = self.rouge.compute(predictions=preds_str, references=targets_str, use_aggregator=False )['rougeL']\n",
    "            reward = [[self.rouge.score(target, pred)['rougeL'][0]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "        \n",
    "        elif self.metric == \"wer\":\n",
    "            wer_scores = [jiwer.wer(target, pred) for pred, target in zip(targets_str, preds_str)]\n",
    "            reward = [[score] * seq_len for score in wer_scores]\n",
    "            reward = - torch.tensor( reward )\n",
    "\n",
    "        elif self.metric == 'cider':\n",
    "            pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "            target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "            #score, all_scores = parallel_cider_evaluation( target_dict, pred_dict )\n",
    "            score, scores = self.scorer.compute_score(target_dict, pred_dict)\n",
    "            reward = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            \n",
    "        #elif self.metric == \"bert\":\n",
    "        #    bert_scores = self.bert.compute(predictions=preds_str, references=targets_str, lang='en')['f1']\n",
    "        #    reward = [[score] * seq_len for score in bert_scores]\n",
    "\n",
    "        #elif self.metric == \"bleurt\":\n",
    "        #    bleurt_scores = self.bleurt.compute(predictions=preds_str, references=targets_str)['scores']\n",
    "        #    reward = [[score] * seq_len for score in bleurt_scores]\n",
    "            \n",
    "        elif self.metric == \"comet\":\n",
    "            data = [{\"src\": source, \"mt\": pred, \"ref\": target} for source, pred, target in zip(sources_str, preds_str, targets_str)]\n",
    "            reward = self.comet.predict(data, batch_size=8, gpus=1)['scores']\n",
    "            reward = [[score] * seq_len for score in reward]\n",
    "        else:\n",
    "            raise ValueError(f\"metric {self.metric} not supported\")\n",
    "        if self.metric != 'cider' and self.metric != 'special':\n",
    "            reward = torch.tensor(reward).to(self.device)\n",
    "        \n",
    "        #print( \"reward size:\", reward.size() )\n",
    "        #return reward\n",
    "        return reward, reward2\n",
    "\n",
    "    def my_index(self, list1, target ):\n",
    "        if target in list1:\n",
    "            return list1.index( target )\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def compute_length_reward( self, preds, targets ):\n",
    "\n",
    "        #def differentiable_argamx( logits, tau ):\n",
    "\n",
    "        #    tmp = F.gumbel_softmax( logits, tau, hard=True )\n",
    "        #    tmp1 = torch.arange( 0, logits.size(2) )[None,None] * tmp\n",
    "        #    tokens = torch.sum( tmp1, dim = 2 )\n",
    "\n",
    "        #    return tokens\n",
    "        '''\n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        #print( first_index )\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / preds.size(1)\n",
    "        #print( pred_lengths ) \n",
    "        \n",
    "        reward = pred_lengths[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "        return reward         \n",
    "        '''\n",
    "        \n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / preds.size(1)\n",
    "        \n",
    "        target_index = targets == eos_token_id\n",
    "        first_index = ( target_index.int().cumsum(dim = 1 ) == 1 ) & target_index\n",
    "        target_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / targets.size(1)\n",
    "        #target_lengths = 1.1 * torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / targets.size(1)\n",
    "        #target_lengths = torch.clamp( target_lengths, max = 1.0 )\n",
    "        #target_lengths = torch.full((preds.size(0),), self.target_length / preds.size(1), device = self.device )\n",
    "        \n",
    "        reward_lengths = - nn.MSELoss(reduction='none')( pred_lengths, target_lengths )\n",
    "        #reward_lengths = - torch.abs( pred_lengths - target_lengths )\n",
    "        \n",
    "        reward = reward_lengths[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "        return reward  \n",
    "        \n",
    "        #reward = torch.tensor( [ self.my_index( pred.tolist(), eos_token_id ) for pred in preds ] ).to(self.device )\n",
    "        #reward = reward[:,None].expand( -1, preds.size(1) ).float()\n",
    "        \n",
    "        ##tokens = differentiable_argamx( logits, tau ) #logits から token を算出。微分可能 B * T\n",
    "        \n",
    "        '''\n",
    "        tmp1 = torch.abs( preds - tokenizer.pad_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        tmp2 = torch.abs( preds - eos_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        tmp = tmp1 * tmp2\n",
    "        pad_preds = F.sigmoid( 10 - 100 * tmp ) # pad のところだけ 1, あとは 0。 B * T  \n",
    "        reward = ( torch.tensor( preds.size(1))[None] - torch.sum( pad_preds, dim = 1 ))[:,None].expand( -1, preds.size(1) ) \n",
    "        '''\n",
    "        # 固定長97 から　pad と eos の長さを引いて、文章の長さ。文章の長さが大きいほどよい。\n",
    "        \n",
    "        #tmp1 = torch.abs( targets - tokenizer.pad_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        #tmp2 = torch.abs( targets - eos_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        #tmp = tmp1 * tmp2\n",
    "        #pad_targets = F.sigmoid( 10 - 100 * tmp ) # pad のところだけ 1, あとは 0。 B * T\n",
    "        \n",
    "        #reward = - nn.MSELoss( reduction = 'none' )( pad_preds, pad_targets )\n",
    "        \n",
    "        #return  reward\n",
    "    \n",
    "    def calc_ngram_repeat( self, preds ):\n",
    "\n",
    "        bsz, seq_len = preds.size()\n",
    "        \n",
    "        ngram_cnt = torch.zeros( (bsz), device = preds.device, dtype = torch.float )\n",
    "        for n in range( 2, 4 ):\n",
    "            for i, pred in enumerate( preds ):\n",
    "                pred = pred.tolist()\n",
    "                ngrams = zip(*[pred[i:] for i in range(n)])\n",
    "                counts = Counter(ngrams)\n",
    "                count_sum = 0\n",
    "                for count in counts.values():\n",
    "                    if count >= self.repeat_thresh:\n",
    "                        count_sum = count_sum + count\n",
    "                ngram_cnt[i] = ngram_cnt[i] + count_sum\n",
    "\n",
    "        return - ngram_cnt[:,None].expand( -1, seq_len ) / seq_len\n",
    "\n",
    "    def unique_ngram_ratio(self, preds):\n",
    "        \n",
    "        bsz, seq_len = preds.size()\n",
    "        ng = 5\n",
    "        unr = torch.zeros( (bsz, ng), device=preds.device, dtype=torch.float)\n",
    "\n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index * arange_index, dim = 1 )\n",
    "        \n",
    "        for b in range( bsz ):\n",
    "            for n in range( 0, ng - 1 ): # n は ngram - 1\n",
    "                if pred_lengths[b] > 0:\n",
    "                    #print( \"pred_lengths[b]:\", pred_lengths[b] )\n",
    "                    pred_tmp = preds[b,:pred_lengths[b]]\n",
    "                    #print( \"pred_tmp:\", pred_tmp )\n",
    "                else:\n",
    "                    pred_tmp = preds[b]\n",
    "                ngram_tensor = pred_tmp.unfold(0, n + 1, 1)\n",
    "                ngram_count = len( ngram_tensor )\n",
    "                unique_count = len( torch.unique( ngram_tensor ) )\n",
    "                #unique_count = len( set( map(tuple, ngram_tensor.tolist())))\n",
    "                unr[b,n] = unique_count / ngram_count\n",
    "\n",
    "        return torch.mean( unr, dim = 1)[:, None].expand(-1, seq_len)\n",
    "    \n",
    "    def calc_ngram_repeat_fast(self, preds):\n",
    "        bsz, seq_len = preds.size()\n",
    "        ngram_cnt = torch.zeros(bsz, device=preds.device, dtype=torch.float)\n",
    "        \n",
    "        for n in range(1, 5):\n",
    "            # 無視するトークンのリスト\n",
    "            #ignore_ids = [self.pad_token_id, self.eos_token_id, self.cls_token_id, self.sep_token_id]\n",
    "            if n == 1:\n",
    "                ignore_ids = [pad_token_id, eos_token_id, cls_token_id, sep_token_id, a_token_id, the_token_id, \\\n",
    "                              period_token_id, comma_token_id, and_token_id, in_token_id ]\n",
    "            else:\n",
    "                ignore_ids = [pad_token_id, eos_token_id, cls_token_id, sep_token_id]\n",
    "            \n",
    "            # 1. 無視すべきトークンの位置を特定 (bsz, seq_len)\n",
    "            # ignore_mask[b, i] が True なら、そのトークンは無視対象\n",
    "            ignore_mask = torch.zeros_like(preds, dtype=torch.bool)\n",
    "            for idx in ignore_ids:\n",
    "                ignore_mask |= (preds == idx)\n",
    "            \n",
    "            if seq_len < n:\n",
    "                continue\n",
    "            \n",
    "            # n-gram を抽出 (bsz, num_ngrams, n)\n",
    "            ngrams = preds.unfold(dimension=1, size=n, step=1)\n",
    "            \n",
    "            # 2. 各 n-gram に無視対象トークンが含まれているか判定\n",
    "            # n-gram内のいずれかが ignore_mask で True なら True\n",
    "            # (bsz, num_ngrams)\n",
    "            ngram_ignore_mask = ignore_mask.unfold(dimension=1, size=n, step=1).any(dim=-1)\n",
    "        \n",
    "            for b in range(bsz):\n",
    "                # このバッチの有効な n-gram だけを抽出\n",
    "                valid_ngrams = ngrams[b][~ngram_ignore_mask[b]]\n",
    "                \n",
    "                if valid_ngrams.size(0) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # ユニークな n-gram とそのカウントを取得\n",
    "                unique_ngrams, counts_per_ngram = torch.unique(valid_ngrams, dim=0, return_counts=True)\n",
    "                \n",
    "                # 閾値以上のカウントを合計\n",
    "                mask = counts_per_ngram >= self.repeat_thresh[n-1]\n",
    "                ngram_cnt[b] += counts_per_ngram[mask].sum().float() * self.repeat_weight[n-1]\n",
    "\n",
    "        penalty = - torch.clamp( torch.pow( 2, ngram_cnt -1 ) / seq_len, max = 1.0 )    \n",
    "        \n",
    "        return penalty[:, None].expand(-1, seq_len)\n",
    "    \n",
    "    def calc_cnt_repeat( self, preds ):\n",
    "\n",
    "        B, T = preds.size()\n",
    "\n",
    "        repeat_count = torch.zeros( ( bsz, vocab_size ), device = preds.device )\n",
    "        for i, pred in enumerate( preds ):\n",
    "            repeat_count0 = torch.bincount( pred )\n",
    "            repeat_count[i,:len(repeat_count0)] = repeat_count0 \n",
    "\n",
    "        repeat_count[:,pad_token_id] = 0\n",
    "        repeat_count[:,eos_token_id] = 0\n",
    "        repeat_count[:,cls_token_id] = 0\n",
    "        repeat_count[:,sep_token_id] = 0\n",
    "        thresh_masks = repeat_count >= self.repeat_thresh\n",
    "        repeat_count = repeat_count * thresh_masks\n",
    "\n",
    "        repeat = torch.sum( repeat_count, dim = 1 )\n",
    "        \n",
    "        return - repeat[:,None].expand( -1, T ).float() / seq_len\n",
    "        '''\n",
    "        B, T = preds.size()\n",
    "\n",
    "        repeat_count = torch.zeros( ( bsz, vocab_size ), device = preds.device )\n",
    "        for i, pred in enumerate( preds ):\n",
    "            repeat_count0 = torch.bincount( pred )\n",
    "            repeat_count[i,:len(repeat_count0)] = repeat_count0 \n",
    "\n",
    "        repeat_times = torch.sum( repeat_count >= self.repeat_thresh, dim = 1 )\n",
    "        #print( \"repeat_times:\",repeat_times )\n",
    "\n",
    "        pad_count = repeat_count[:,pad_token_id]\n",
    "        pad_times = (pad_count >= self.repeat_thresh).long()\n",
    "        #print( \"pad_times:\", pad_times )\n",
    "\n",
    "        eos_count = repeat_count[:,eos_token_id]\n",
    "        eos_times = (eos_count >= self.repeat_thresh).long()\n",
    "        #print( \"eos_times:\", eos_times )\n",
    "\n",
    "        repeat = repeat_times - pad_times - eos_times\n",
    "        #print( repeat )\n",
    "\n",
    "        #print( \"repeat size:\", repeat.size() )\n",
    "        \n",
    "        return - repeat[:,None].expand( -1, T ).float()\n",
    "        '''\n",
    "        '''\n",
    "        tokens = preds\n",
    "        #tokens[0,0] = 1000\n",
    "        #tokens[0,1] = 1000\n",
    "        #print( \"tokens size:\", tokens.size() )\n",
    "        #print( \"tokens[:,0]:\", tokens[:,0] )\n",
    "    \n",
    "        cnt = torch.zeros( (B, T),  device=preds.device, dtype=torch.float16 )\n",
    "        for i in range( T ):\n",
    "            cntj = torch.zeros( (B),  device=preds.device, dtype=torch.float16 )\n",
    "            num_j = 0\n",
    "            for j in range( max( 0, i - self.c ), min(  T, i + self.c ) ):\n",
    "                if j != i:\n",
    "                    ##print( \"torch.eq\", torch.eq( tokens[:,i], tokens[:,j]))\n",
    "                    #tmp1 = torch.eq( tokens[:,i], tokens[:,j]) \n",
    "                    #tmp2 = torch.ne( tokens[:,i], eos_token_id )\n",
    "                    #tmp3 = torch.ne( tokens[:,i], tokenizer.pad_token_id )\n",
    "                    ##print( \"tmp1:\", tmp1 )\n",
    "                    ##print( \"tmp2:\", tmp2 )\n",
    "                    ##print( \"tmp3:\", tmp3 )\n",
    "                    #tmp = (torch.logical_and(torch.logical_and( tmp1 , tmp2 ),tmp3 )).to(torch.float16)\n",
    "                    if self.decode_t == 'no-endoftext':\n",
    "                        tmp = ((tokens[:,i] != tokens[:,j] ).to(torch.float) ) * 10 \\\n",
    "                            +  ((tokens[:,i] == endoftext_token_id).to(torch.float) )  * 10\n",
    "                    elif self.decode_t == 'no-pad':\n",
    "                        tmp = ((tokens[:,i] != tokens[:,j] ).to(torch.float) ) * 10 \\\n",
    "                            + ((tokens[:,i] == pad_token_id).to(torch.float) )  * 10 \\\n",
    "                            + ((tokens[:,i] == eos_token_id).to(torch.float) ) * 10\n",
    "                            # tokens[:,i] と tokens[:,j] が同じで、tokens[:,i] が　eos、pad でなければ 0 その他は 10 以上の整数 \n",
    "                    tmp = F.sigmoid( 10 - 100 * tmp ) # tokens[:,i]とtokens[:,j] が同じで eos pad ではないところだけ 1, あとは 0.\n",
    "                    #if torch.any( tmp != 0 ):\n",
    "                    #    print( \"tmp:\", tmp )\n",
    "                    cntj = cntj + tmp # repeat の数。[B]  jについて足しこんでいる。\n",
    "                    #if torch.any( cntj != 0 ):\n",
    "                    #    print( \"cntj:\", cntj )\n",
    "                    num_j = num_j + 1\n",
    "            cnt[:,i] = cntj / num_j # repeat の数。[B] i について足しこんでいる。\n",
    "        \n",
    "        #mask = cnt != 0\n",
    "        #print( \"cnt mask:\", cnt[mask] )\n",
    "        #return - torch.mean( cnt, dim = 1 ) # \n",
    "        return - torch.mean( cnt, dim = 1 )[:,None].expand(-1, T )  # B * T repeat の数が少ないほど良い。repeat の数の -1 倍が多いほどよい。\n",
    "        '''\n",
    "    def forward(self, top_probs, sampled_beam_idx, top_indices, targets, imgs2,  sources=None, masks=None):\n",
    "        \"\"\"\n",
    "        outputs: batch x len x d_model\n",
    "        targets: batch x len\n",
    "        sources: batch x len\n",
    "        masks:   batch x len\n",
    "        \"\"\"\n",
    "        self.device = sampled_beam_idx.device\n",
    "        # input to device\n",
    "        targets = targets.to(self.device)\n",
    "        bsz, seq_len, beam = top_probs.size()\n",
    "        eps = 1e-8\n",
    "\n",
    "        preds = torch.gather( top_indices, -1, sampled_beam_idx ).squeeze( -1 )\n",
    "        tmp = torch.clamp( top_probs, eps )\n",
    "        top_log_probs = torch.log( tmp )\n",
    "        sample_log_probs = torch.gather( top_log_probs, -1, sampled_beam_idx ).squeeze( -1 )\n",
    "\n",
    "        if self.reward_t == 'ordinary':\n",
    "            reward_ord = self.compute_reward(preds, targets, imgs2, sources)   #  bsz\n",
    "            reward_repeat = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_length = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == \"ord+rep\":\n",
    "            reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            #reward = reward_ord + reward_repeat - b # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == \"ord+rep+len\":\n",
    "            reward_ord, reward_ord2 = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == 'ord+len':\n",
    "            reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == 'ord+rep+len+unr':\n",
    "            reward_ord, reward_ord2 = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = self.unique_ngram_ratio(preds)\n",
    "        \n",
    "        ## apply mask\n",
    "        #if masks is not None:\n",
    "        #    masks = masks.to(self.device)\n",
    "        #    probs, targets = probs[masks], targets[masks]\n",
    "        #    # outputs, targets = outputs[masks], targets[masks]\n",
    "        #    reward, preds = reward[masks], preds[masks]\n",
    "       \n",
    "        return reward_ord, reward_ord2, reward_repeat, reward_length, reward_unr, preds, sample_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, dim=1):\n",
    "    return torch.logsumexp(x.float(), dim=dim).type_as(x)\n",
    "\n",
    "class DynamicCRF(nn.Module):\n",
    "    def __init__(self, num_embedding, low_rank=32, beam_size=64, crf_coef=1.0, temp = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        #low_rank = num_embedding\n",
    "        self.E1 = nn.Embedding(num_embedding, low_rank)\n",
    "        self.E2 = nn.Embedding(num_embedding, low_rank)\n",
    "\n",
    "        self.vocb = num_embedding\n",
    "        self.rank = low_rank\n",
    "        self.beam = beam_size\n",
    "        self.crf_coef = crf_coef\n",
    "        self.temp = temp\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"vocab_size={}, low_rank={}, beam_size={}\".format(\n",
    "            self.vocb, self.rank, self.beam)\n",
    "\n",
    "    def forward(self, emissions, top_logits, top_indices, targets, masks, beam=None):\n",
    "        numerator = self._compute_score(emissions, targets, masks)\n",
    "        denominator = self._compute_normalizer(emissions, targets, masks, beam )\n",
    "        beam_probs = self._compute_normalizer2(top_logits, top_indices, targets, masks, beam)\n",
    "\n",
    "        return numerator - denominator, beam_probs\n",
    "    \n",
    "    def forward_decoder(self, emissions, masks=None, beam=None):\n",
    "        return self._viterbi_decode(emissions, masks, beam)\n",
    "\n",
    "    def _compute_score(self, emissions, targets, masks=None):\n",
    "        batch_size, seq_len = targets.size()\n",
    "\n",
    "        emission_scores = emissions.gather(2, targets[:, :, None])[:, :, 0]  # B x T\n",
    "        transition_scores = (self.E1(targets[:, :-1]) * self.E2(targets[:, 1:])).sum(2)\n",
    "       \n",
    "        scores = emission_scores\n",
    "        scores[:, 1:] += transition_scores\n",
    "        \n",
    "        if masks is not None:\n",
    "            scores = scores * masks.type_as(scores)\n",
    "\n",
    "        return scores.sum(-1)\n",
    "        \n",
    "    def _compute_normalizer(self, emissions, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        if targets is not None:\n",
    "            #_emissions = emissions.scatter(2, targets[:, :, None], np.float('inf'))\n",
    "            _emissions = emissions.scatter(2, targets[:, :, None], float('inf'))\n",
    "            beam_targets = _emissions.topk(beam, 2)[1]\n",
    "            beam_emission_scores = emissions.gather(2, beam_targets)\n",
    "        else:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step.\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        for i in range(1, seq_len):\n",
    "            next_score = score[:, :, None] + beam_transition_matrix[:, i-1]\n",
    "            next_score = logsumexp(next_score, dim=1) + beam_emission_scores[:, i]\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i:i+1], next_score, score)\n",
    "            else:\n",
    "                score = next_score\n",
    "\n",
    "        return logsumexp(score, dim=1)\n",
    "\n",
    "    def _compute_top_probs(self, beam_emission_scores, beam_transition_matrix, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "\n",
    "            # greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "\n",
    "            # multinomial selection\n",
    "            B, C, W = _score.shape\n",
    "            flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            _index_flat = torch.multinomial(probs, num_samples=1)\n",
    "            _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            _index = _index_flat.view(B, W)\n",
    "            _score = _score_flat.view(B, W)\n",
    "\n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "        \n",
    "        all_scores = traj_scores\n",
    "        all_scores.append( score )\n",
    "        all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 ).to(device)\n",
    "        #beam_probs = F.softmax( all_scores / self.temp, dim = 2 )\n",
    "        beam_probs = F.softmax( all_scores, dim = 2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        #finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return beam_probs, finalized_tokens.unsqueeze(-1)\n",
    "\n",
    "    def _compute_top_probs2(self, beam_emission_scores, beam_transition_matrix, beam_targets, n_best = 10, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        window = 5\n",
    "\n",
    "        exclude_token_id2 = torch.tensor( [pad_token_id, eos_token_id, a_token_id, the_token_id, and_token_id, in_token_id, \\\n",
    "            we_token_id, i_token_id, he_token_id, she_token_id, it_token_id, they_token_id, \\\n",
    "            period_token_id, comma_token_id, dbl_token_id, sgl_token_id], device=device )\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "    \n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        _score = beam_emission_scores[:, 0][:,None,:].expand(-1,beam,-1) # t = -1 → 0 への beam_transition_matrix bsz*beam C*beam W は　0 \n",
    "        #_score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "        B, C, W = _score.shape\n",
    "        flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "        probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "        _index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )\n",
    "        _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "        _index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "        _score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "    \n",
    "        traj_scores2 = []\n",
    "        traj_tokens2 = []\n",
    "    \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score) # bsz * beam\n",
    "            traj_scores2.append( _score2 )\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            \n",
    "            ## greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            #_score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "            \n",
    "            ## multinomial selection\n",
    "            B, C, W = _score.shape\n",
    "            flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            #print( \"flat_score size:\", flat_score.size() )\n",
    "            #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            _index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )  # C が n_best になる。\n",
    "            _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            _index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "            _score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "            \n",
    "            #_score = _score + beam_emission_scores[:, i] # bsz, beam        \n",
    "    \n",
    "            _score2 = _score2 + beam_emission_scores[:, i][:,None,:].expand(-1,beam,-1).gather( 1, _index2 ) # bsz, n_best, beam       \n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score2[:,0,:], _index2[:,0,:]\n",
    "            traj_tokens.append(index)\n",
    "            traj_tokens2.append( _index2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        used_index = torch.zeros( bsz, seq_len, dtype=torch.long, device=device)\n",
    "        used_index[:,seq_len - 1] = best_index\n",
    "        beam_probs = torch.zeros( bsz, seq_len, beam, dtype=torch.float, device=device )\n",
    "        beam_probs[:,seq_len - 1,:] = score \n",
    "        for t_reverse, (idx2, scs2) in enumerate( zip( reversed(traj_tokens2), reversed(traj_scores2))):\n",
    "            t = seq_len - t_reverse - 2\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            #print( \"previous_index size:\", previous_index.size() )\n",
    "            selected_idx = idx2[:,0,:].gather( 1, previous_index )\n",
    "            selected_scs = scs2[:,0,:].gather( 1, previous_index )\n",
    "            beam_probs[ :, t ,: ] = scs2[:,0,: ]\n",
    "            for n in range( 1, n_best ):\n",
    "                for b in range( bsz ):\n",
    "                    t_max = torch.min( t + window, seq_len - 1 )\n",
    "                    used_index_tmp = used_index[b,t+1:t_max]\n",
    "                    corrected_selected_idx = torch.gather( beam_targets[b,t,:], 0, selected_idx[b] )\n",
    "                    #if corrected_selected_idx in used_index[b] and corrected_selected_idx not in exclude_token_id2:\n",
    "                    if corrected_selected_idx in used_index_tmp and corrected_selected_idx not in exclude_token_id2:\n",
    "                        selected_idx[b] = idx2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        selected_scs[b] = scs2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        beam_probs[ b, t ,: ] = scs2[b, n,: ]\n",
    "                    else:\n",
    "                        break\n",
    "            #print( \"beam_targets[:,t,:].size():\",beam_targets[:,t,:].size())\n",
    "            #print( \"selected_idx.size():\", selected_idx.size() )\n",
    "            used_index[:,t] =  torch.gather( beam_targets[:,t,:], -1, selected_idx )[:,0] # bsz * 1 → bsz\n",
    "            #print( \"used_index[:,:]:\", used_index[:,:] )\n",
    "            #used_index[:,t] = selected_idx[:,0]\n",
    "            finalized_tokens.append( selected_idx )\n",
    "            finalized_scores.append( selected_scs )\n",
    "       \n",
    "        finalized_tokens.reverse()\n",
    "        sampled_beam_idx = torch.cat(finalized_tokens, 1)\n",
    "        #finalized_tokens = beam_targets.gather(2, sampled_beam_idx[:, :, None])[:, :, 0]\n",
    "        #print( \"finalized_tokens:\", finalized_tokens )\n",
    "        \n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        beam_probs = F.softmax( beam_probs, dim = 2 )\n",
    "        \n",
    "        #return finalized_scores, finalized_tokens\n",
    "        return beam_probs, sampled_beam_idx.unsqueeze(-1)\n",
    "\n",
    "    def _compute_viterbi_no_repeat(self, beam_emission_scores, beam_transition_matrix, beam_targets, n_best = 10, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        exclude_token_id2 = torch.tensor( [pad_token_id, eos_token_id, a_token_id, the_token_id, and_token_id, in_token_id, \\\n",
    "            we_token_id, i_token_id, he_token_id, she_token_id, it_token_id, they_token_id, \\\n",
    "            period_token_id, comma_token_id, dbl_token_id, sgl_token_id], device=device )\n",
    "        window = 5\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "    \n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        _score = beam_emission_scores[:, 0][:,None,:].expand(-1,beam,-1) # t = -1 → 0 への beam_transition_matrix bsz*beam C*beam W は　0 \n",
    "        _score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "        #B, C, W = _score.shape\n",
    "        #flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "        #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "        #_index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )\n",
    "        #_score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "        #_index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "        #_score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "    \n",
    "        traj_scores2 = []\n",
    "        traj_tokens2 = []\n",
    "    \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score) # bsz * beam\n",
    "            traj_scores2.append( _score2 )\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            \n",
    "            ## greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "            \n",
    "            ### multinomial selection\n",
    "            #B, C, W = _score.shape\n",
    "            #flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            ##print( \"flat_score size:\", flat_score.size() )\n",
    "            ##probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            #_index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )  # C が n_best になる。\n",
    "            #_score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            #_index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "            #_score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "            \n",
    "            #_score = _score + beam_emission_scores[:, i] # bsz, beam        \n",
    "    \n",
    "            _score2 = _score2 + beam_emission_scores[:, i][:,None,:].expand(-1,beam,-1).gather( 1, _index2 ) # bsz, n_best, beam       \n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score2[:,0,:], _index2[:,0,:]\n",
    "            traj_tokens.append(index)\n",
    "            traj_tokens2.append( _index2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        used_index = torch.zeros( bsz, seq_len, dtype=torch.long, device=device )\n",
    "        used_index[:,seq_len - 1] = best_index\n",
    "        beam_probs = torch.zeros( bsz, seq_len, beam, dtype=torch.float, device=device )\n",
    "        beam_probs[:,seq_len - 1,:] = score \n",
    "        for t_reverse, (idx2, scs2) in enumerate( zip( reversed(traj_tokens2), reversed(traj_scores2))):\n",
    "            t = seq_len - t_reverse - 2\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            #print( \"previous_index size:\", previous_index.size() )\n",
    "            selected_idx = idx2[:,0,:].gather( 1, previous_index )\n",
    "            selected_scs = scs2[:,0,:].gather( 1, previous_index )\n",
    "            beam_probs[ :, t ,: ] = scs2[:,0,: ]\n",
    "            for n in range( 1, n_best ):\n",
    "                for b in range( bsz ):\n",
    "                    t_max = torch.min( t + window, seq_len - 1 )\n",
    "                    used_index_tmp = used_index[b,t+1:t_max]\n",
    "                    corrected_selected_idx = torch.gather( beam_targets[b,t,:], 0, selected_idx[b] )\n",
    "                    #if corrected_selected_idx in used_index[b] and corrected_selected_idx not in exclude_token_id2:\n",
    "                    if corrected_selected_idx in used_index_tmp and corrected_selected_idx not in exclude_token_id2:\n",
    "                        selected_idx[b] = idx2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        selected_scs[b] = scs2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        beam_probs[ b, t ,: ] = scs2[b, n,: ]\n",
    "                    else:\n",
    "                        break\n",
    "            #print( \"beam_targets[:,t,:].size():\",beam_targets[:,t,:].size())\n",
    "            #print( \"selected_idx.size():\", selected_idx.size() )\n",
    "            used_index[:,t] =  torch.gather( beam_targets[:,t,:], -1, selected_idx )[:,0] # bsz * 1 → bsz\n",
    "            #print( \"used_index[:,:]:\", used_index[:,:] )\n",
    "            #used_index[:,t] = selected_idx[:,0]\n",
    "            finalized_tokens.append( selected_idx )\n",
    "            finalized_scores.append( selected_scs )\n",
    "       \n",
    "        finalized_tokens.reverse()\n",
    "        sampled_beam_idx = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, sampled_beam_idx[:, :, None])[:, :, 0]\n",
    "        #print( \"finalized_tokens:\", finalized_tokens )\n",
    "        \n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        #beam_probs = F.softmax( beam_probs, dim = 2 )\n",
    "        \n",
    "        return finalized_scores, finalized_tokens\n",
    "    \n",
    "    \n",
    "    def _compute_f_algorithm(self, emissions, temp = 1.0, targets=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "   \n",
    "        # 結果格納用\n",
    "        scores = torch.zeros((batch_size, seq_len, beam), device=emissions.device)\n",
    "        all_preds = []\n",
    "        sampled_beam_idx = []\n",
    "        \n",
    "        # --- ステップ 0 ---\n",
    "        scores[:, 0, :] = beam_emission_scores[:, 0]\n",
    "        probs_0 = F.softmax(scores[:, 0, :] / temp, dim=-1)\n",
    "        sampled_beam_idx_0 = torch.multinomial(probs_0, 1)\n",
    "\n",
    "        # 256個中の位置から、実際のVocab IDに変換\n",
    "        all_preds.append(  torch.gather(beam_targets[:, 0], 1, sampled_beam_idx_0).squeeze(1) )\n",
    "        sampled_beam_idx.append( sampled_beam_idx_0 )\n",
    "        \n",
    "        # --- ステップ 1 以降 ---\n",
    "        for i in range(1, seq_len):\n",
    "            # 1. 直前に自分が選んだ単語(Vocab ID)を取得\n",
    "            prev_vocab_idx = all_preds[-1]\n",
    "            \n",
    "            # 2. 遷移スコアの計算\n",
    "            prev_trans_feat = self.E1(prev_vocab_idx) # B x D\n",
    "            curr_trans_feat = self.E2(beam_targets[:, i]) # B x beam x D\n",
    "        \n",
    "            # B x 1 x D  @  B x D x beam  -> B x 1 x beam\n",
    "            #D = prev_trans_feat.size(-1) # 次元のサイズ\n",
    "            #current_trans_scores = torch.bmm(\n",
    "            #    prev_trans_feat.unsqueeze(1), \n",
    "            #    curr_trans_feat.transpose(1, 2)\n",
    "            #).squeeze(1) / (D ** 0.5) \n",
    "            current_trans_scores = torch.bmm(\n",
    "                prev_trans_feat.unsqueeze(1), \n",
    "                curr_trans_feat.transpose(1, 2)\n",
    "            ).squeeze(1)\n",
    "\n",
    "            # 3. 現在のスコアを確定\n",
    "            scores[:, i, :] = beam_emission_scores[:, i] + current_trans_scores\n",
    "        \n",
    "            # 4. 次のステップのために、現在の単語をサンプリング（正規化が必要）\n",
    "            probs_i = F.softmax(scores[:, i, :] / temp , dim=-1)\n",
    "            sampled_beam_idx_i = torch.multinomial(probs_i, 1).long()\n",
    "            all_preds.append( torch.gather(beam_targets[:, i], 1, sampled_beam_idx_i).squeeze(1) )\n",
    "            sampled_beam_idx.append( sampled_beam_idx_i )\n",
    "                              \n",
    "\n",
    "        all_preds = torch.stack( all_preds, dim = 0 ).transpose(0,1)\n",
    "        sampled_beam_idx = torch.stack( sampled_beam_idx, dim = 0 ).transpose(0,1)\n",
    "        \n",
    "        ## --- 全体の log_probs 算出（ratio計算用） ---\n",
    "        #max_score, _ = torch.max(scores, dim=2, keepdim=True)\n",
    "        #exp1 = torch.exp(scores - max_score)\n",
    "        #sumof = torch.sum(exp1, dim=2, keepdim=True)\n",
    "        #denominator1 = torch.log(sumof + 1e-8) + max_score\n",
    "    \n",
    "        #beam_log_probs = scores - denominator1\n",
    "        ##sample_log_probs = torch.gather( beam_log_probs, -1, sampled_beam_idx ).squeeze(-1)\n",
    "        #beam_probs = torch.exp(beam_log_probs)\n",
    "\n",
    "        #return beam_probs, preds, sample_log_probs, sampled_beam_idx\n",
    "        return scores, all_preds\n",
    "\n",
    "    def _viterbi_decode(self, emissions, masks=None, beam=None):\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        #print( \"score size:\", score.size() )\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return finalized_scores, finalized_tokens\n",
    "    \n",
    "    def _compute_many_values(self, emissions, targets, top_indices = None, masks=None, beam=None):\n",
    "\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        \n",
    "        if top_indices == None:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        else:\n",
    "            beam_emission_scores = torch.gather( emissions, -1, top_indices )\n",
    "            beam_targets = top_indices\n",
    "        \n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam  step i-1 における 256 → 256 の max から 256 への遷移確率と \n",
    "                                                # 256 → 256 の前の 256 の max のインデックストークン\n",
    "                                                # index b * 256 の 位置が i の token で、値が i-1 のtoken   \n",
    "\n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam i における 256 の遷移確率ではない確率を加える。i における 256 の全確率。\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        all_scores = traj_scores\n",
    "        all_scores.append( score )\n",
    "        all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 )\n",
    "        \n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None]) # 時刻 T における b*256 の確率最大の token\n",
    "        finalized_scores.append(best_score[:, None]) #時刻 T における b*256 の確率最大の score\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)): #idx,scs は、反転時刻 i と i-1における b * 256のトークンと確率\n",
    "            previous_index = finalized_tokens[-1] # 時刻 Tなど 求めたいトークンと確率の一個後 における token　b * 1\n",
    "            finalized_tokens.append(idx.gather(1, previous_index)) # 時刻 一個後iのトークン previou_index に至るための時刻i-1 のトークン\n",
    "                                                                    # b* 256 の token から b * 1 の previous_idnex token で gather\n",
    "            finalized_scores.append(scs.gather(1, previous_index)) # 時刻一個後 i のトークンに至るための時刻 i-1 の確率\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "        \n",
    "        if self.crf_coef != 0.0:\n",
    "            numerator = self._compute_score(emissions, targets)\n",
    "            denominator = self._compute_normalizer(emissions, targets)\n",
    "            crf_loss = - ( numerator - denominator ).mean() / seq_len\n",
    "        else:\n",
    "            crf_loss = torch.zeros( (1), device = emissions.device, dtype = torch.float )\n",
    "        \n",
    "        top_probs, sampled_beam_idx = self._compute_top_probs(beam_emission_scores, beam_transition_matrix)\n",
    "        \n",
    "        return finalized_scores, finalized_tokens, top_probs, beam_targets, crf_loss, sampled_beam_idx\n",
    "    '''\n",
    "    \n",
    "    def _compute_many_values(self, emissions, targets, top_indices = None, masks=None, beam=None):\n",
    "\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        \n",
    "        if top_indices == None:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        else:\n",
    "            beam_emission_scores = torch.gather( emissions, -1, top_indices )\n",
    "            beam_targets = top_indices\n",
    "        \n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "        \n",
    "        if self.crf_coef != 0.0:\n",
    "            numerator = self._compute_score(emissions, targets)\n",
    "            denominator = self._compute_normalizer(emissions, targets)\n",
    "            #denominator = self._compute_normalizer2(emissions, targets)\n",
    "            crf_loss = - ( numerator - denominator ).mean() / seq_len\n",
    "        else:\n",
    "            crf_loss = torch.zeros( (1), device = emissions.device, dtype = torch.float )\n",
    "\n",
    "        finalized_scores, finalized_tokens = self._compute_viterbi_no_repeat(beam_emission_scores, beam_transition_matrix, beam_targets)\n",
    "        top_probs, sampled_beam_idx = self._compute_top_probs2(beam_emission_scores, beam_transition_matrix, beam_targets)\n",
    "        \n",
    "        return finalized_scores, finalized_tokens, top_probs, beam_targets, crf_loss, sampled_beam_idx\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, crf_low_rank, crf_beam_size, dropout, padding_idx,\n",
    "                crf_coef = 1.0, temp = 0.5 ):\n",
    "        super(TopLayer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.padding_idx = padding_idx\n",
    "        print( \"in TopLayer:\" )\n",
    "        self.crf_layer = DynamicCRF(num_embedding = vocab_size, low_rank = crf_low_rank, \n",
    "                                    beam_size = crf_beam_size, crf_coef=crf_coef, temp=temp)\n",
    "\n",
    "        #self.one_more_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        #self.tgt_word_prj = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "        ## gae 学習用\n",
    "        #self.linear_critical = nn.Linear(crf_beam_size, 1 )\n",
    "\n",
    "    def forward(self, src_representation, top_logits, top_indices, src_input, tgt_input, is_training ):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        #assert src_input.size() == tgt_input.size()\n",
    "\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        #seqlen, bsz = src_input.size()\n",
    "        seqlen, bsz = src_input.shape[:2]\n",
    "\n",
    "        src_representation = F.dropout(src_representation, p=self.dropout, training=is_training)\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "\n",
    "        src = src_representation\n",
    "\n",
    "        #emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "        emissions = src_representation\n",
    "        #log_probs = torch.log_softmax(emissions, -1)\n",
    "        #assert log_probs.size() == torch.Size([seqlen, bsz, self.vocab_size])\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz x src_len x vocab_size]\n",
    "        #emission_mask = ~tgt_input.eq(self.padding_idx) # [bsz x src_len] #pad のところは 0 padでないところが 1\n",
    "        emission_mask = torch.ones_like( tgt_input, dtype=torch.bool ) #全部　pad でないとして 1\n",
    "        batch_crf_loss, top_probs = self.crf_layer(emissions, top_logits, top_indices, tgt_input, emission_mask) # [bsz]\n",
    "        #critical_value = self.linear_critical( top_probs )\n",
    "        #critical_value = torch.zeros( ( 1,1,1) )\n",
    "        batch_crf_loss = - batch_crf_loss\n",
    "        assert batch_crf_loss.size() == torch.Size([bsz])\n",
    "        return batch_crf_loss, top_probs\n",
    "\n",
    "    def decoding(self, src_representation, src_input):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        seqlen, bsz = src_input.size()\n",
    "\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "        src = src_representation\n",
    "\n",
    "        emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        _, finalized_tokens = self.crf_layer.forward_decoder(emissions)\n",
    "        assert finalized_tokens.size() == torch.Size([bsz, seqlen])\n",
    "        return finalized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HcmR9lKrIbiL"
   },
   "outputs": [],
   "source": [
    "class CaptioningTransformer(nn.Module):\n",
    "    \n",
    "    #CaptioningTransformerのコンストラクタ\n",
    "    #dim_embedding  : 埋め込み次元\n",
    "    #dim_feedforward: FNNの中間特徴次元\n",
    "    #num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    #num_layers     : Transformerデコーダ層の数\n",
    "    #vocab_size     : 辞書の次元\n",
    "    #null_index     : NULLのID\n",
    "    #dropout        : ドロップアウト確率\n",
    "    \n",
    "    def __init__(self, img_size: int,  dim_embedding: int, length_max: int, vocab_size: int, tokenizer, dropout: float = 0.0, \\\n",
    "                 pad_token_id: int=0, use_repeat_logits_half=False, crf_coef = 1.0, temp=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        #CLIP\n",
    "        model_id = \"openai/clip-vit-large-patch14-336\"\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(model_id )\n",
    "        memory = self.clip_model( torch.randn( 1, 3, 336, 336 ) )\n",
    "        memory = memory.last_hidden_state\n",
    "        img_length = memory.size(1)\n",
    "        clip_dim = memory.size(2)\n",
    "        self.connector_pool = nn.AdaptiveAvgPool1d(length_max - 1 )\n",
    "        self.connector_ln = nn.LayerNorm( clip_dim )\n",
    "        self.connector_linear1 = nn.Linear( clip_dim, dim_embedding )\n",
    "        self.connector_gleu = nn.GELU()\n",
    "        self.connector_linear2 = nn.Linear( dim_embedding, dim_embedding )\n",
    "\n",
    "       \n",
    "        # Connector\n",
    "        self.connector_pool = nn.AdaptiveAvgPool1d(length_max - 1 )\n",
    "       # Down Sampling\n",
    "        cls_token = memory[:, :1, :] # (bsz, 1, 1024)\n",
    "        patch_tokens = memory[:, 1:, :] # (bsz, 576, 1024)\n",
    "        # パッチ部分を 576 -> 96 に圧縮\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 1024, 576)\n",
    "        patch_tokens = self.connector_pool(patch_tokens)\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 96, 1024)\n",
    "        # CLSと結合して合計 97 トークンにする\n",
    "        memory = torch.cat([cls_token, patch_tokens], dim=1) # (bsz, 97, 1024)\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding( dim_embedding )\n",
    "\n",
    "        model_id = \"google-bert/bert-large-uncased\"\n",
    "        self.bert = BertModel.from_pretrained( model_id )\n",
    "\n",
    "        ## 単語出力分布計算\n",
    "        self.ln_outputs = nn.LayerNorm( dim_embedding )\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "\n",
    "        crf_low_rank = 32\n",
    "        crf_beam_size = 256\n",
    "        self.crf_beam_size = crf_beam_size\n",
    "        top_dropout = 0.0\n",
    "        tgt_padding_idx = tokenizer.pad_token_id\n",
    "        self.toplayer = TopLayer( vocab_size, dim_embedding, crf_low_rank, crf_beam_size, top_dropout, \n",
    "                                  tgt_padding_idx, crf_coef = crf_coef, temp=temp )\n",
    "        \n",
    "        ### GAE 用\n",
    "        self.ln_critical = nn.LayerNorm( crf_beam_size )\n",
    "        self.linear_critical = nn.Linear( crf_beam_size, 1)\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.use_repeat_logits_half = use_repeat_logits_half\n",
    "\n",
    "\n",
    "    def mlp_connector(self, memory ):\n",
    "\n",
    "        cls_token = memory[:, :1, :] # (bsz, 1, 1024)\n",
    "        patch_tokens = memory[:, 1:, :] # (bsz, 576, 1024)\n",
    "\n",
    "        # パッチ部分を 576 -> 96 に圧縮\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 1024, 576)\n",
    "        patch_tokens = self.connector_pool(patch_tokens)\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 96, 1024)\n",
    "\n",
    "        # CLSと結合して合計 97 トークンにする\n",
    "        memory = torch.cat([cls_token, patch_tokens], dim=1) # (bsz, 97, 1024)\n",
    "\n",
    "        memory = self.connector_ln( memory )\n",
    "        memory = self.connector_linear1( memory )\n",
    "        memory = self.connector_gleu( memory )\n",
    "        memory = self.connector_linear2( memory )\n",
    "        \n",
    "        return memory\n",
    "\n",
    "    def forward(self, images: torch.Tensor, targets: torch.Tensor, top_indices = None ):\n",
    "\n",
    "        self.device = images.device\n",
    "        \n",
    "        memory = self.clip_model( images ).last_hidden_state\n",
    "        memory = self.mlp_connector( memory )\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        emissions = self.linear( outputs )\n",
    "\n",
    "        if self.use_repeat_logits_half == True:\n",
    "            emissions = repeat_logits_half( emissions )\n",
    "\n",
    "\n",
    "        finalized_scores, finalized_tokens, top_probs, top_indices, crf_loss, sampled_beam_idx  = \\\n",
    "            self.toplayer.crf_layer._compute_many_values(emissions, targets, top_indices)\n",
    "            #self.crf_layer._compute_many_values(emissions, targets, top_indices)\n",
    "        \n",
    "        critical_value = self.ln_critical( top_probs )\n",
    "        critical_value = self.linear_critical( critical_value )\n",
    "\n",
    "        return finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "            critical_value[:,:,0], crf_loss, emissions, sampled_beam_idx\n",
    "        #return finalized_scores, finalized_tokens, top_probs, top_indices, crf_loss, emissions\n",
    "\n",
    "    def inference(self, images: torch.Tensor, inf_t = 'v' ):\n",
    "\n",
    "        self.device = images.device\n",
    "        \n",
    "        memory = self.clip_model( images ).last_hidden_state\n",
    "        memory = self.mlp_connector( memory )\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        emissions = self.linear( outputs )\n",
    "\n",
    "        if self.use_repeat_logits_half == True:\n",
    "            emissions = repeat_logits_half( emissions )\n",
    "\n",
    "        if inf_t == 'v':\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._viterbi_decode(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._viterbi_decode(emissions)\n",
    "        elif inf_t == 'v-nr':\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._compute_viterbi_no_repeat(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._compute_viterbi_no_repeat(emissions)\n",
    "        else:\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._compute_f_algorithm(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._compute_f_algorithm(emissions)\n",
    "\n",
    "        return finalized_scores, finalized_tokens\n",
    "\n",
    "    def repeat_logits_half(self, emissions ):\n",
    "        \n",
    "        penalty = 1.2\n",
    "        scores, preds = torch.max( emissions, 2 )\n",
    "        masks = emissions == scores[:,:,None]\n",
    "        masks = masks.permute( 1, 0, 2 )\n",
    "        new_mask = torch.zeros( (  masks.size(1), masks.size(2)), device = emissions.device, dtype=torch.bool )\n",
    "        new_masks = torch.zeros( ( masks.size(0), masks.size(1), masks.size(2)), device = emissions.device, dtype=torch.bool )\n",
    "        for i, mask in enumerate( masks ):\n",
    "            new_mask = torch.logical_or( mask,  new_mask  )\n",
    "            new_masks[i] = new_mask\n",
    "        new_masks = new_masks.transpose(0,1)\n",
    "        first_true_mask = ( new_masks.int().cumsum(dim = 1 ) == 1 ) & new_masks\n",
    "        new_masks = new_masks & ( ~first_true_mask )\n",
    "\n",
    "        p_masks = emissions > 0\n",
    "        m_masks = emissions < 0\n",
    "        p_new_masks = p_masks & new_masks\n",
    "        m_new_masks = m_masks & new_masks\n",
    "        emissions2 = emissions.clone()\n",
    "        emissions2[p_new_masks] = emissions[p_new_masks] / penalty\n",
    "        emissions2[m_new_masks] = emissions2[m_new_masks] * penalty\n",
    "\n",
    "        return emissions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path: str, img_directory: str, transforms, transforms2, tokenizer, length_max = None ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_directory = img_directory\n",
    "        self.transforms = transforms\n",
    "        self.transforms2 = transforms2 \n",
    "        # TODO: fix to original data\n",
    "        #画像の前処理\n",
    "        self.img_file = []\n",
    "        self.tokens = []\n",
    "        #vocab_size = len( tokenizer )\n",
    "        #c1 = torch.zeros( ( vocab_size ) )\n",
    "        #c2 = torch.zeros( ( vocab_size, vocab_size ) )\n",
    "        if length_max == None:\n",
    "            self.length_max = 0\n",
    "        else:\n",
    "            self.length_max = length_max\n",
    "        length_sum = 0\n",
    "\n",
    "        with open( file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for i, line_data in enumerate( data ):\n",
    "            if i % 100000 == 0:\n",
    "                print( \"i:\", i )\n",
    "            self.img_file.append( line_data['img_file'] )\n",
    "            id_tokens = line_data['id_tokens']\n",
    "            id_tokens.append( eos_token_id )\n",
    "            id_tokens.append( eos_token_id )\n",
    "            length_sum += len( id_tokens )\n",
    "            if length_max != None:\n",
    "                id_tokens = torch.tensor( id_tokens )[:self.length_max]\n",
    "            else:\n",
    "                if self.length_max < len( id_tokens ):\n",
    "                    self.length_max = len( id_tokens )\n",
    "                id_tokens = torch.tensor( id_tokens )\n",
    "            self.tokens.append( id_tokens )\n",
    "        # w1, w2 を作る時は length_max = None　でお願いします。\n",
    "        #    for i2 in range( len(id_tokens) ):\n",
    "        #        if i2 == len( id_tokens ) - 1:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #        else:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #            c2[id_tokens[i2], id_tokens[i2+1] ] += 1\n",
    "        '''\n",
    "        c1avg = int( torch.sum( c1 ) / torch.sum( torch.ne( c1, 0 ).int()) )\n",
    "        c2avg = int( torch.sum( torch.sum( c2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( c2, 0 ).int() ) )\n",
    "\n",
    "        c1[0] = c1avg\n",
    "\n",
    "        c2[:,0] = c2avg\n",
    "        c2[0,:] = c2avg\n",
    "        \n",
    "        sumc1 = torch.sum( c1, dim = 0 )\n",
    "        sumc2 = torch.sum( torch.sum( c2, dim = 1 ), dim = 0 )\n",
    "\n",
    "        prob1 = c1 / sumc1\n",
    "        prob2 = c2 / sumc2\n",
    "\n",
    "        self.w1 = prob1 ** -0.4\n",
    "        self.w1 = torch.nan_to_num( self.w1, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg1 = torch.sum( self.w1, dim = 0 ) / torch.sum( torch.ne( self.w1, 0.0 ).int() )\n",
    "        self.w1 = self.w1 / avg1\n",
    "\n",
    "        self.w2 = prob2 ** -0.4\n",
    "        self.w2 = torch.nan_to_num( self.w2, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg2 = torch.sum( torch.sum( self.w2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( self.w2, 0.0 ).int() )\n",
    "        self.w2 = self.w2 / avg2\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_unigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w1, f )\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_bigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w2, f )\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_unigram.pkl\", 'rb') as f:\n",
    "        #    self.w1 = pickle.load(f)\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_bigram.pkl\", 'rb') as f:\n",
    "        #    self.w2 = pickle.load(f)\n",
    "        \n",
    "        if length_max == None:\n",
    "            print( \"length max:\", self.length_max )\n",
    "            print( \"avg length:\", length_sum / len( self.tokens ) )\n",
    "    \n",
    "    # ここで取り出すデータを指定している\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        tokens = self.tokens[index]\n",
    "        img_file = self.img_file[index] + \".jpg\"\n",
    "        img_path = os.path.join( self.img_directory, img_file ) #index番目の画像のパスを取得\n",
    "        img = Image.open(img_path) #PIL形式で画像を読み込み\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        img1 = self.transforms(img)\n",
    "        img2 = self.transforms2(img)\n",
    "        \n",
    "        return img1, img2, tokens\n",
    "\n",
    "    # この method がないと DataLoader を呼び出す際にエラーを吐かれる\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def length_max(self):\n",
    "        return self.length_max\n",
    "\n",
    "    #def w1(self):\n",
    "    #    return self.w1\n",
    "\n",
    "    #def w2(self):\n",
    "    #    return self.w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, str]]], pad_index, length_max ):\n",
    "    imgs1, imgs2, tokens = zip(*batch)\n",
    "\n",
    "    max_length = length_max\n",
    "    #max_length = 0\n",
    "    #for target in tokens:\n",
    "    #    if max_length < len( target ):\n",
    "    #        max_length = len( target )\n",
    "    \n",
    "    targets = []\n",
    "    lengths = []\n",
    "    for target in tokens:\n",
    "        pad_len = max_length - len( target ) \n",
    "        #print( \"target:\", target )\n",
    "        input2= F.pad( target, (0, pad_len), mode='constant', value = pad_index)\n",
    "        targets.append( input2 )\n",
    "        lengths.append( len( target ) )\n",
    "    \n",
    "    imgs1 = torch.stack( imgs1, dim = 0 )\n",
    "    imgs2 = torch.stack( imgs2, dim = 0 )\n",
    "    targets = torch.stack( targets, dim = 0 )\n",
    "    lengths = torch.tensor( lengths, requires_grad = False  )\n",
    "\n",
    "    return imgs1, imgs2, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4x-PO05mCS-"
   },
   "source": [
    "###学習におけるハイパーパラメータやオプションの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQES3A8OG-V_"
   },
   "outputs": [],
   "source": [
    "class ConfigTrain(object):\n",
    "    '''\n",
    "    ハイパーパラメータ、システム共通変数の設定\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        # ハイパーパラメータ\n",
    "        self.img_size = 336\n",
    "        self.dim_embedding = 1024   # 埋め込み層の次元\n",
    "        self.length_max = 97\n",
    "        self.lr_clip = 0.0\n",
    "        #self.lr_con = 1.66e-10\n",
    "        #self.lr_bert = 2.19e-8          # 学習率\n",
    "        #self.lr_cri = 7.20e-5\n",
    "        #self.lr_others = 5.82e-7\n",
    "        self.lr_con = 1.66e-11\n",
    "        self.lr_bert = 2.19e-9          # 学習率\n",
    "        self.lr_cri = 7.20e-6\n",
    "        self.lr_others = 5.82e-8\n",
    "        #self.clip_thresh_clip = 1\n",
    "        self.clip_thresh_con = 3.3e-4\n",
    "        self.clip_thresh_bert = 9e-3            # 学習率\n",
    "        self.clip_thresh_cri = 3.3e-4\n",
    "        self.clip_thresh_others = 3.3e-4\n",
    "        self.dropout = 0.0         # dropout確率\n",
    "        #self.batch_size = 160       # ミニバッチ数\n",
    "        self.batch_size = 128       # ミニバッチ数\n",
    "        #self.batch_size = 80       # ミニバッチ数\n",
    "        #self.batch_size = 64\n",
    "        #self.batch_size = 48\n",
    "        #self.batch_size = 40       # ミニバッチ数\n",
    "        #self.batch_size = 4       # ミニバッチ数\n",
    "        self.num_epochs = 1       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        self.use_amp = True\n",
    "        #self.use_amp = False\n",
    "        self.use_saved_pth = True\n",
    "        #self.use_saved_pth = False\n",
    "        self.vocab_size = len( tokenizer )\n",
    "        self.weight_decay = 0.0232\n",
    "        self.betas = (0.9, 0.999 )\n",
    "        self.warmup = 0.1\n",
    "        self.metric = \"special\" # bleu, meteor, wer, rouge\n",
    "        #self.decode_t = \"ordinary\"\n",
    "        self.decode_t = \"no-pad\"\n",
    "        self.reward_t = \"ord+rep+len+unr\"\n",
    "        #self.reward_t = \"ordinary\"\n",
    "        self.clip_range = 0.105\n",
    "        self.clip_grad_threshold = 1.73\n",
    "        self.ord_coef = 1.0 #固定\n",
    "        self.cider_coef = 1.0 #固定\n",
    "        self.rouge_coef = 2.53\n",
    "        self.clip_coef = 1.65\n",
    "        self.bert_coef = 6.68\n",
    "        self.rep_coef = 5.84\n",
    "        self.repeat_thresh = [ 3,2,2,2]\n",
    "        self.repeat_weight = [ 1, 1, 1, 1 ]\n",
    "        self.len_coef = 2.10\n",
    "        self.unr_coef = 2.49\n",
    "        self.policy_coef = 1.0 #固定\n",
    "        self.crf_coef = 0.0784\n",
    "        self.ce_coef = 0.688\n",
    "        self.ent_coef = 0.00269\n",
    "        self.cri_coef = 0.0 # モンテカルロ法\n",
    "        self.gae_coef = 2.177 # GAE\n",
    "        self.kl_coef = 0.0401\n",
    "        self.target_kl = 8.0\n",
    "        self.buffer_kl = 1.2\n",
    "        self.kl_max = 0.1\n",
    "        self.kl_min = 0.1\n",
    "        self.gamma = 0.972\n",
    "        self.lam = 0.974\n",
    "        self.ratio_clamp_max = -1.0 # -1.0 ratio is free.  0.0 ratio calmp 1.1 for mainas advantage.  value > 0 ratio clamps value  \n",
    "        self.use_repeat_logits_half = False\n",
    "        self.use_ce_bert = True\n",
    "        self.display_include_coef = True\n",
    "        self.use_adaptive_KL = False\n",
    "        self.temp = 0.710\n",
    "        \n",
    "        # パスの設定\n",
    "        self.img_directory = '/mnt/ssd2/v7/img'\n",
    "        self.anno_file = '/mnt/ssd2/v7/data.pkl'\n",
    "        self.save_directory = './model'\n",
    "        #self.PATH = \"model/model_ar_hfgpt2_v7_curr.pth\"\n",
    "        #self.PATH = \"../test/model/model_bert_large_NAR_PAD_curr.pth\"\n",
    "        self.PATH = \"../pre_train_crf/model/model_bert_large_NAR_PAD_sft2_curr.pth\"\n",
    "        #self.PATH = \"model/model_bert_large_NAR_PAD_curr.pth\"\n",
    "        #self.PATH = \"model/model_bert_mask_curr.pth\"\n",
    "\n",
    "        # 検証に使う学習セット内のデータの割合\n",
    "        self.test_ratio = 0.1\n",
    "        self.val_ratio = 0.1\n",
    "        #self.val_ratio = 0.0004\n",
    "        #self.test_ratio = 0.0004\n",
    "        \n",
    "        # 学習に使うデバイス\n",
    "        #self.device = 'cuda'\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = 'cpu'\n",
    "        \n",
    "        # データローダーに使うCPUプロセスの数\n",
    "        #self.num_workers = 4\n",
    "        self.num_workers = 0 if self.device == torch.device('cpu') else 10\n",
    "        #self.num_workers = 0 if self.device == torch.device('cpu') else 4\n",
    "        #self.num_workers = 0\n",
    "        \n",
    "        # 移動平均で計算する損失の値の数\n",
    "        self.moving_avg = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--NNWCwZI5qS"
   },
   "source": [
    "### 学習を行う関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xBOP-3aIHFjB"
   },
   "outputs": [],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "config = ConfigTrain()\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = config.bert_model_path)\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "# モデル出力用のディレクトリを作成\n",
    "os.makedirs(config.save_directory, exist_ok=True)\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((336, 336)),\n",
    "    v2.AutoAugment(),\n",
    "    #v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    ## Coco データセット 2017 train の平均と標準偏差\n",
    "    #v2.Normalize((0.456,0.427,0.401),(0.224,0.219,0.231) )\n",
    "    # ImageNetデータセットの平均と標準偏差\n",
    "    #v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    # Clip Model の config から引用。\n",
    "    v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "transforms2 = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(),\n",
    "    #v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "# v7 データセット\n",
    "train_dataset = MyDataset( file_path=config.anno_file,\n",
    "                           img_directory = config.img_directory,\n",
    "                           transforms=transforms, transforms2 = transforms2,\n",
    "                           tokenizer=tokenizer, length_max = config.length_max)\n",
    "\n",
    "# Subset samplerの生成\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, config.test_ratio, config.val_ratio )\n",
    "    \n",
    "# 学習時にランダムにサンプルするためのサンプラー\n",
    "train_sampler = SubsetRandomSampler(train_set)\n",
    "#test_sampler = SubsetRandomSampler(test_set)\n",
    "\n",
    "# DataLoaderを生成\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id, config.length_max)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=train_sampler,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "#train_loader = torch.utils.data.DataLoader(\n",
    "#                    train_dataset,\n",
    "#                    #batch_size=config.batch_size,\n",
    "#                    batch_size=config.batch_size,\n",
    "#                    num_workers=config.num_workers,\n",
    "#                    sampler=test_sampler,\n",
    "#                    collate_fn=collate_func_lambda)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=val_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    #batch_size=config.batch_size,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=test_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "\n",
    "print( \"config.device:\", config.device )\n",
    "print( \"学習セット数:\",len( train_loader ) )\n",
    "print( \"評価セット数:\",len( val_loader ))\n",
    "print( \"テストセット数:\",len( test_loader ))\n",
    "print( \"use_amp:\", config.use_amp )\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "\n",
    "# モデルの定義\n",
    "#model = CaptioningTransformer(img_size = config.img_size, length_max = config.length_max,  dim_embedding=config.dim_embedding, \\\n",
    "#                              vocab_size=vocab_size, tokenizer=tokenizer, \\\n",
    "#                             dropout = config.dropout, pad_token_id = tokenizer.pad_token_id ).to(config.device)\n",
    "# モデルの定義\n",
    "model = CaptioningTransformer( config.img_size,\n",
    "    config.dim_embedding, config.length_max, config.vocab_size,\n",
    "    tokenizer, config.dropout, pad_token_id = tokenizer.pad_token_id,\n",
    "    use_repeat_logits_half = config.use_repeat_logits_half,\n",
    "    crf_coef = config.crf_coef, temp=config.temp )\n",
    "model.to(config.device)\n",
    "\n",
    "\n",
    "ref_model = CaptioningTransformer( config.img_size,\n",
    "    config.dim_embedding, config.length_max, config.vocab_size,\n",
    "    tokenizer, config.dropout, pad_token_id = tokenizer.pad_token_id,\n",
    "    use_repeat_logits_half = config.use_repeat_logits_half,\n",
    "    crf_coef = config.crf_coef, temp=1.0 )\n",
    "ref_model.to(config.device)\n",
    "\n",
    "#compute_reward = ComputeReward( reward_t = config.reward_t, decode_t = config.decode_t, device = config.device, \n",
    "#                               sentence_level_metric=config.metric, repeat_thresh = config.repeat_thresh, \n",
    "#                          repeat_weight = config.repeat_weight, cider_coef = config.cider_coef, rouge_coef = config.rouge_coef, \n",
    "#                          clip_coef = config.clip_coef, use_amp = config.use_amp )\n",
    "compute_reward = ComputeReward( reward_t = config.reward_t, decode_t = config.decode_t, device = config.device, \n",
    "                                sentence_level_metric=config.metric, repeat_thresh = config.repeat_thresh, \n",
    "                                repeat_weight = config.repeat_weight, cider_coef = config.cider_coef, \n",
    "                                rouge_coef = config.rouge_coef, clip_coef = config.clip_coef, bert_coef = config.bert_coef,\n",
    "                                use_amp = config.use_amp )\n",
    "\n",
    "\n",
    "# 最適化手法の定義\n",
    "# Optimizerの生成, clipとそうでないモジュールとの\n",
    "# パラメータで異なる学習率を適用\n",
    "#params_clip = []\n",
    "params_con = []\n",
    "params_bert = []\n",
    "params_others = []\n",
    "params_cri = []\n",
    "for name, parameter in model.named_parameters():\n",
    "    if parameter.requires_grad:\n",
    "        if 'clip_model' in name:\n",
    "            #params_clip.append(parameter)\n",
    "            parameter.requires_grad = False\n",
    "        elif 'connector' in name:\n",
    "            params_con.append(parameter)\n",
    "        elif 'bert' in name and 'critical' not in name:\n",
    "            params_bert.append(parameter)\n",
    "        elif 'critical' in name:\n",
    "            params_cri.append(parameter)\n",
    "        else:\n",
    "            params_others.append(parameter)\n",
    "param_groups = [\n",
    "    #{'params': params_clip, 'lr': config.lr_clip},\n",
    "    {'params': params_con, 'lr': config.lr_con},\n",
    "    {'params': params_bert, 'lr': config.lr_bert},\n",
    "    {'params': params_cri, 'lr': config.lr_cri},\n",
    "    {'params': params_others, 'lr': config.lr_others}]\n",
    "\n",
    "optimizer = torch.optim.AdamW( param_groups, weight_decay = config.weight_decay, betas=config.betas )\n",
    "thresh_groups = {\n",
    "#    'clip': config.clip_thresh_clip,\n",
    "    'con': config.clip_thresh_con,\n",
    "    'bert': config.clip_thresh_bert,\n",
    "    'cri': config.clip_thresh_cri,\n",
    "    'others':config.clip_thresh_others\n",
    "}\n",
    "\n",
    "# 全ステップ数\n",
    "num_global_steps = len( train_loader ) * config.num_epochs\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "print( \"warmup:\", config.warmup )\n",
    "num_warmup_steps = num_global_steps * config.warmup\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps )   \n",
    "\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "print( \"PATH:\", config.PATH )\n",
    "print( \"exist saved_pth:\", os.path.isfile(config.PATH) ) \n",
    "use_saved_pth = config.use_saved_pth\n",
    "if use_saved_pth and os.path.isfile(config.PATH):\n",
    "    checkpoint = torch.load(config.PATH, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict = False)\n",
    "    print( \"model parameters were loaded\")\n",
    "    ref_model.load_state_dict(checkpoint['model_state_dict'], strict = False)\n",
    "    ref_model.eval() # 必須：DropoutやBatchNormを無効化\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False # 必須：メモリ節約と誤学習防止\n",
    "    ref_model = ref_model.to(config.device )\n",
    "    print( \"ref_model parameters were loaded\")\n",
    "\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "    #for state in optimizer.state.values():\n",
    "        #for k, v in state.items():\n",
    "            #if isinstance(v, torch.Tensor):\n",
    "                #state[k] = v.to(device)\n",
    "    #begin_epoch = checkpoint['epoch']\n",
    "    #loss = checkpoint['loss']\n",
    "    #global_step = checkpoint['global_step']    \n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "else:\n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "file_param = 5\n",
    "print( \"begin_epoch:\", begin_epoch )\n",
    "print( \"global_step:\", global_step )\n",
    "print( \"file_param:\", file_param )\n",
    "\n",
    "def get_nearest_multiple(a, b):\n",
    "    \"\"\"\n",
    "    a に最も近い b の倍数を求める\n",
    "    \"\"\"\n",
    "    # a/b を四捨五入して、それに b を掛ける\n",
    "    # round() は .5 の場合、偶数側に丸める性質があるため、\n",
    "    # 厳密な四捨五入が必要な場合は整数演算を使用する\n",
    "    return round(a / b) * b\n",
    "\n",
    "len_tr_loader = len( train_loader )\n",
    "train_param = len_tr_loader // 30\n",
    "#train_param = len_tr_loader // 10\n",
    "len_val_loader = len( val_loader )\n",
    "#train_param = len_val_loader // 3\n",
    "train_param = get_nearest_multiple( train_param, file_param )\n",
    "val_param = len_val_loader // 10\n",
    "print( \"train_param:\", train_param )\n",
    "print( \"val_param:\", val_param )\n",
    "\n",
    "print( \"epochs:\", config.num_epochs )\n",
    "print( \"batch_size:\", config.batch_size )\n",
    "print( \"lr_clip:\", config.lr_clip )\n",
    "print( \"lr_con:\", config.lr_con )\n",
    "print( \"lr_bert:\", config.lr_bert )\n",
    "print( \"lr_cri:\", config.lr_cri )\n",
    "print( \"lr_others:\", config.lr_others )\n",
    "#print( \"clip_grad_threshold:\", config.clip_grad_threshold ) \n",
    "if config.clip_grad_threshold == 0.0:\n",
    "    print( 'clip_thresh_con:', config.clip_thresh_con )\n",
    "    print( 'clip_thresh_bert:', config.clip_thresh_bert )\n",
    "    print( 'clip_thresh_cri:', config.clip_thresh_cri )\n",
    "    print( 'clip_thresh_others:',config.clip_thresh_others )\n",
    "print( \"weight_decay:\", config.weight_decay )\n",
    "print( \"betas:\", config.betas )\n",
    "print( \"metric:\", config.metric )\n",
    "print( \"reward_type:\", config.reward_t )\n",
    "print( \"decode_type:\", config.decode_t )\n",
    "print( \"clip_range ppo clip:\", config.clip_range )\n",
    "print( \"clip_grad_threshold gradient norm:\", config.clip_grad_threshold)\n",
    "print( \"ord_coef:\", config.ord_coef )\n",
    "print( \"cider_coef:\", config.cider_coef )\n",
    "print( \"rouge_coef:\", config.rouge_coef )\n",
    "print( \"clip_coef:\", config.clip_coef )\n",
    "print( \"rep_coef:\", config.rep_coef )\n",
    "print( \"repeat_thresh:\", config.repeat_thresh )\n",
    "print( \"repeat_weight:\", config.repeat_weight )\n",
    "print( \"len_coef:\", config.len_coef )\n",
    "print( \"unr_coef:\", config.unr_coef )\n",
    "print( \"policy_coef:\", config.policy_coef )\n",
    "print( \"crf_coef:\", config.crf_coef )\n",
    "print( \"ce_coef:\", config.ce_coef )\n",
    "print( \"ent_coef:\", config.ent_coef )\n",
    "#print( \"cri_coef:\", config.cri_coef )\n",
    "print( \"gae_coef:\", config.gae_coef )\n",
    "print( \"kl_coef:\", config.kl_coef )\n",
    "print( \"target_kl:\", config.target_kl )\n",
    "print( \"buffer_kl:\", config.buffer_kl )\n",
    "print( \"kl_max:\", config.kl_max )\n",
    "print( \"kl_min:\", config.kl_min )\n",
    "print( \"gamma:\", config.gamma )\n",
    "print( \"lambda:\", config.lam )\n",
    "print( \"use_repeat_logits_half:\", config.use_repeat_logits_half )\n",
    "print( \"use_ce_bert:\", config.use_ce_bert )\n",
    "print( \"ratio_clamp_max:\", config.ratio_clamp_max )\n",
    "print( \"display_include_coef:\", config.display_include_coef )\n",
    "print( \"temp:\", config.temp )\n",
    "\n",
    "# 学習経過の書き込み\n",
    "now = datetime.datetime.now()\n",
    "train_loss_file = '{}/MyOriginal_train_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(train_loss_file, 'a') as f:\n",
    "    print(f'{len_tr_loader}', file=f) \n",
    "print( \"train_loss_file:\", train_loss_file )\n",
    "val_loss_file = '{}/MyOriginal_val_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(val_loss_file, 'a') as f:\n",
    "    print(f'{len_val_loader}', file=f) \n",
    "norm_file = '{}/norm_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "# 学習\n",
    "val_loss_best = float('inf')\n",
    "\n",
    "#fn = bleu_score.SmoothingFunction().method7\n",
    "#bleu_func = BLEU(effective_order=\"True\")\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# AMP用のスケーラー\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "eps = 1e-8\n",
    "last_sample_log_probs = 0\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "def entropy_func(probs):\n",
    "    input_probs = torch.clamp( probs, eps )\n",
    "    log_probs = torch.log( input_probs )\n",
    "    p_log_p = probs * log_probs\n",
    "    return - p_log_p.sum(-1)\n",
    "\n",
    "def baseline( probs, targets, top_k ):\n",
    "    probs_k, preds_k = torch.topk( probs, dim = 2, k = top_k )\n",
    "    renorm_probs_k = probs_k / torch.sum( probs_k, dim = 2 )[:,:, None]\n",
    "    base_ = torch.stack( [ probs_k[:,:,k] * compue_reward._compute_reward(preds_k[:,:,k], targets, sources = None ) \\\n",
    "                        for k in range( top_k ) ], dim = 0 )\n",
    "    base = torch.sum( base_, dim = 0 )\n",
    "    \n",
    "    return base\n",
    "\n",
    "def custom_gradient_clipping(clip_params, gpt2_params, cri_params, others_params, \n",
    "                             clip_threshold, gpt2_threshold, cri_threshold, others_threshold):\n",
    "    # エンコーダーの勾配クリッピング\n",
    "    if clip_params:\n",
    "        # torch.nn.utils.clip_grad_norm_ は、与えられたパラメータのリストに勾配クリッピングを適用する\n",
    "        torch.nn.utils.clip_grad_norm_(clip_params, clip_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if gpt2_params:\n",
    "        torch.nn.utils.clip_grad_norm_(gpt2_params, gpt2_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if cri_params:\n",
    "        torch.nn.utils.clip_grad_norm_(cri_params, cri_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if gpt2_params:\n",
    "        torch.nn.utils.clip_grad_norm_(others_params, others_threshold)\n",
    "\n",
    "def generalized_advantage_estimation(rewards, values, gamma=0.99, lam=0.95):\n",
    "\n",
    "    B, T = rewards.size()\n",
    "    \n",
    "    # 各タイムステップでのTD誤差 (delta_t) を計算\n",
    "    # delta_t = R_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "    # values[:, :-1] は V(s_t)、values[:, 1:] は V(s_{t+1}) に対応\n",
    "    deltas = rewards + gamma * values[:, 1:] - values[:, :-1]\n",
    "    # deltas の形状は (B, T)\n",
    "\n",
    "    advantages = torch.zeros((B, T), device=rewards.device, dtype=torch.float)\n",
    "    \n",
    "    # 最後のタイムステップのadvantageは delta_t そのもの\n",
    "    advantages[:, T-1] = deltas[:, T-1]\n",
    "    \n",
    "    for t in range(T - 2, -1, -1):\n",
    "        advantages[:, t] = deltas[:, t] + gamma * lam * advantages[:, t+1]\n",
    "        \n",
    "    return advantages\n",
    "    \n",
    "#if config.metric == 'rouge':\n",
    "#    rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def my_index( list1, target ):\n",
    "    if target in list1:\n",
    "        return list1.index( target )\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    with tqdm(train_loader) as pbar:\n",
    "    #with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[エポック {epoch + 1}]')\n",
    "\n",
    "        # 学習モードに設定\n",
    "        model.train()\n",
    "\n",
    "        train_losses = deque()\n",
    "        train_policys = deque()\n",
    "        train_entropies = deque()\n",
    "        train_critics = deque()\n",
    "        train_kl_divs = deque()\n",
    "        train_rewards = deque()\n",
    "        train_rewards2 = deque()\n",
    "        train_ord = deque()\n",
    "        train_repeat = deque()\n",
    "        train_length = deque()\n",
    "        train_adv = deque()\n",
    "        train_errors = deque()\n",
    "        train_bleus = deque()\n",
    "        train_crfs = deque()\n",
    "        train_ces = deque()\n",
    "        train_clips = deque()\n",
    "        train_unrs = deque()\n",
    "        train_berts = deque()\n",
    "        for n_batch, (imgs, imgs2, captions, caption_lengths) in enumerate( pbar ):\n",
    "            #print( \"captions[0]:\", captions[0] )\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            imgs2 = imgs2.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "\n",
    "            if imgs.size(0) != config.batch_size:\n",
    "                print( f\"bsz {imgs.size(0)} is not batch_size {config.batch_size}. skip\")\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 最後の単語から次を予測する必要はないため最後の単語を除外\n",
    "            with autocast(str(config.device),enabled=config.use_amp):\n",
    "                finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "                critical_value, crf_loss, bert_logits, sampled_beam_idx  = \\\n",
    "                model( imgs, captions, top_indices = None )\n",
    "                bsz, seq_len, beam = top_probs.size()\n",
    "                if config.use_ce_bert == False:\n",
    "                    ce_tensor = torch.full((bsz, seq_len, vocab_size), float(eps), device=config.device)\n",
    "                    ce_tensor = torch.scatter( ce_tensor, 2, top_indices, top_probs )\n",
    "                    ce_tensor = torch.clamp( ce_tensor, eps )\n",
    "                    log_ce_tensor = torch.log( ce_tensor )\n",
    "                hypo_ids = finalized_tokens\n",
    "                reward_ord, reward_ord2, reward_repeat, reward_length, reward_unr, preds, sample_log_probs \\\n",
    "                    = compute_reward( top_probs, sampled_beam_idx, top_indices, captions, imgs2 )\n",
    "                with torch.no_grad():\n",
    "                    #if config.use_inference == True:\n",
    "                    #    _, ref_captions, _, _, _, _ = model.inference( imgs )\n",
    "                    #else:\n",
    "                    #    ref_probs = F.softmax( logits, dim = 2 )\n",
    "                    #    ref_captions = torch.multinomial( ref_probs.view( bsz * seq_len, -1 ), num_samples = 1 ).view( bsz, seq_len )\n",
    "                    #ref_logits, _ = ref_model( imgs )\n",
    "                    #ref_top_logits = torch.gather( ref_logits, -1, top_indices )\n",
    "                    #_, ref_top_probs, _ = model.toplayer( ref_logits, ref_top_logits, top_indices, \\\n",
    "                    #                                                   ref_top_logits, ref_captions, is_training = True )\n",
    "                    ref_captions = preds\n",
    "                    _, _, ref_top_probs, _, _, _, _, _ = ref_model( imgs, ref_captions, top_indices = top_indices )\n",
    "\n",
    "                \n",
    "                \n",
    "                # 1. Policy側の対数確率（学習対象）\n",
    "                tmp = torch.clamp( top_probs, min = eps )\n",
    "                top_log_probs = torch.log( tmp )\n",
    "                policy_lp = torch.gather(top_log_probs, -1, sampled_beam_idx).squeeze(-1) # lp は log_prob の略と思われる。 bsz * seq_len\n",
    "                \n",
    "                # 2. Reference側の対数確率（固定）\n",
    "                tmp = torch.clamp( ref_top_probs, min = eps )\n",
    "                ref_top_log_probs = torch.log(tmp)\n",
    "                ref_lp = torch.gather(ref_top_log_probs, -1, sampled_beam_idx).squeeze(-1)\n",
    "                \n",
    "                if config.decode_t == \"no-pad\":\n",
    "                    lengths = []\n",
    "                    for pred in preds:\n",
    "                        length = my_index( pred.tolist(), eos_token_id )\n",
    "                        if length != -1:\n",
    "                            lengths.append( length )\n",
    "                        else:\n",
    "                            lengths.append( 0 )\n",
    "                    lengths = torch.tensor( lengths, device = config.device )[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "                    arange1 = torch.arange( 0, preds.size(1), device = config.device )\n",
    "                    arange1 = arange1[None,:].expand( preds.size(0), -1 )\n",
    "                    \n",
    "                    masks = arange1 < ( lengths + 2 )\n",
    "                \n",
    "                    kl_divs = ( policy_lp - ref_lp ) * masks.float()\n",
    "                else:\n",
    "                    kl_divs = policy_lp - ref_lp \n",
    "                \n",
    "                rewards = config.ord_coef * reward_ord + config.rep_coef * reward_repeat + config.len_coef * reward_length \\\n",
    "                    + config.unr_coef * reward_unr\n",
    "                rewards2 = reward_ord2 + reward_repeat + reward_length + reward_unr\n",
    "                \n",
    "                if global_step == 0:\n",
    "                    last_sample_log_probs = sample_log_probs.detach()\n",
    "        \n",
    "                ratio = torch.exp(sample_log_probs - last_sample_log_probs[:sample_log_probs.size(0)]) # bsz * seq_len\n",
    "                rewards_t = torch.zeros( ( bsz, seq_len ) , device = config.device, dtype = torch.float)\n",
    "                rewards_t[:,-1] = rewards[:,0]\n",
    "                values = critical_value\n",
    "                zeros = torch.zeros((values.size(0), 1), device=values.device, dtype=torch.float)\n",
    "                values_cat = torch.cat((values, zeros), dim=1)\n",
    "                advantages = generalized_advantage_estimation(rewards_t, values_cat, gamma=config.gamma, lam=config.lam)\n",
    "                advantages_norm = (advantages - advantages.mean()) / (advantages.std() + eps) # bsz * seq_len\n",
    "        \n",
    "                if config.ratio_clamp_max == -1.0:\n",
    "\n",
    "                    # clipped surrogate loss\n",
    "                    policy_loss_1 = - advantages_norm * ratio\n",
    "                    policy_loss_2 = - advantages_norm * torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range)\n",
    "                \n",
    "                    policy_loss = - torch.min(policy_loss_1, policy_loss_2).mean() # bsz * seq_len の平均\n",
    "                \n",
    "                elif config.ratio_clamp_max != 0.0:                \n",
    "        \n",
    "                    ratio = torch.clamp( ratio , min = 0.0, max = config.ratio_clamp_max )\n",
    "                \n",
    "                    # clipped surrogate loss\n",
    "                    policy_loss_1 = - advantages_norm * ratio\n",
    "                    policy_loss_2 = - advantages_norm * torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range)\n",
    "                \n",
    "                    #policy_loss = - torch.min(policy_loss_1, policy_loss_2).mean() # bsz * seq_len の平均\n",
    "                    if config.decode_t == \"no-pad\":\n",
    "                        policy_loss = ( torch.min( policy_loss_1, policy_loss_2) )\n",
    "                        policy_loss = policy_loss.sum() / bsz / masks.float().sum()\n",
    "                    else:\n",
    "                        policy_loss = ( torch.min( policy_loss_1, policy_loss_2))\n",
    "                        policy_loss = policy_loss.sum() / bsz / seq_len\n",
    "                else:\n",
    "                    mul_minus = ( advantages_norm > 0.0 ).float() * 2.0 - 1.0 \n",
    "                \n",
    "                    # clipped surrogate loss\n",
    "                    policy_loss_1 = - advantages_norm * ratio\n",
    "                    policy_loss_2 = - advantages_norm * torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range)\n",
    "                \n",
    "                    if config.decode_t == \"no-pad\":\n",
    "                        policy_loss_before = ( torch.min( torch.abs(policy_loss_1), torch.abs( policy_loss_2)) )\n",
    "                        policy_loss = ( policy_loss_before * mul_minus ).sum() / bsz / masks.float().sum()\n",
    "                    else:\n",
    "                        policy_loss_before = ( torch.min(torch.abs(policy_loss_1), torch.abs(policy_loss_2)))\n",
    "                        policy_loss = (policy_loss_before * mul_minus).sum() / bsz / seq_len\n",
    "\n",
    "                entropy = entropy_func(top_probs)\n",
    "                entropy_loss = - torch.mean(entropy)\n",
    "\n",
    "                targets = (values + advantages).detach()\n",
    "                gae_loss = nn.MSELoss()( targets, values )\n",
    "\n",
    "                kl_per_sample = torch.sum(kl_divs, dim=1) / (torch.sum(masks, dim=1) + 1e-8)\n",
    "                kl_div_loss = torch.mean(kl_per_sample)\n",
    "                if config.use_adaptive_KL:\n",
    "                    if n_batch % 100 == 0:\n",
    "                        if kl_div_loss < config.target_kl / config.buffer_kl:\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                            print(f\"Update beta (low KL): {config.kl_coef} -> {config.kl_coef / 2.0}\")\n",
    "                            config.kl_coef = max(config.kl_coef / 2.0, config.beta_min) # 下限 0.05\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                        elif kl_div_loss > config.target_kl * config.buffer_kl:\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                            print(f\"Update beta (high KL): {config.kl_coef} -> {config.kl_coef * 2.0}\")\n",
    "                            config.kl_coef = min(config.kl_coef * 2.0, config.beta_max)  # 上限 5.0\n",
    "                            print(f\"bklcoef:\", config.kl_coef )\n",
    "                \n",
    "                loss =  config.policy_coef * policy_loss \\\n",
    "                    + config.ent_coef * entropy_loss + config.gae_coef * gae_loss + config.kl_coef * kl_div_loss\n",
    "                if config.crf_coef != 0.0:\n",
    "                    loss =  loss + config.crf_coef * crf_loss\n",
    "                if config.ce_coef != 0.0:\n",
    "                    if config.use_ce_bert:\n",
    "                        ce_loss = nn.CrossEntropyLoss()( bert_logits.transpose(1,2), captions )\n",
    "                    else:\n",
    "                        ce_loss = nn.NLLLoss()( log_ce_tensor.view( bsz * seq_len, -1 ), captions.view( bsz * seq_len ) )\n",
    "                    loss =  loss + config.ce_coef * ce_loss\n",
    "               \n",
    "            with torch.no_grad():\n",
    "                last_sample_log_probs = sample_log_probs.detach()\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            loss_item = loss.item()\n",
    "            reward_item = rewards.mean().item()\n",
    "            reward2_item = rewards2.mean().item()\n",
    "            adv_item = advantages.mean().item()\n",
    "            if config.display_include_coef:\n",
    "                policy_item = config.policy_coef * policy_loss.item()\n",
    "                entropy_item = config.ent_coef * entropy_loss.item()\n",
    "                critic_item = config.gae_coef * gae_loss.item()\n",
    "                #critic_item = 0\n",
    "                kl_div_item = config.kl_coef * kl_div_loss.item()\n",
    "                ord_item = config.ord_coef * reward_ord.mean().item()\n",
    "                repeat_item = config.rep_coef * reward_repeat.mean().item()\n",
    "                length_item = config.len_coef * reward_length.mean().item()\n",
    "                if config.crf_coef != 0.0:\n",
    "                    crf_item = config.crf_coef * crf_loss.mean().item()\n",
    "                else:\n",
    "                    crf_item = 0.0\n",
    "                if config.ce_coef != 0.0:\n",
    "                    ce_item = config.ce_coef * ce_loss.mean().item()\n",
    "                else:\n",
    "                    ce_item = 0.0\n",
    "                unr_item = config.unr_coef * reward_unr.mean().item()\n",
    "            else:\n",
    "                policy_item = policy_loss.item()\n",
    "                entropy_item = entropy_loss.item()\n",
    "                critic_item = gae_loss.item()\n",
    "                #critic_item = 0\n",
    "                kl_div_item = kl_div_loss.item()\n",
    "                ord_item = reward_ord.mean().item()\n",
    "                repeat_item = reward_repeat.mean().item()\n",
    "                length_item = reward_length.mean().item()\n",
    "                if config.crf_coef != 0.0:\n",
    "                    crf_item = crf_loss.mean().item()\n",
    "                else:\n",
    "                    crf_item = 0.0\n",
    "                if config.ce_coef != 0.0:\n",
    "                    ce_item = ce_loss.mean().item()\n",
    "                else:\n",
    "                    ce_item = 0.0\n",
    "                unr_item = reward_unr.mean().item()\n",
    "            del loss, rewards, reward_ord, reward_repeat, reward_length\n",
    "            del policy_loss, policy_loss_1, policy_loss_2, entropy_loss, kl_div_loss\n",
    "            del sample_log_probs, advantages, ratio\n",
    "            del finalized_scores, finalized_tokens\n",
    "            del kl_divs, masks, kl_per_sample\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            if config.clip_grad_threshold != 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(\\\n",
    "                   model.parameters(),\n",
    "                   config.clip_grad_threshold)\n",
    "            else:\n",
    "                custom_gradient_clipping(\n",
    "                    #params_clip, \n",
    "                    params_con, \n",
    "                    params_bert,\n",
    "                    params_cri,\n",
    "                    params_others,\n",
    "                    #thresh_groups['clip'], \n",
    "                    thresh_groups['con'], \n",
    "                    thresh_groups['bert'], \n",
    "                    thresh_groups['cri'], \n",
    "                    thresh_groups['others'], \n",
    "                )\n",
    "                      \n",
    "            # オプティマイザにより，パラメータを更新する\n",
    "            #for i, ( name, param ) in enumerate(model.named_parameters()):\n",
    "            #    #params.grad = grad[i]\n",
    "            #    print( \"parameter name:\", name )\n",
    "            #norm0 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[0].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            #norm1 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[12].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            norm0 = torch.norm( model.ln_critical.weight.grad, p = 2 ).item()\n",
    "            norm1 = torch.norm( model.linear_critical.weight.grad, p = 2 ).item()\n",
    "            norm2 = torch.norm( model.bert.encoder.layer[0].attention.self.query.weight.grad, p = 2 ).item()\n",
    "            norm3 = torch.norm( model.bert.encoder.layer[23].attention.self.query.weight.grad, p = 2 ).item()\n",
    "            mean_norm = torch.mean( torch.stack ([  torch.norm( param.grad, p = 2 ) \\\n",
    "                                                  for param in model.parameters() if param.grad is not None ] ) ).item()\n",
    "            #total_norm = torch.nn.utils.clip_grad_norm_(params_bert, thresh_groups['bert']).item()\n",
    "            #print( norm0, norm1, norm2, norm3, norm_mean )\n",
    "            with open(norm_file, 'a') as f:\n",
    "                print( \"epcoch:\", epoch, \", step:\", global_step, \", norm0:\", norm0, \", norm1:\", norm1, \", norm2:\", norm2, \\\n",
    "                       \", norm3:\", norm3, \", mean_norm:\", mean_norm, file=f  )\n",
    "                f.flush()\n",
    "        \n",
    "            #optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()            \n",
    "            scheduler.step()\n",
    "            #end_time = time.time()\n",
    "            #print( \"step time:\", end_time - start_time )\n",
    "            \n",
    "            if global_step % file_param == 0:                \n",
    "                #start_time = time.time()\n",
    "                hypo_sentence1 = []\n",
    "                ref_sentence1 = []\n",
    "                if config.metric == 'cider' or config.metric == 'special':\n",
    "                    if config.decode_t == 'no-endoftext':\n",
    "                        preds_str = [tokenizer.decode(\n",
    "                            [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                            ) for pred in hypo_ids]\n",
    "                        samps_str = [tokenizer.decode(\n",
    "                            [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                            ) for pred in preds]\n",
    "                        targets_str = [tokenizer.decode(\n",
    "                            [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                            ) for target in captions]\n",
    "                    elif config.decode_t == 'no-pad':\n",
    "                        preds_str = [\n",
    "                            tokenizer.decode([i for i in pred if i != eos_token_id \\\n",
    "                            and i != pad_token_id ] ) \n",
    "                            for pred in hypo_ids\n",
    "                        ]\n",
    "                        preds_str2 = [\n",
    "                            tokenizer.decode([i for i in pred if i != eos_token_id], skip_special_tokens=True) \n",
    "                            for pred in hypo_ids\n",
    "                        ]\n",
    "                        samps_str = [tokenizer.decode(\n",
    "                            [ i for i in pred \\\n",
    "                             if i != pad_token_id \\\n",
    "                             and i != eos_token_id ] \\\n",
    "                            ) for pred in preds]\n",
    "                        targets_str = [tokenizer.decode(\n",
    "                            [ i for i in target \\\n",
    "                            if i != pad_token_id \\\n",
    "                             and i != eos_token_id ]       \n",
    "                            ) for target in captions]\n",
    "                    else:\n",
    "                        preds_str = [tokenizer.decode(pred) for pred in hypo_ids]\n",
    "                        targets_str = [tokenizer.decode(target) for target in captions]\n",
    "                    pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "                    target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "                    avg_bleu, scores = compute_reward.scorer.compute_score(target_dict, pred_dict) # cider の計算\n",
    "                    #avg_error = compute_reward.meteor.compute(predictions=preds_str, references=targets_str)['meteor']\n",
    "                    #avg_error = bleu_score.corpus_bleu( targets_str, preds_str, smoothing_function=fn  )\n",
    "                    rouge_scores = [compute_reward.rougeL.score(target, pred)['rougeL'][0] for pred, target in zip(preds_str, targets_str)]\n",
    "                    avg_error = sum( rouge_scores ) / len( rouge_scores )\n",
    "                    #clip_scores = [compute_reward.metric( img2, pred).detach() for img2, pred in zip( imgs2, preds_str2 )]\n",
    "                    with autocast(str(config.device),enabled=config.use_amp):\n",
    "                        with torch.no_grad():\n",
    "                            clip_score = compute_reward.metric( imgs2, preds_str2 ) / 100.0\n",
    "                    bert_scores = compute_reward.bert.compute(predictions=preds_str, references=targets_str, model_type=model_name, \\\n",
    "                                                              lang='en',  device=config.device)['f1']\n",
    "                    bert_scores = [score for score in bert_scores]\n",
    "                    bert_score = sum( bert_scores ) / len( bert_scores )\n",
    "                    if  global_step % train_param == 0:\n",
    "                        hypo_sentence1 = [preds_str[0]]\n",
    "                        samp_sentence1 = [samps_str[0]]\n",
    "                        ref_sentence1 = [targets_str[0]]\n",
    "\n",
    "\n",
    "                #if config.display_include_coef:                \n",
    "                #    avg_bleu = config.cider_coef *  avg_bleu\n",
    "                #    avg_error = config.rouge_coef * avg_error\n",
    "                #    clip_score = config.clip_coef * clip_score\n",
    "                #    bert_score = config.bert_coef * bert_score\n",
    "    \n",
    "                            \n",
    "                # 学習時の損失をログに書き込み\n",
    "                #エポック内の平均\n",
    "                train_losses.append(loss_item)\n",
    "                train_policys.append(policy_item)\n",
    "                train_entropies.append(entropy_item)\n",
    "                train_critics.append(critic_item)\n",
    "                train_kl_divs.append(kl_div_item)\n",
    "                train_rewards.append(reward_item)\n",
    "                train_rewards2.append(reward2_item)\n",
    "                train_ord.append(ord_item)\n",
    "                train_repeat.append(repeat_item)\n",
    "                train_length.append(length_item)\n",
    "                train_adv.append(adv_item)\n",
    "                train_errors.append( avg_error )\n",
    "                train_bleus.append( avg_bleu )\n",
    "                train_crfs.append( crf_item )\n",
    "                train_ces.append( ce_item )\n",
    "                train_clips.append( clip_score )\n",
    "                train_unrs.append( unr_item )\n",
    "                train_berts.append( bert_score )\n",
    "                if len(train_losses) > config.moving_avg:\n",
    "                    train_losses.popleft()\n",
    "                    train_policys.popleft()\n",
    "                    train_entropies.popleft()\n",
    "                    train_critics.popleft()\n",
    "                    train_kl_divs.popleft()\n",
    "                    train_rewards.popleft()\n",
    "                    train_rewards2.popleft()\n",
    "                    train_ord.popleft()\n",
    "                    train_repeat.popleft()\n",
    "                    train_length.popleft()\n",
    "                    train_adv.popleft()\n",
    "                    train_errors.popleft()\n",
    "                    train_bleus.popleft()\n",
    "                    train_crfs.popleft()\n",
    "                    train_ces.popleft()\n",
    "                    train_clips.popleft()\n",
    "                    train_unrs.popleft()\n",
    "                    train_berts.popleft()\n",
    "                mean_loss = torch.Tensor(train_losses).mean().item()\n",
    "                mean_policy = torch.Tensor(train_policys).mean().item()\n",
    "                mean_entropy = torch.Tensor(train_entropies).mean().item()\n",
    "                mean_critic = torch.Tensor(train_critics).mean().item()\n",
    "                mean_kl_div = torch.Tensor(train_kl_divs).mean().item()\n",
    "                mean_reward = torch.Tensor(train_rewards).mean().item()\n",
    "                mean_reward2 = torch.Tensor(train_rewards2).mean().item()\n",
    "                mean_ord = torch.Tensor(train_ord).mean().item()\n",
    "                mean_repeat = torch.Tensor(train_repeat).mean().item()\n",
    "                mean_length = torch.Tensor(train_length).mean().item()\n",
    "                mean_adv = torch.Tensor(train_adv).mean().item()\n",
    "                mean_error = torch.Tensor(train_errors).mean().item()\n",
    "                mean_bleu = torch.Tensor(train_bleus).mean().item()\n",
    "                mean_crf = torch.Tensor(train_crfs).mean().item()\n",
    "                mean_ce = torch.Tensor(train_ces).mean().item()\n",
    "                mean_clip = torch.Tensor(train_clips).mean().item()\n",
    "                mean_unr = torch.Tensor(train_unrs).mean().item()\n",
    "                mean_bert = torch.Tensor(train_berts).mean().item()\n",
    "                #print( \"mean_reward2:\", mean_reward2 ) \n",
    "                pbar.set_postfix({\n",
    "                    'loss': mean_loss,\n",
    "                    'policy': mean_policy,\n",
    "                    'entropy': mean_entropy,\n",
    "                    'gae': mean_critic,\n",
    "                    'kl_div': mean_kl_div,\n",
    "                    'reward': mean_reward,\n",
    "                    'reward2': mean_reward2,\n",
    "                    'ord': mean_ord,\n",
    "                    'repeat': mean_repeat,\n",
    "                    'length': mean_length,\n",
    "                    'adv': mean_adv,\n",
    "                    'rougeL': mean_error,\n",
    "                    'cider': mean_bleu,\n",
    "                    'crf': mean_crf,\n",
    "                    'ce': mean_ce,\n",
    "                    'clip': mean_clip,\n",
    "                    'unr': mean_unr,\n",
    "                    'bert': mean_bert,\n",
    "                })\n",
    "                with open(train_loss_file, 'a') as f:\n",
    "                    print(f' {global_step}, {mean_loss}, {mean_policy}, {mean_entropy}, {mean_critic}, {mean_kl_div}, {mean_reward}, ' \\\n",
    "                          f'{mean_ord}, {mean_repeat}, {mean_length}, {mean_adv}, {mean_error}, {mean_bleu}, {mean_crf}, {mean_ce}, '\\\n",
    "                          f'{mean_clip}, {mean_unr}, {mean_bert}', file=f)\n",
    "                print_flag = 1\n",
    "                for ( hypo_se, ref_se, samp_se ) in zip( hypo_sentence1, ref_sentence1, samp_sentence1 ):\n",
    "                    if print_flag == 1:\n",
    "                        print( \"lr con   :\", optimizer.param_groups[0][\"lr\"] )\n",
    "                        print( \"lr bert  :\", optimizer.param_groups[1][\"lr\"] )\n",
    "                        print( \"lr cri   :\", optimizer.param_groups[2][\"lr\"] )\n",
    "                        print( \"lr others:\", optimizer.param_groups[3][\"lr\"] )\n",
    "                        print_flag = 0\n",
    "                    print(f'Train epoch = {global_step/len_tr_loader}, loss = {mean_loss}, policy = {mean_policy}, '\\\n",
    "                          f'entropy_loss = {mean_entropy}, gae = {mean_critic}, kl_div = {mean_kl_div}, reward = {mean_reward}, '\\\n",
    "                          f'ord = {mean_ord}, repeat = {mean_repeat}, length = {mean_length}, adv = {mean_adv}, '\\\n",
    "                          f'rougeL = {mean_error}, cider = {mean_bleu}, clip = {mean_clip}, crf = {mean_crf}, ce = {mean_ce}, '\\\n",
    "                          f'unr = {mean_unr}, ber = {mean_bert}' )\n",
    "                    print( \"refe:\", ref_se )\n",
    "                    print( \"hypo:\", hypo_se )\n",
    "                    print( \"samp:\", samp_se )\n",
    "                #end_time = time.time()\n",
    "                        #print( \"file time:\", end_time - start_time )\n",
    "\n",
    "            global_step += 1\n",
    "    #各値を表示\n",
    "    print(f'Train loss: {mean_loss}')\n",
    "    print(f'Train policy: {mean_policy}')\n",
    "    print(f'Train entropy: {mean_entropy}')\n",
    "    print(f'Train gae: {mean_critic}')\n",
    "    print(f'Train kl_div: {mean_kl_div}')\n",
    "    print(f'Train reward: {mean_reward}')\n",
    "    print(f'Train reward2: {mean_reward2}')\n",
    "    print(f'Train ord: {mean_ord}')\n",
    "    print(f'Train repeat: {mean_repeat}')\n",
    "    print(f'Train pad: {mean_length}')\n",
    "    print(f'Train adv: {mean_adv}')\n",
    "    print(f'Train clip: {mean_clip}')\n",
    "    print(f'Train rougeL: {mean_error}')        \n",
    "    print(f'Train cider: {mean_bleu}')\n",
    "    print(f'Train crf: {mean_crf}')        \n",
    "    print(f'Train ce: {mean_ce}')\n",
    "    print(f'Train unr: {mean_unr}')        \n",
    "    print(f'Train bert: {mean_bert}')\n",
    "    \n",
    "    \n",
    "    # 検証\n",
    "    with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[検証]')\n",
    "\n",
    "        # 評価モード\n",
    "        model.eval()\n",
    "\n",
    "        #val_losses = deque()\n",
    "        #val_rewards = deque()\n",
    "        val_errors = deque()\n",
    "        val_bleus = deque()\n",
    "        val_clips = deque()\n",
    "        val_berts = deque()\n",
    "        for n_batch, (imgs, imgs2, captions, caption_lengths) in enumerate( pbar ):\n",
    "\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            imgs2 = imgs2.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "                critical_value, crf_loss, bert_logits, sampled_beam_idx  = \\\n",
    "                model( imgs, captions, top_indices = None )\n",
    "                hypo_ids = finalized_tokens\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "               \n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            n2 = 0\n",
    "            if config.metric == 'cider' or config.metric == 'special':\n",
    "                if config.decode_t == 'no-endoftext':\n",
    "                    preds_str = [tokenizer.decode(\n",
    "                        [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                        ) for pred in hypo_ids]\n",
    "                    samps_str = [tokenizer.decode(\n",
    "                        [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                        ) for pred in preds]\n",
    "                    targets_str = [tokenizer.decode(\n",
    "                        [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                        ) for target in captions]\n",
    "                elif config.decode_t == 'no-pad':\n",
    "                    preds_str = [\n",
    "                        tokenizer.decode([i for i in pred if i != eos_token_id \\\n",
    "                        and i != pad_token_id ] ) \n",
    "                        for pred in hypo_ids\n",
    "                    ]\n",
    "                    preds_str2 = [\n",
    "                        tokenizer.decode([i for i in pred if i != eos_token_id], skip_special_tokens=True) \n",
    "                        for pred in hypo_ids\n",
    "                    ]\n",
    "                    samps_str = [tokenizer.decode(\n",
    "                        [ i for i in pred \\\n",
    "                         if i != pad_token_id \\\n",
    "                         and i != eos_token_id ] \\\n",
    "                        ) for pred in preds]\n",
    "                    targets_str = [tokenizer.decode(\n",
    "                        [ i for i in target \\\n",
    "                        if i != pad_token_id \\\n",
    "                         and i != eos_token_id ]       \n",
    "                        ) for target in captions]\n",
    "                else:\n",
    "                    preds_str = [tokenizer.decode(pred) for pred in hypo_ids]\n",
    "                    targets_str = [tokenizer.decode(target) for target in captions]\n",
    "                pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "                target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "                avg_bleu, scores = compute_reward.scorer.compute_score(target_dict, pred_dict) # cider の計算\n",
    "                #avg_error = compute_reward.meteor.compute(predictions=preds_str, references=targets_str)['meteor']\n",
    "                #avg_error = bleu_score.corpus_bleu( targets_str, preds_str, smoothing_function=fn  )\n",
    "                rouge_scores = [compute_reward.rougeL.score(target, pred)['rougeL'][0] for pred, target in zip(preds_str, targets_str)]\n",
    "                avg_error = sum( rouge_scores ) / len( rouge_scores )\n",
    "                #clip_scores = [compute_reward.metric( img2, pred).detach() for img2, pred in zip( imgs2, preds_str2 )]\n",
    "                with autocast(str(config.device),enabled=config.use_amp):\n",
    "                    with torch.no_grad():\n",
    "                        clip_score = compute_reward.metric( imgs2, preds_str2 ) / 100.0\n",
    "                        bert_scores = compute_reward.bert.compute(predictions=preds_str, references=targets_str, model_type=model_name, \\\n",
    "                                                          lang='en',  device=config.device)['f1']\n",
    "                        bert_score = sum( bert_scores ) / len( bert_scores )\n",
    "                if n_batch % val_param == 0:\n",
    "                    hypo_sentence1 = [preds_str[0]]\n",
    "                    samp_sentence1 = [samps_str[0]]\n",
    "                    ref_sentence1 = [targets_str[0]]\n",
    "\n",
    "\n",
    "            #if config.display_include_coef:                \n",
    "            #    avg_bleu = config.cider_coef *  avg_bleu\n",
    "            #    avg_error = config.rouge_coef * avg_error\n",
    "            #    clip_score = config.clip_coef * clip_score\n",
    "            #    bert_score = config.bert_coef * bert_score\n",
    "\n",
    "            val_errors.append( avg_error )\n",
    "            val_bleus.append( avg_bleu )\n",
    "            val_clips.append( clip_score )\n",
    "            val_berts.append( bert_score )\n",
    "            if len(val_errors) > config.moving_avg:\n",
    "                #val_losses.popleft()\n",
    "                #val_rewards.popleft()\n",
    "                val_errors.popleft()\n",
    "                val_bleus.popleft()\n",
    "                val_clips.popleft()\n",
    "                val_berts.popleft()\n",
    "             #mean_loss = torch.Tensor(val_losses).mean().item()\n",
    "            #mean_reward = torch.Tensor(val_rewards).mean().item()\n",
    "            mean_error = torch.Tensor(val_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(val_bleus).mean().item()\n",
    "            mean_clip = torch.Tensor(val_clips).mean().item()\n",
    "            mean_bert = torch.Tensor(val_berts).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                #'loss': mean_loss,\n",
    "                #'reward': mean_reward,\n",
    "                'rougeL': mean_error,\n",
    "                'CIDER': mean_bleu,\n",
    "                'clip': mean_clip,\n",
    "                'bert': mean_bert,\n",
    "            })\n",
    "            # Validation Lossをログに書き込み\n",
    "            with open(val_loss_file, 'a') as f:\n",
    "                print(f'{epoch}, {mean_error}, {mean_bleu}, {mean_clip}, {mean_bert}', file=f)\n",
    "            \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                print(f'Val epoch = {epoch}, rougeL = {mean_error}, cider = {mean_bleu}, clip = {mean_clip}, bert = {mean_bert}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "   \n",
    "    # Loss 表示\n",
    "    #print(f'Validation loss: {val_loss}')\n",
    "    #print(f'Validation loss: {val_reward}')\n",
    "    print(f'Validation rougeL: {mean_error}')\n",
    "    print(f'Validation cider: {mean_bleu}')\n",
    "    print(f'Validation clip: {mean_clip}')\n",
    "    print(f'Validation bert: {mean_bert}')\n",
    "    \n",
    "    ## より良い検証結果が得られた場合、モデルを保存\n",
    "            \n",
    "    # モデルを保存\n",
    "    torch.save({'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        #'loss': loss,\n",
    "        },\n",
    "        f'{config.save_directory}/model_rl_ppo_critic_crf32_52_curr.pth')\n",
    "    ## モデルを保存\n",
    "## モデルを保存\n",
    "#torch.save({'epoch': epoch,\n",
    "#    'global_step': global_step,\n",
    "#    'model_state_dict': model.state_dict(),\n",
    "#    'optimizer_state_dict': optimizer.state_dict(),\n",
    "#    'scheduler_state_dict': scheduler.state_dict(),\n",
    "#    #'loss': loss,\n",
    "#    },\n",
    "#    f'{config.save_directory}/model_rl_ppo_critic_crf32_final.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存\n",
    "torch.save({'epoch': epoch,\n",
    "    'global_step': global_step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    #'loss': loss,\n",
    "    },\n",
    "    f'{config.save_directory}/model_rl_ppo_critic_crf_ontheway.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10ac0991cf0d4346a2dd950c4e69de74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc134416b78b4e16ab143f7b3c9b4db1",
      "placeholder": "​",
      "style": "IPY_MODEL_3e3ad9c31b8244d9987cb5d09ac80909",
      "value": " 230M/230M [00:03&lt;00:00, 57.2MB/s]"
     }
    },
    "189ddc1a5e674235a5a4c9461ec37bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89ed549065394bad9c015c612b04c1d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f9994c1870c348b0ab983404decb722e",
      "value": "100%"
     }
    },
    "2030796b20a24135908446e8a05b2447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e3ad9c31b8244d9987cb5d09ac80909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741ba7eec1f94e6889152972dbf220c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89ed549065394bad9c015c612b04c1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8d4c88aa714714b864351862297512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_741ba7eec1f94e6889152972dbf220c0",
      "max": 241669177,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be3b477f459a4fdd80b055ef88cf8b52",
      "value": 241669177
     }
    },
    "be3b477f459a4fdd80b055ef88cf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc134416b78b4e16ab143f7b3c9b4db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1785c740b7449584ceec0225c64c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_189ddc1a5e674235a5a4c9461ec37bb7",
       "IPY_MODEL_af8d4c88aa714714b864351862297512",
       "IPY_MODEL_10ac0991cf0d4346a2dd950c4e69de74"
      ],
      "layout": "IPY_MODEL_2030796b20a24135908446e8a05b2447"
     }
    },
    "f9994c1870c348b0ab983404decb722e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
