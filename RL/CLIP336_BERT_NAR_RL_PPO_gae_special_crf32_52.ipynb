{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl7yR7P3CTTj"
   },
   "source": [
    "### ライブラリの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7IQqCE7mCeD"
   },
   "source": [
    "###モジュールのインポートとGoogleドライブのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLEMbaPJOs9v",
    "outputId": "f47ad633-eb58-49cd-8893-914d53aa4475",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-07 15:48:22.874182: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2026-02-07 15:48:22.920442: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2026-02-07 15:48:24.028663: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch.autograd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import datetime\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "import gc\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import skimage.transform\n",
    "from collections import deque\n",
    "from typing import Sequence, Dict, Tuple, Union, Optional\n",
    "#from argparse import Namespace\n",
    "from dataclasses import dataclass, field\n",
    "from sacrebleu.metrics import BLEU\n",
    "from evaluate import load\n",
    "import jiwer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "#from timm.scheduler import CosineLRScheduler\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "\n",
    "#from transformers import AutoImageProcessor, AutoModel, AutoProcessor, CLIPVisionModel\n",
    "#from transformers import AutoTokenizer, CLIPVisionModel, AutoModelForCausalLM\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "from evaluate import load\n",
    "\n",
    "import util\n",
    "import levenshtein\n",
    "from nltk import bleu_score\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "import ssl\n",
    "from torch.amp import autocast, GradScaler\n",
    "from collections import OrderedDict\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import json\n",
    "import collections\n",
    "from collections import Counter\n",
    "import plotly\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "fn = bleu_score.SmoothingFunction().method7\n",
    "import time\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "#logging.getLogger('rouge_score.rouge_scorer').setLevel(logging.WARNING)\n",
    "#logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M5jwqxtGH7JR"
   },
   "source": [
    "### 位置エンコーディングの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja_g99AUIJTF"
   },
   "source": [
    "### Transformerデコーダの実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siyThdM4Icll"
   },
   "source": [
    "### CaptioningTransformerの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "a_token_id = tokenizer.encode( [ \"a\" ] )[1]\n",
    "the_token_id = tokenizer.encode( [ \"the\" ] )[1]\n",
    "and_token_id = tokenizer.encode( [ \"and\" ] )[1]\n",
    "in_token_id = tokenizer.encode( [ \"in\" ] )[1]\n",
    "we_token_id = tokenizer.encode( [ \"we\" ] )[1]\n",
    "i_token_id = tokenizer.encode( [ \"i\" ] )[1]\n",
    "he_token_id = tokenizer.encode( [ \"he\" ] )[1]\n",
    "she_token_id = tokenizer.encode( [ \"she\" ] )[1]\n",
    "it_token_id = tokenizer.encode( [ \"it\" ] )[1]\n",
    "they_token_id = tokenizer.encode( [ \"they\" ] )[1]\n",
    "period_token_id = tokenizer.encode( [ \".\" ] )[1]\n",
    "comma_token_id = tokenizer.encode( [ \",\" ] )[1]\n",
    "dbl_token_id = tokenizer.encode( [ '\"' ] )[1]\n",
    "sgl_token_id = tokenizer.encode( [ \"'\" ] )[1]\n",
    "\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "class ComputeReward(nn.Module):\n",
    "    def __init__(self, device, reward_t = 'ordinary', decode_t = 'ordinary', sentence_level_metric=\"bleu\", \n",
    "                 repeat_thresh = [4,2,2,2], repeat_weight = [0.5, 1, 1, 2], cider_coef = 1.0, rouge_coef = 1.0, clip_coef = 2.0, \n",
    "                 bert_coef = 1.0, use_amp = True ):\n",
    "        super().__init__()\n",
    "        self.metric = sentence_level_metric\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tgt_lang = \"en\"\n",
    "        self.device = device\n",
    "\n",
    "        if sentence_level_metric =='special':\n",
    "            #self.bleu = BLEU(effective_order=\"True\")\n",
    "            self.scorer = Cider()\n",
    "            #self.meteor = load('meteor')\n",
    "            self.rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "            self.bert = load('bertscore')\n",
    "            self.metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "            for param in self.metric.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.spider = Spice()\n",
    "        elif sentence_level_metric == 'bleu':\n",
    "            self.bleu = BLEU(effective_order=\"True\")\n",
    "        elif sentence_level_metric == 'meteor':\n",
    "            self.meteor = load('meteor')\n",
    "        elif sentence_level_metric == 'rouge':\n",
    "            self.rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        elif sentence_level_metric == 'cider':\n",
    "            self.scorer = Cider() \n",
    "        elif sentence_level_metric == 'ter':\n",
    "            #self.ter = load('ter')\n",
    "            pass\n",
    "        elif sentence_level_metric == 'bert':\n",
    "            #self.bert = load('bertscore')\n",
    "            pass\n",
    "        elif sentence_level_metric == 'bleurt':\n",
    "            #self.bleurt = load('bleurt', module_type='metric', checkpoint='bleurt-large-128')\n",
    "            pass\n",
    "        elif self.metric == \"comet\":\n",
    "            model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "            self.comet = load_from_checkpoint(model_path)\n",
    "        self.reward_t = reward_t\n",
    "        self.repeat_thresh = repeat_thresh\n",
    "        self.repeat_weight = repeat_weight\n",
    "        self.decode_t = decode_t\n",
    "        self.cider_coef = cider_coef\n",
    "        self.rouge_coef = rouge_coef\n",
    "        self.clip_coef = clip_coef\n",
    "        self.bert_coef = bert_coef\n",
    "        self.use_amp = use_amp\n",
    "    \n",
    "    def _compute_reward_ord(self, preds, targets, imgs2, sources=None):\n",
    "        \"\"\"\n",
    "        Compute reward metric for a batch of prediction and target sentences\n",
    "        \"\"\"\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        # detokenize (convert to str) preds & targets\n",
    "        if self.decode_t == 'no-endoftext':\n",
    "            preds_str = [self.tokenizer.decode(\n",
    "                [pred[i] for i in range( 1,  len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                ) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(\n",
    "                [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                ) for target in targets]\n",
    "        elif self.decode_t == 'no-pad':\n",
    "            preds_str = [self.tokenizer.decode(\n",
    "                [ i for i in pred \\\n",
    "                 if i != pad_token_id \\\n",
    "                 and i != eos_token_id ] \\\n",
    "                ) for pred in preds]\n",
    "            preds_str2 = [self.tokenizer.decode(\n",
    "                [ i for i in pred \\\n",
    "                 if  i != eos_token_id  ] \\\n",
    "                 , skip_special_tokens = True ) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(\n",
    "                [ i for i in target \\\n",
    "                if i != pad_token_id \\\n",
    "                 and i != eos_token_id ]       \n",
    "                ) for target in targets]\n",
    "        else:\n",
    "            preds_str = [self.tokenizer.decode(pred) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(target) for target in targets]\n",
    "        sources_str = [self.tokenizer.decode(source, ref=\"src\") for source in sources] if sources is not None else None\n",
    "        #print( \"preds size:\", preds.size() )\n",
    "        #print( \"targets size:\", targets.size() )\n",
    "        \n",
    "        #print(f'1st target sent: {targets_str[0]}')\n",
    "        #print(f'1st pred sent: {preds_str[0]}')\n",
    "\n",
    "        # compute reward metric\n",
    "        seq_len = preds.shape[1]\n",
    "\n",
    "        if self.metric == 'special':\n",
    "            #reward_bleu = [[self.bleu.sentence_score(pred, [target]).score] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward_bleu = [[bleu_score.sentence_bleu( target, pred, smoothing_function=fn)] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward_bleu = torch.tensor(reward_bleu).to(self.device) / 100.0\n",
    "            #wer_scores = [jiwer.wer(target, pred) for pred, target in zip(targets_str, preds_str)]\n",
    "            #reward = [[score] * seq_len for score in wer_scores]\n",
    "            #reward_wer = - torch.tensor( reward ).to(self.device)\n",
    "            #start_time = time.time()\n",
    "            pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "            target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "            score, scores = self.scorer.compute_score(target_dict, pred_dict)\n",
    "            reward_cider = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            #end_time = time.time()\n",
    "            #print( \"cider time:\", end_time - start_time )\n",
    "            #start_time = time.time()\n",
    "            reward_rouge = [[self.rougeL.score(target, pred)['rougeL'][0]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            reward_rouge = torch.tensor( reward_rouge ).to( self.device )\n",
    "            #end_time = time.time()\n",
    "            #print( \"rouge time:\", end_time - start_time )\n",
    "            #start_time = time.time()\n",
    "            with autocast(str(self.device),enabled=self.use_amp):\n",
    "                with torch.no_grad():\n",
    "                    #clip_scores = [[self.metric( img2, pred).detach()] * seq_len for img2, pred in zip( imgs2, preds_str2 )]\n",
    "                    #tmp = self.metric( imgs2, preds_str2 )\n",
    "                    #print( \"tmp size:\", tmp.size() )\n",
    "                    #clip_scores = [[self.metric( imgs2, preds_str2 ).detach()] * seq_len ]\n",
    "                    processed = self.metric.processor(text=preds_str2, images=imgs2, return_tensors=\"pt\", padding=True, \\\n",
    "                                                      truncation=True, max_length=77 ).to(self.device)\n",
    "                    outputs = self.metric.model(**processed)\n",
    "                    # 特徴量の正規化\n",
    "                    image_features = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                    text_features = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                    # 3. コサイン類似度を一括計算 (100倍してマイナスをカットするのが CLIPScore の定義)\n",
    "                    # 各画像ペアの個別スコア (Batch Size,) が得られる\n",
    "                    individual_scores = torch.clamp( (image_features * text_features).sum(axis=-1), min=0)\n",
    "                    #print( \"individual_scores size:\", individual_scores.size() )\n",
    "                    clip_scores = individual_scores[:,None].expand( -1, seq_len ).to( self.device ) / 100.0\n",
    "                    #print( \"clip_scores size:\", clip_scores.size() )\n",
    "                    reward_clip = clip_scores\n",
    "                    #end_time = time.time()\n",
    "                    #print( \"clip time:\", end_time - start_time )\n",
    "                    #start_time = time.time()\n",
    "                    bert_scores = self.bert.compute(predictions=preds_str, references=targets_str, use_fast_tokenizer=True, \\\n",
    "                                            model_type=model_name, lang='en',  device=self.device)['f1']\n",
    "                    reward_bert = torch.tensor( bert_scores )[:,None].expand( -1, seq_len ).to( self.device )\n",
    "                    #end_time = time.time()\n",
    "                    #print( \"bert time:\", end_time - start_time )\n",
    "            #image = torch.randint(255, (3, 224, 224), generator=torch.Generator().manual_seed(42))\n",
    "            #score = metric(image, \"a photo of a cat\")\n",
    "            #average_score, scores = self.spice.compute_score(target_dict, pred_dict)\n",
    "            #print( \"average_score:\", average_score ) \n",
    "            #reward_spice = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            #meteor_scores = [self.meteor.compute(predictions=[preds], references=[targets])['meteor'] for preds, targets in zip(preds_str, targets_str)]\n",
    "            #reward_meteor = [[score] * seq_len for score in meteor_scores]\n",
    "            #reward_meteor = torch.tensor( reward_meteor ).to( self.device )\n",
    "            #reward = reward_bleu + reward_cider\n",
    "            #print( \"reward_bleu:\", reward_bleu )\n",
    "            #print( \"reward_cider:\", reward_cider )\n",
    "            #print( \"reward_meteor:\", reward_meteor )\n",
    "            #reward = reward_meteor + reward_cider\n",
    "            #print( \"self.bert_coef:\", self.bert_coef )\n",
    "            #print( \"reward_bert:\", reward_bert )\n",
    "            #reward = self.clip_coef * reward_clip + self.bert_coef * reward_bert\n",
    "            reward = self.cider_coef * reward_cider + self.rouge_coef * reward_rouge \\\n",
    "                + self.clip_coef * reward_clip + self.bert_coef * reward_bert\n",
    "            reward2 = reward_cider + reward_rouge + reward_bert + reward_clip\n",
    "            #score, scores = self.spider.compute_score(gts, res)\n",
    "            #reward_rouge = [[self.spider.compute_score(target, pred)[1]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward2 = reward_rouge + reward_bert + reward_clip\n",
    "            #reward = self.rouge_coef * reward_rouge + self.cider_coef * reward_cider + self.clip_coef * reward_clip \\\n",
    "            #    + self.bert_coef * reward_bert\n",
    "            #reward = reward_bleu + reward_cider\n",
    "            #reward = reward_bleu + reward_cider + reward_meteor\n",
    "            #reward = reward_wer + reward_cider\n",
    "            #reward = reward_bleu + reward_wer + reward_cider\n",
    "            #reward = reward_bleu + reward_wer + reward_cider + reward_meteor\n",
    "            #print( \"wer:\", reward_wer )\n",
    "            #print( \"cider:\", reward_cider )\n",
    "        elif self.metric == \"bleu\":\n",
    "            reward = [[self.bleu.sentence_score(pred, [target]).score] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "        \n",
    "        elif self.metric == \"meteor\":\n",
    "            meteor_scores = [self.meteor.compute(predictions=[preds], references=[targets])['meteor'] for preds, targets in zip(preds_str, targets_str)]\n",
    "            reward = [[score] * seq_len for score in meteor_scores]\n",
    "            \n",
    "        elif self.metric == \"rouge\":\n",
    "            #rouge_scores = self.rouge.compute(predictions=preds_str, references=targets_str, use_aggregator=False )['rougeL']\n",
    "            reward = [[self.rouge.score(target, pred)['rougeL'][0]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "        \n",
    "        elif self.metric == \"wer\":\n",
    "            wer_scores = [jiwer.wer(target, pred) for pred, target in zip(targets_str, preds_str)]\n",
    "            reward = [[score] * seq_len for score in wer_scores]\n",
    "            reward = - torch.tensor( reward )\n",
    "\n",
    "        elif self.metric == 'cider':\n",
    "            pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "            target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "            #score, all_scores = parallel_cider_evaluation( target_dict, pred_dict )\n",
    "            score, scores = self.scorer.compute_score(target_dict, pred_dict)\n",
    "            reward = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            \n",
    "        #elif self.metric == \"bert\":\n",
    "        #    bert_scores = self.bert.compute(predictions=preds_str, references=targets_str, lang='en')['f1']\n",
    "        #    reward = [[score] * seq_len for score in bert_scores]\n",
    "\n",
    "        #elif self.metric == \"bleurt\":\n",
    "        #    bleurt_scores = self.bleurt.compute(predictions=preds_str, references=targets_str)['scores']\n",
    "        #    reward = [[score] * seq_len for score in bleurt_scores]\n",
    "            \n",
    "        elif self.metric == \"comet\":\n",
    "            data = [{\"src\": source, \"mt\": pred, \"ref\": target} for source, pred, target in zip(sources_str, preds_str, targets_str)]\n",
    "            reward = self.comet.predict(data, batch_size=8, gpus=1)['scores']\n",
    "            reward = [[score] * seq_len for score in reward]\n",
    "        else:\n",
    "            raise ValueError(f\"metric {self.metric} not supported\")\n",
    "        if self.metric != 'cider' and self.metric != 'special':\n",
    "            reward = torch.tensor(reward).to(self.device)\n",
    "        \n",
    "        #print( \"reward size:\", reward.size() )\n",
    "        #return reward\n",
    "        return reward, reward2\n",
    "\n",
    "    def my_index(self, list1, target ):\n",
    "        if target in list1:\n",
    "            return list1.index( target )\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def compute_length_reward( self, preds, targets ):\n",
    "\n",
    "        #def differentiable_argamx( logits, tau ):\n",
    "\n",
    "        #    tmp = F.gumbel_softmax( logits, tau, hard=True )\n",
    "        #    tmp1 = torch.arange( 0, logits.size(2) )[None,None] * tmp\n",
    "        #    tokens = torch.sum( tmp1, dim = 2 )\n",
    "\n",
    "        #    return tokens\n",
    "        '''\n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        #print( first_index )\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / preds.size(1)\n",
    "        #print( pred_lengths ) \n",
    "        \n",
    "        reward = pred_lengths[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "        return reward         \n",
    "        '''\n",
    "        \n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / preds.size(1)\n",
    "        \n",
    "        target_index = targets == eos_token_id\n",
    "        first_index = ( target_index.int().cumsum(dim = 1 ) == 1 ) & target_index\n",
    "        target_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / targets.size(1)\n",
    "        #target_lengths = 1.1 * torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / targets.size(1)\n",
    "        #target_lengths = torch.clamp( target_lengths, max = 1.0 )\n",
    "        #target_lengths = torch.full((preds.size(0),), self.target_length / preds.size(1), device = self.device )\n",
    "        \n",
    "        reward_lengths = - nn.MSELoss(reduction='none')( pred_lengths, target_lengths )\n",
    "        #reward_lengths = - torch.abs( pred_lengths - target_lengths )\n",
    "        \n",
    "        reward = reward_lengths[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "        return reward  \n",
    "        \n",
    "        #reward = torch.tensor( [ self.my_index( pred.tolist(), eos_token_id ) for pred in preds ] ).to(self.device )\n",
    "        #reward = reward[:,None].expand( -1, preds.size(1) ).float()\n",
    "        \n",
    "        ##tokens = differentiable_argamx( logits, tau ) #logits から token を算出。微分可能 B * T\n",
    "        \n",
    "        '''\n",
    "        tmp1 = torch.abs( preds - tokenizer.pad_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        tmp2 = torch.abs( preds - eos_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        tmp = tmp1 * tmp2\n",
    "        pad_preds = F.sigmoid( 10 - 100 * tmp ) # pad のところだけ 1, あとは 0。 B * T  \n",
    "        reward = ( torch.tensor( preds.size(1))[None] - torch.sum( pad_preds, dim = 1 ))[:,None].expand( -1, preds.size(1) ) \n",
    "        '''\n",
    "        # 固定長97 から　pad と eos の長さを引いて、文章の長さ。文章の長さが大きいほどよい。\n",
    "        \n",
    "        #tmp1 = torch.abs( targets - tokenizer.pad_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        #tmp2 = torch.abs( targets - eos_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        #tmp = tmp1 * tmp2\n",
    "        #pad_targets = F.sigmoid( 10 - 100 * tmp ) # pad のところだけ 1, あとは 0。 B * T\n",
    "        \n",
    "        #reward = - nn.MSELoss( reduction = 'none' )( pad_preds, pad_targets )\n",
    "        \n",
    "        #return  reward\n",
    "    \n",
    "    def calc_ngram_repeat( self, preds ):\n",
    "\n",
    "        bsz, seq_len = preds.size()\n",
    "        \n",
    "        ngram_cnt = torch.zeros( (bsz), device = preds.device, dtype = torch.float )\n",
    "        for n in range( 2, 4 ):\n",
    "            for i, pred in enumerate( preds ):\n",
    "                pred = pred.tolist()\n",
    "                ngrams = zip(*[pred[i:] for i in range(n)])\n",
    "                counts = Counter(ngrams)\n",
    "                count_sum = 0\n",
    "                for count in counts.values():\n",
    "                    if count >= self.repeat_thresh:\n",
    "                        count_sum = count_sum + count\n",
    "                ngram_cnt[i] = ngram_cnt[i] + count_sum\n",
    "\n",
    "        return - ngram_cnt[:,None].expand( -1, seq_len ) / seq_len\n",
    "\n",
    "    def unique_ngram_ratio(self, preds):\n",
    "        \n",
    "        bsz, seq_len = preds.size()\n",
    "        ng = 5\n",
    "        unr = torch.zeros( (bsz, ng), device=preds.device, dtype=torch.float)\n",
    "\n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index * arange_index, dim = 1 )\n",
    "        \n",
    "        for b in range( bsz ):\n",
    "            for n in range( 0, ng - 1 ): # n は ngram - 1\n",
    "                if pred_lengths[b] > 0:\n",
    "                    #print( \"pred_lengths[b]:\", pred_lengths[b] )\n",
    "                    pred_tmp = preds[b,:pred_lengths[b]]\n",
    "                    #print( \"pred_tmp:\", pred_tmp )\n",
    "                else:\n",
    "                    pred_tmp = preds[b]\n",
    "                ngram_tensor = pred_tmp.unfold(0, n + 1, 1)\n",
    "                ngram_count = len( ngram_tensor )\n",
    "                unique_count = len( torch.unique( ngram_tensor ) )\n",
    "                #unique_count = len( set( map(tuple, ngram_tensor.tolist())))\n",
    "                unr[b,n] = unique_count / ngram_count\n",
    "\n",
    "        return torch.mean( unr, dim = 1)[:, None].expand(-1, seq_len)\n",
    "    \n",
    "    def calc_ngram_repeat_fast(self, preds):\n",
    "        bsz, seq_len = preds.size()\n",
    "        ngram_cnt = torch.zeros(bsz, device=preds.device, dtype=torch.float)\n",
    "        \n",
    "        for n in range(1, 5):\n",
    "            # 無視するトークンのリスト\n",
    "            #ignore_ids = [self.pad_token_id, self.eos_token_id, self.cls_token_id, self.sep_token_id]\n",
    "            if n == 1:\n",
    "                ignore_ids = [pad_token_id, eos_token_id, cls_token_id, sep_token_id, a_token_id, the_token_id, \\\n",
    "                              period_token_id, comma_token_id, and_token_id, in_token_id ]\n",
    "            else:\n",
    "                ignore_ids = [pad_token_id, eos_token_id, cls_token_id, sep_token_id]\n",
    "            \n",
    "            # 1. 無視すべきトークンの位置を特定 (bsz, seq_len)\n",
    "            # ignore_mask[b, i] が True なら、そのトークンは無視対象\n",
    "            ignore_mask = torch.zeros_like(preds, dtype=torch.bool)\n",
    "            for idx in ignore_ids:\n",
    "                ignore_mask |= (preds == idx)\n",
    "            \n",
    "            if seq_len < n:\n",
    "                continue\n",
    "            \n",
    "            # n-gram を抽出 (bsz, num_ngrams, n)\n",
    "            ngrams = preds.unfold(dimension=1, size=n, step=1)\n",
    "            \n",
    "            # 2. 各 n-gram に無視対象トークンが含まれているか判定\n",
    "            # n-gram内のいずれかが ignore_mask で True なら True\n",
    "            # (bsz, num_ngrams)\n",
    "            ngram_ignore_mask = ignore_mask.unfold(dimension=1, size=n, step=1).any(dim=-1)\n",
    "        \n",
    "            for b in range(bsz):\n",
    "                # このバッチの有効な n-gram だけを抽出\n",
    "                valid_ngrams = ngrams[b][~ngram_ignore_mask[b]]\n",
    "                \n",
    "                if valid_ngrams.size(0) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # ユニークな n-gram とそのカウントを取得\n",
    "                unique_ngrams, counts_per_ngram = torch.unique(valid_ngrams, dim=0, return_counts=True)\n",
    "                \n",
    "                # 閾値以上のカウントを合計\n",
    "                mask = counts_per_ngram >= self.repeat_thresh[n-1]\n",
    "                ngram_cnt[b] += counts_per_ngram[mask].sum().float() * self.repeat_weight[n-1]\n",
    "\n",
    "        penalty = - torch.clamp( torch.pow( 2, ngram_cnt -1 ) / seq_len, max = 1.0 )    \n",
    "        \n",
    "        return penalty[:, None].expand(-1, seq_len)\n",
    "    \n",
    "    def calc_cnt_repeat( self, preds ):\n",
    "\n",
    "        B, T = preds.size()\n",
    "\n",
    "        repeat_count = torch.zeros( ( bsz, vocab_size ), device = preds.device )\n",
    "        for i, pred in enumerate( preds ):\n",
    "            repeat_count0 = torch.bincount( pred )\n",
    "            repeat_count[i,:len(repeat_count0)] = repeat_count0 \n",
    "\n",
    "        repeat_count[:,pad_token_id] = 0\n",
    "        repeat_count[:,eos_token_id] = 0\n",
    "        repeat_count[:,cls_token_id] = 0\n",
    "        repeat_count[:,sep_token_id] = 0\n",
    "        thresh_masks = repeat_count >= self.repeat_thresh\n",
    "        repeat_count = repeat_count * thresh_masks\n",
    "\n",
    "        repeat = torch.sum( repeat_count, dim = 1 )\n",
    "        \n",
    "        return - repeat[:,None].expand( -1, T ).float() / seq_len\n",
    "        '''\n",
    "        B, T = preds.size()\n",
    "\n",
    "        repeat_count = torch.zeros( ( bsz, vocab_size ), device = preds.device )\n",
    "        for i, pred in enumerate( preds ):\n",
    "            repeat_count0 = torch.bincount( pred )\n",
    "            repeat_count[i,:len(repeat_count0)] = repeat_count0 \n",
    "\n",
    "        repeat_times = torch.sum( repeat_count >= self.repeat_thresh, dim = 1 )\n",
    "        #print( \"repeat_times:\",repeat_times )\n",
    "\n",
    "        pad_count = repeat_count[:,pad_token_id]\n",
    "        pad_times = (pad_count >= self.repeat_thresh).long()\n",
    "        #print( \"pad_times:\", pad_times )\n",
    "\n",
    "        eos_count = repeat_count[:,eos_token_id]\n",
    "        eos_times = (eos_count >= self.repeat_thresh).long()\n",
    "        #print( \"eos_times:\", eos_times )\n",
    "\n",
    "        repeat = repeat_times - pad_times - eos_times\n",
    "        #print( repeat )\n",
    "\n",
    "        #print( \"repeat size:\", repeat.size() )\n",
    "        \n",
    "        return - repeat[:,None].expand( -1, T ).float()\n",
    "        '''\n",
    "        '''\n",
    "        tokens = preds\n",
    "        #tokens[0,0] = 1000\n",
    "        #tokens[0,1] = 1000\n",
    "        #print( \"tokens size:\", tokens.size() )\n",
    "        #print( \"tokens[:,0]:\", tokens[:,0] )\n",
    "    \n",
    "        cnt = torch.zeros( (B, T),  device=preds.device, dtype=torch.float16 )\n",
    "        for i in range( T ):\n",
    "            cntj = torch.zeros( (B),  device=preds.device, dtype=torch.float16 )\n",
    "            num_j = 0\n",
    "            for j in range( max( 0, i - self.c ), min(  T, i + self.c ) ):\n",
    "                if j != i:\n",
    "                    ##print( \"torch.eq\", torch.eq( tokens[:,i], tokens[:,j]))\n",
    "                    #tmp1 = torch.eq( tokens[:,i], tokens[:,j]) \n",
    "                    #tmp2 = torch.ne( tokens[:,i], eos_token_id )\n",
    "                    #tmp3 = torch.ne( tokens[:,i], tokenizer.pad_token_id )\n",
    "                    ##print( \"tmp1:\", tmp1 )\n",
    "                    ##print( \"tmp2:\", tmp2 )\n",
    "                    ##print( \"tmp3:\", tmp3 )\n",
    "                    #tmp = (torch.logical_and(torch.logical_and( tmp1 , tmp2 ),tmp3 )).to(torch.float16)\n",
    "                    if self.decode_t == 'no-endoftext':\n",
    "                        tmp = ((tokens[:,i] != tokens[:,j] ).to(torch.float) ) * 10 \\\n",
    "                            +  ((tokens[:,i] == endoftext_token_id).to(torch.float) )  * 10\n",
    "                    elif self.decode_t == 'no-pad':\n",
    "                        tmp = ((tokens[:,i] != tokens[:,j] ).to(torch.float) ) * 10 \\\n",
    "                            + ((tokens[:,i] == pad_token_id).to(torch.float) )  * 10 \\\n",
    "                            + ((tokens[:,i] == eos_token_id).to(torch.float) ) * 10\n",
    "                            # tokens[:,i] と tokens[:,j] が同じで、tokens[:,i] が　eos、pad でなければ 0 その他は 10 以上の整数 \n",
    "                    tmp = F.sigmoid( 10 - 100 * tmp ) # tokens[:,i]とtokens[:,j] が同じで eos pad ではないところだけ 1, あとは 0.\n",
    "                    #if torch.any( tmp != 0 ):\n",
    "                    #    print( \"tmp:\", tmp )\n",
    "                    cntj = cntj + tmp # repeat の数。[B]  jについて足しこんでいる。\n",
    "                    #if torch.any( cntj != 0 ):\n",
    "                    #    print( \"cntj:\", cntj )\n",
    "                    num_j = num_j + 1\n",
    "            cnt[:,i] = cntj / num_j # repeat の数。[B] i について足しこんでいる。\n",
    "        \n",
    "        #mask = cnt != 0\n",
    "        #print( \"cnt mask:\", cnt[mask] )\n",
    "        #return - torch.mean( cnt, dim = 1 ) # \n",
    "        return - torch.mean( cnt, dim = 1 )[:,None].expand(-1, T )  # B * T repeat の数が少ないほど良い。repeat の数の -1 倍が多いほどよい。\n",
    "        '''\n",
    "    def forward(self, top_probs, sampled_beam_idx, top_indices, targets, imgs2,  sources=None, masks=None):\n",
    "        \"\"\"\n",
    "        outputs: batch x len x d_model\n",
    "        targets: batch x len\n",
    "        sources: batch x len\n",
    "        masks:   batch x len\n",
    "        \"\"\"\n",
    "        self.device = sampled_beam_idx.device\n",
    "        # input to device\n",
    "        targets = targets.to(self.device)\n",
    "        bsz, seq_len, beam = top_probs.size()\n",
    "        eps = 1e-8\n",
    "\n",
    "        preds = torch.gather( top_indices, -1, sampled_beam_idx ).squeeze( -1 )\n",
    "        tmp = torch.clamp( top_probs, eps )\n",
    "        top_log_probs = torch.log( tmp )\n",
    "        sample_log_probs = torch.gather( top_log_probs, -1, sampled_beam_idx ).squeeze( -1 )\n",
    "\n",
    "        if self.reward_t == 'ordinary':\n",
    "            reward_ord = self.compute_reward(preds, targets, imgs2, sources)   #  bsz\n",
    "            reward_repeat = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_length = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == \"ord+rep\":\n",
    "            reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            #reward = reward_ord + reward_repeat - b # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == \"ord+rep+len\":\n",
    "            reward_ord, reward_ord2 = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == 'ord+len':\n",
    "            reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == 'ord+rep+len+unr':\n",
    "            reward_ord, reward_ord2 = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = self.unique_ngram_ratio(preds)\n",
    "        \n",
    "        ## apply mask\n",
    "        #if masks is not None:\n",
    "        #    masks = masks.to(self.device)\n",
    "        #    probs, targets = probs[masks], targets[masks]\n",
    "        #    # outputs, targets = outputs[masks], targets[masks]\n",
    "        #    reward, preds = reward[masks], preds[masks]\n",
    "       \n",
    "        return reward_ord, reward_ord2, reward_repeat, reward_length, reward_unr, preds, sample_log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, dim=1):\n",
    "    return torch.logsumexp(x.float(), dim=dim).type_as(x)\n",
    "\n",
    "class DynamicCRF(nn.Module):\n",
    "    def __init__(self, num_embedding, low_rank=32, beam_size=64, crf_coef=1.0, temp = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        #low_rank = num_embedding\n",
    "        self.E1 = nn.Embedding(num_embedding, low_rank)\n",
    "        self.E2 = nn.Embedding(num_embedding, low_rank)\n",
    "\n",
    "        self.vocb = num_embedding\n",
    "        self.rank = low_rank\n",
    "        self.beam = beam_size\n",
    "        self.crf_coef = crf_coef\n",
    "        self.temp = temp\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"vocab_size={}, low_rank={}, beam_size={}\".format(\n",
    "            self.vocb, self.rank, self.beam)\n",
    "\n",
    "    def forward(self, emissions, top_logits, top_indices, targets, masks, beam=None):\n",
    "        numerator = self._compute_score(emissions, targets, masks)\n",
    "        denominator = self._compute_normalizer(emissions, targets, masks, beam )\n",
    "        beam_probs = self._compute_normalizer2(top_logits, top_indices, targets, masks, beam)\n",
    "\n",
    "        return numerator - denominator, beam_probs\n",
    "    \n",
    "    def forward_decoder(self, emissions, masks=None, beam=None):\n",
    "        return self._viterbi_decode(emissions, masks, beam)\n",
    "\n",
    "    def _compute_score(self, emissions, targets, masks=None):\n",
    "        batch_size, seq_len = targets.size()\n",
    "\n",
    "        emission_scores = emissions.gather(2, targets[:, :, None])[:, :, 0]  # B x T\n",
    "        transition_scores = (self.E1(targets[:, :-1]) * self.E2(targets[:, 1:])).sum(2)\n",
    "       \n",
    "        scores = emission_scores\n",
    "        scores[:, 1:] += transition_scores\n",
    "        \n",
    "        if masks is not None:\n",
    "            scores = scores * masks.type_as(scores)\n",
    "\n",
    "        return scores.sum(-1)\n",
    "        \n",
    "    def _compute_normalizer(self, emissions, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        if targets is not None:\n",
    "            #_emissions = emissions.scatter(2, targets[:, :, None], np.float('inf'))\n",
    "            _emissions = emissions.scatter(2, targets[:, :, None], float('inf'))\n",
    "            beam_targets = _emissions.topk(beam, 2)[1]\n",
    "            beam_emission_scores = emissions.gather(2, beam_targets)\n",
    "        else:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step.\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        for i in range(1, seq_len):\n",
    "            next_score = score[:, :, None] + beam_transition_matrix[:, i-1]\n",
    "            next_score = logsumexp(next_score, dim=1) + beam_emission_scores[:, i]\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i:i+1], next_score, score)\n",
    "            else:\n",
    "                score = next_score\n",
    "\n",
    "        return logsumexp(score, dim=1)\n",
    "\n",
    "    def _compute_top_probs(self, beam_emission_scores, beam_transition_matrix, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "\n",
    "            # greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "\n",
    "            # multinomial selection\n",
    "            B, C, W = _score.shape\n",
    "            flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            _index_flat = torch.multinomial(probs, num_samples=1)\n",
    "            _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            _index = _index_flat.view(B, W)\n",
    "            _score = _score_flat.view(B, W)\n",
    "\n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "        \n",
    "        all_scores = traj_scores\n",
    "        all_scores.append( score )\n",
    "        all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 ).to(device)\n",
    "        #beam_probs = F.softmax( all_scores / self.temp, dim = 2 )\n",
    "        beam_probs = F.softmax( all_scores, dim = 2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        #finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return beam_probs, finalized_tokens.unsqueeze(-1)\n",
    "\n",
    "    def _compute_top_probs2(self, beam_emission_scores, beam_transition_matrix, beam_targets, n_best = 10, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        window = 5\n",
    "\n",
    "        exclude_token_id2 = torch.tensor( [pad_token_id, eos_token_id, a_token_id, the_token_id, and_token_id, in_token_id, \\\n",
    "            we_token_id, i_token_id, he_token_id, she_token_id, it_token_id, they_token_id, \\\n",
    "            period_token_id, comma_token_id, dbl_token_id, sgl_token_id], device=device )\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "    \n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        _score = beam_emission_scores[:, 0][:,None,:].expand(-1,beam,-1) # t = -1 → 0 への beam_transition_matrix bsz*beam C*beam W は　0 \n",
    "        #_score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "        B, C, W = _score.shape\n",
    "        flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "        probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "        _index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )\n",
    "        _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "        _index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "        _score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "    \n",
    "        traj_scores2 = []\n",
    "        traj_tokens2 = []\n",
    "    \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score) # bsz * beam\n",
    "            traj_scores2.append( _score2 )\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            \n",
    "            ## greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            #_score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "            \n",
    "            ## multinomial selection\n",
    "            B, C, W = _score.shape\n",
    "            flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            #print( \"flat_score size:\", flat_score.size() )\n",
    "            #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            _index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )  # C が n_best になる。\n",
    "            _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            _index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "            _score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "            \n",
    "            #_score = _score + beam_emission_scores[:, i] # bsz, beam        \n",
    "    \n",
    "            _score2 = _score2 + beam_emission_scores[:, i][:,None,:].expand(-1,beam,-1).gather( 1, _index2 ) # bsz, n_best, beam       \n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score2[:,0,:], _index2[:,0,:]\n",
    "            traj_tokens.append(index)\n",
    "            traj_tokens2.append( _index2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        used_index = torch.zeros( bsz, seq_len, dtype=torch.long, device=device)\n",
    "        used_index[:,seq_len - 1] = best_index\n",
    "        beam_probs = torch.zeros( bsz, seq_len, beam, dtype=torch.float, device=device )\n",
    "        beam_probs[:,seq_len - 1,:] = score \n",
    "        for t_reverse, (idx2, scs2) in enumerate( zip( reversed(traj_tokens2), reversed(traj_scores2))):\n",
    "            t = seq_len - t_reverse - 2\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            #print( \"previous_index size:\", previous_index.size() )\n",
    "            selected_idx = idx2[:,0,:].gather( 1, previous_index )\n",
    "            selected_scs = scs2[:,0,:].gather( 1, previous_index )\n",
    "            beam_probs[ :, t ,: ] = scs2[:,0,: ]\n",
    "            for n in range( 1, n_best ):\n",
    "                for b in range( bsz ):\n",
    "                    t_max = torch.min( t + window, seq_len - 1 )\n",
    "                    used_index_tmp = used_index[b,t+1:t_max]\n",
    "                    corrected_selected_idx = torch.gather( beam_targets[b,t,:], 0, selected_idx[b] )\n",
    "                    #if corrected_selected_idx in used_index[b] and corrected_selected_idx not in exclude_token_id2:\n",
    "                    if corrected_selected_idx in used_index_tmp and corrected_selected_idx not in exclude_token_id2:\n",
    "                        selected_idx[b] = idx2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        selected_scs[b] = scs2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        beam_probs[ b, t ,: ] = scs2[b, n,: ]\n",
    "                    else:\n",
    "                        break\n",
    "            #print( \"beam_targets[:,t,:].size():\",beam_targets[:,t,:].size())\n",
    "            #print( \"selected_idx.size():\", selected_idx.size() )\n",
    "            used_index[:,t] =  torch.gather( beam_targets[:,t,:], -1, selected_idx )[:,0] # bsz * 1 → bsz\n",
    "            #print( \"used_index[:,:]:\", used_index[:,:] )\n",
    "            #used_index[:,t] = selected_idx[:,0]\n",
    "            finalized_tokens.append( selected_idx )\n",
    "            finalized_scores.append( selected_scs )\n",
    "       \n",
    "        finalized_tokens.reverse()\n",
    "        sampled_beam_idx = torch.cat(finalized_tokens, 1)\n",
    "        #finalized_tokens = beam_targets.gather(2, sampled_beam_idx[:, :, None])[:, :, 0]\n",
    "        #print( \"finalized_tokens:\", finalized_tokens )\n",
    "        \n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        beam_probs = F.softmax( beam_probs, dim = 2 )\n",
    "        \n",
    "        #return finalized_scores, finalized_tokens\n",
    "        return beam_probs, sampled_beam_idx.unsqueeze(-1)\n",
    "\n",
    "    def _compute_viterbi_no_repeat(self, beam_emission_scores, beam_transition_matrix, beam_targets, n_best = 10, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        exclude_token_id2 = torch.tensor( [pad_token_id, eos_token_id, a_token_id, the_token_id, and_token_id, in_token_id, \\\n",
    "            we_token_id, i_token_id, he_token_id, she_token_id, it_token_id, they_token_id, \\\n",
    "            period_token_id, comma_token_id, dbl_token_id, sgl_token_id], device=device )\n",
    "        window = 5\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "    \n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        _score = beam_emission_scores[:, 0][:,None,:].expand(-1,beam,-1) # t = -1 → 0 への beam_transition_matrix bsz*beam C*beam W は　0 \n",
    "        _score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "        #B, C, W = _score.shape\n",
    "        #flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "        #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "        #_index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )\n",
    "        #_score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "        #_index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "        #_score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "    \n",
    "        traj_scores2 = []\n",
    "        traj_tokens2 = []\n",
    "    \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score) # bsz * beam\n",
    "            traj_scores2.append( _score2 )\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            \n",
    "            ## greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "            \n",
    "            ### multinomial selection\n",
    "            #B, C, W = _score.shape\n",
    "            #flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            ##print( \"flat_score size:\", flat_score.size() )\n",
    "            ##probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            #_index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )  # C が n_best になる。\n",
    "            #_score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            #_index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "            #_score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "            \n",
    "            #_score = _score + beam_emission_scores[:, i] # bsz, beam        \n",
    "    \n",
    "            _score2 = _score2 + beam_emission_scores[:, i][:,None,:].expand(-1,beam,-1).gather( 1, _index2 ) # bsz, n_best, beam       \n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score2[:,0,:], _index2[:,0,:]\n",
    "            traj_tokens.append(index)\n",
    "            traj_tokens2.append( _index2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        used_index = torch.zeros( bsz, seq_len, dtype=torch.long, device=device )\n",
    "        used_index[:,seq_len - 1] = best_index\n",
    "        beam_probs = torch.zeros( bsz, seq_len, beam, dtype=torch.float, device=device )\n",
    "        beam_probs[:,seq_len - 1,:] = score \n",
    "        for t_reverse, (idx2, scs2) in enumerate( zip( reversed(traj_tokens2), reversed(traj_scores2))):\n",
    "            t = seq_len - t_reverse - 2\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            #print( \"previous_index size:\", previous_index.size() )\n",
    "            selected_idx = idx2[:,0,:].gather( 1, previous_index )\n",
    "            selected_scs = scs2[:,0,:].gather( 1, previous_index )\n",
    "            beam_probs[ :, t ,: ] = scs2[:,0,: ]\n",
    "            for n in range( 1, n_best ):\n",
    "                for b in range( bsz ):\n",
    "                    t_max = torch.min( t + window, seq_len - 1 )\n",
    "                    used_index_tmp = used_index[b,t+1:t_max]\n",
    "                    corrected_selected_idx = torch.gather( beam_targets[b,t,:], 0, selected_idx[b] )\n",
    "                    #if corrected_selected_idx in used_index[b] and corrected_selected_idx not in exclude_token_id2:\n",
    "                    if corrected_selected_idx in used_index_tmp and corrected_selected_idx not in exclude_token_id2:\n",
    "                        selected_idx[b] = idx2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        selected_scs[b] = scs2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        beam_probs[ b, t ,: ] = scs2[b, n,: ]\n",
    "                    else:\n",
    "                        break\n",
    "            #print( \"beam_targets[:,t,:].size():\",beam_targets[:,t,:].size())\n",
    "            #print( \"selected_idx.size():\", selected_idx.size() )\n",
    "            used_index[:,t] =  torch.gather( beam_targets[:,t,:], -1, selected_idx )[:,0] # bsz * 1 → bsz\n",
    "            #print( \"used_index[:,:]:\", used_index[:,:] )\n",
    "            #used_index[:,t] = selected_idx[:,0]\n",
    "            finalized_tokens.append( selected_idx )\n",
    "            finalized_scores.append( selected_scs )\n",
    "       \n",
    "        finalized_tokens.reverse()\n",
    "        sampled_beam_idx = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, sampled_beam_idx[:, :, None])[:, :, 0]\n",
    "        #print( \"finalized_tokens:\", finalized_tokens )\n",
    "        \n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        #beam_probs = F.softmax( beam_probs, dim = 2 )\n",
    "        \n",
    "        return finalized_scores, finalized_tokens\n",
    "    \n",
    "    \n",
    "    def _compute_f_algorithm(self, emissions, temp = 1.0, targets=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "   \n",
    "        # 結果格納用\n",
    "        scores = torch.zeros((batch_size, seq_len, beam), device=emissions.device)\n",
    "        all_preds = []\n",
    "        sampled_beam_idx = []\n",
    "        \n",
    "        # --- ステップ 0 ---\n",
    "        scores[:, 0, :] = beam_emission_scores[:, 0]\n",
    "        probs_0 = F.softmax(scores[:, 0, :] / temp, dim=-1)\n",
    "        sampled_beam_idx_0 = torch.multinomial(probs_0, 1)\n",
    "\n",
    "        # 256個中の位置から、実際のVocab IDに変換\n",
    "        all_preds.append(  torch.gather(beam_targets[:, 0], 1, sampled_beam_idx_0).squeeze(1) )\n",
    "        sampled_beam_idx.append( sampled_beam_idx_0 )\n",
    "        \n",
    "        # --- ステップ 1 以降 ---\n",
    "        for i in range(1, seq_len):\n",
    "            # 1. 直前に自分が選んだ単語(Vocab ID)を取得\n",
    "            prev_vocab_idx = all_preds[-1]\n",
    "            \n",
    "            # 2. 遷移スコアの計算\n",
    "            prev_trans_feat = self.E1(prev_vocab_idx) # B x D\n",
    "            curr_trans_feat = self.E2(beam_targets[:, i]) # B x beam x D\n",
    "        \n",
    "            # B x 1 x D  @  B x D x beam  -> B x 1 x beam\n",
    "            #D = prev_trans_feat.size(-1) # 次元のサイズ\n",
    "            #current_trans_scores = torch.bmm(\n",
    "            #    prev_trans_feat.unsqueeze(1), \n",
    "            #    curr_trans_feat.transpose(1, 2)\n",
    "            #).squeeze(1) / (D ** 0.5) \n",
    "            current_trans_scores = torch.bmm(\n",
    "                prev_trans_feat.unsqueeze(1), \n",
    "                curr_trans_feat.transpose(1, 2)\n",
    "            ).squeeze(1)\n",
    "\n",
    "            # 3. 現在のスコアを確定\n",
    "            scores[:, i, :] = beam_emission_scores[:, i] + current_trans_scores\n",
    "        \n",
    "            # 4. 次のステップのために、現在の単語をサンプリング（正規化が必要）\n",
    "            probs_i = F.softmax(scores[:, i, :] / temp , dim=-1)\n",
    "            sampled_beam_idx_i = torch.multinomial(probs_i, 1).long()\n",
    "            all_preds.append( torch.gather(beam_targets[:, i], 1, sampled_beam_idx_i).squeeze(1) )\n",
    "            sampled_beam_idx.append( sampled_beam_idx_i )\n",
    "                              \n",
    "\n",
    "        all_preds = torch.stack( all_preds, dim = 0 ).transpose(0,1)\n",
    "        sampled_beam_idx = torch.stack( sampled_beam_idx, dim = 0 ).transpose(0,1)\n",
    "        \n",
    "        ## --- 全体の log_probs 算出（ratio計算用） ---\n",
    "        #max_score, _ = torch.max(scores, dim=2, keepdim=True)\n",
    "        #exp1 = torch.exp(scores - max_score)\n",
    "        #sumof = torch.sum(exp1, dim=2, keepdim=True)\n",
    "        #denominator1 = torch.log(sumof + 1e-8) + max_score\n",
    "    \n",
    "        #beam_log_probs = scores - denominator1\n",
    "        ##sample_log_probs = torch.gather( beam_log_probs, -1, sampled_beam_idx ).squeeze(-1)\n",
    "        #beam_probs = torch.exp(beam_log_probs)\n",
    "\n",
    "        #return beam_probs, preds, sample_log_probs, sampled_beam_idx\n",
    "        return scores, all_preds\n",
    "\n",
    "    def _viterbi_decode(self, emissions, masks=None, beam=None):\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        #print( \"score size:\", score.size() )\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return finalized_scores, finalized_tokens\n",
    "    \n",
    "    def _compute_many_values(self, emissions, targets, top_indices = None, masks=None, beam=None):\n",
    "\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        \n",
    "        if top_indices == None:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        else:\n",
    "            beam_emission_scores = torch.gather( emissions, -1, top_indices )\n",
    "            beam_targets = top_indices\n",
    "        \n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam  step i-1 における 256 → 256 の max から 256 への遷移確率と \n",
    "                                                # 256 → 256 の前の 256 の max のインデックストークン\n",
    "                                                # index b * 256 の 位置が i の token で、値が i-1 のtoken   \n",
    "\n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam i における 256 の遷移確率ではない確率を加える。i における 256 の全確率。\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        all_scores = traj_scores\n",
    "        all_scores.append( score )\n",
    "        all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 )\n",
    "        \n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None]) # 時刻 T における b*256 の確率最大の token\n",
    "        finalized_scores.append(best_score[:, None]) #時刻 T における b*256 の確率最大の score\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)): #idx,scs は、反転時刻 i と i-1における b * 256のトークンと確率\n",
    "            previous_index = finalized_tokens[-1] # 時刻 Tなど 求めたいトークンと確率の一個後 における token　b * 1\n",
    "            finalized_tokens.append(idx.gather(1, previous_index)) # 時刻 一個後iのトークン previou_index に至るための時刻i-1 のトークン\n",
    "                                                                    # b* 256 の token から b * 1 の previous_idnex token で gather\n",
    "            finalized_scores.append(scs.gather(1, previous_index)) # 時刻一個後 i のトークンに至るための時刻 i-1 の確率\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "        \n",
    "        if self.crf_coef != 0.0:\n",
    "            numerator = self._compute_score(emissions, targets)\n",
    "            denominator = self._compute_normalizer(emissions, targets)\n",
    "            crf_loss = - ( numerator - denominator ).mean() / seq_len\n",
    "        else:\n",
    "            crf_loss = torch.zeros( (1), device = emissions.device, dtype = torch.float )\n",
    "        \n",
    "        top_probs, sampled_beam_idx = self._compute_top_probs(beam_emission_scores, beam_transition_matrix)\n",
    "        \n",
    "        return finalized_scores, finalized_tokens, top_probs, beam_targets, crf_loss, sampled_beam_idx\n",
    "    '''\n",
    "    \n",
    "    def _compute_many_values(self, emissions, targets, top_indices = None, masks=None, beam=None):\n",
    "\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        \n",
    "        if top_indices == None:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        else:\n",
    "            beam_emission_scores = torch.gather( emissions, -1, top_indices )\n",
    "            beam_targets = top_indices\n",
    "        \n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "        \n",
    "        if self.crf_coef != 0.0:\n",
    "            numerator = self._compute_score(emissions, targets)\n",
    "            denominator = self._compute_normalizer(emissions, targets)\n",
    "            #denominator = self._compute_normalizer2(emissions, targets)\n",
    "            crf_loss = - ( numerator - denominator ).mean() / seq_len\n",
    "        else:\n",
    "            crf_loss = torch.zeros( (1), device = emissions.device, dtype = torch.float )\n",
    "\n",
    "        finalized_scores, finalized_tokens = self._compute_viterbi_no_repeat(beam_emission_scores, beam_transition_matrix, beam_targets)\n",
    "        top_probs, sampled_beam_idx = self._compute_top_probs2(beam_emission_scores, beam_transition_matrix, beam_targets)\n",
    "        \n",
    "        return finalized_scores, finalized_tokens, top_probs, beam_targets, crf_loss, sampled_beam_idx\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, crf_low_rank, crf_beam_size, dropout, padding_idx,\n",
    "                crf_coef = 1.0, temp = 0.5 ):\n",
    "        super(TopLayer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.padding_idx = padding_idx\n",
    "        print( \"in TopLayer:\" )\n",
    "        self.crf_layer = DynamicCRF(num_embedding = vocab_size, low_rank = crf_low_rank, \n",
    "                                    beam_size = crf_beam_size, crf_coef=crf_coef, temp=temp)\n",
    "\n",
    "        #self.one_more_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        #self.tgt_word_prj = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "        ## gae 学習用\n",
    "        #self.linear_critical = nn.Linear(crf_beam_size, 1 )\n",
    "\n",
    "    def forward(self, src_representation, top_logits, top_indices, src_input, tgt_input, is_training ):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        #assert src_input.size() == tgt_input.size()\n",
    "\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        #seqlen, bsz = src_input.size()\n",
    "        seqlen, bsz = src_input.shape[:2]\n",
    "\n",
    "        src_representation = F.dropout(src_representation, p=self.dropout, training=is_training)\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "\n",
    "        src = src_representation\n",
    "\n",
    "        #emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "        emissions = src_representation\n",
    "        #log_probs = torch.log_softmax(emissions, -1)\n",
    "        #assert log_probs.size() == torch.Size([seqlen, bsz, self.vocab_size])\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz x src_len x vocab_size]\n",
    "        #emission_mask = ~tgt_input.eq(self.padding_idx) # [bsz x src_len] #pad のところは 0 padでないところが 1\n",
    "        emission_mask = torch.ones_like( tgt_input, dtype=torch.bool ) #全部　pad でないとして 1\n",
    "        batch_crf_loss, top_probs = self.crf_layer(emissions, top_logits, top_indices, tgt_input, emission_mask) # [bsz]\n",
    "        #critical_value = self.linear_critical( top_probs )\n",
    "        #critical_value = torch.zeros( ( 1,1,1) )\n",
    "        batch_crf_loss = - batch_crf_loss\n",
    "        assert batch_crf_loss.size() == torch.Size([bsz])\n",
    "        return batch_crf_loss, top_probs\n",
    "\n",
    "    def decoding(self, src_representation, src_input):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        seqlen, bsz = src_input.size()\n",
    "\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "        src = src_representation\n",
    "\n",
    "        emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        _, finalized_tokens = self.crf_layer.forward_decoder(emissions)\n",
    "        assert finalized_tokens.size() == torch.Size([bsz, seqlen])\n",
    "        return finalized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HcmR9lKrIbiL"
   },
   "outputs": [],
   "source": [
    "class CaptioningTransformer(nn.Module):\n",
    "    \n",
    "    #CaptioningTransformerのコンストラクタ\n",
    "    #dim_embedding  : 埋め込み次元\n",
    "    #dim_feedforward: FNNの中間特徴次元\n",
    "    #num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    #num_layers     : Transformerデコーダ層の数\n",
    "    #vocab_size     : 辞書の次元\n",
    "    #null_index     : NULLのID\n",
    "    #dropout        : ドロップアウト確率\n",
    "    \n",
    "    def __init__(self, img_size: int,  dim_embedding: int, length_max: int, vocab_size: int, tokenizer, dropout: float = 0.0, \\\n",
    "                 pad_token_id: int=0, use_repeat_logits_half=False, crf_coef = 1.0, temp=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        #CLIP\n",
    "        model_id = \"openai/clip-vit-large-patch14-336\"\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(model_id )\n",
    "        memory = self.clip_model( torch.randn( 1, 3, 336, 336 ) )\n",
    "        memory = memory.last_hidden_state\n",
    "        img_length = memory.size(1)\n",
    "        clip_dim = memory.size(2)\n",
    "        self.connector_pool = nn.AdaptiveAvgPool1d(length_max - 1 )\n",
    "        self.connector_ln = nn.LayerNorm( clip_dim )\n",
    "        self.connector_linear1 = nn.Linear( clip_dim, dim_embedding )\n",
    "        self.connector_gleu = nn.GELU()\n",
    "        self.connector_linear2 = nn.Linear( dim_embedding, dim_embedding )\n",
    "\n",
    "       \n",
    "        # Connector\n",
    "        self.connector_pool = nn.AdaptiveAvgPool1d(length_max - 1 )\n",
    "       # Down Sampling\n",
    "        cls_token = memory[:, :1, :] # (bsz, 1, 1024)\n",
    "        patch_tokens = memory[:, 1:, :] # (bsz, 576, 1024)\n",
    "        # パッチ部分を 576 -> 96 に圧縮\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 1024, 576)\n",
    "        patch_tokens = self.connector_pool(patch_tokens)\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 96, 1024)\n",
    "        # CLSと結合して合計 97 トークンにする\n",
    "        memory = torch.cat([cls_token, patch_tokens], dim=1) # (bsz, 97, 1024)\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding( dim_embedding )\n",
    "\n",
    "        model_id = \"google-bert/bert-large-uncased\"\n",
    "        self.bert = BertModel.from_pretrained( model_id )\n",
    "\n",
    "        ## 単語出力分布計算\n",
    "        self.ln_outputs = nn.LayerNorm( dim_embedding )\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "\n",
    "        crf_low_rank = 32\n",
    "        crf_beam_size = 256\n",
    "        self.crf_beam_size = crf_beam_size\n",
    "        top_dropout = 0.0\n",
    "        tgt_padding_idx = tokenizer.pad_token_id\n",
    "        self.toplayer = TopLayer( vocab_size, dim_embedding, crf_low_rank, crf_beam_size, top_dropout, \n",
    "                                  tgt_padding_idx, crf_coef = crf_coef, temp=temp )\n",
    "        \n",
    "        ### GAE 用\n",
    "        self.ln_critical = nn.LayerNorm( crf_beam_size )\n",
    "        self.linear_critical = nn.Linear( crf_beam_size, 1)\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.use_repeat_logits_half = use_repeat_logits_half\n",
    "\n",
    "\n",
    "    def mlp_connector(self, memory ):\n",
    "\n",
    "        cls_token = memory[:, :1, :] # (bsz, 1, 1024)\n",
    "        patch_tokens = memory[:, 1:, :] # (bsz, 576, 1024)\n",
    "\n",
    "        # パッチ部分を 576 -> 96 に圧縮\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 1024, 576)\n",
    "        patch_tokens = self.connector_pool(patch_tokens)\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 96, 1024)\n",
    "\n",
    "        # CLSと結合して合計 97 トークンにする\n",
    "        memory = torch.cat([cls_token, patch_tokens], dim=1) # (bsz, 97, 1024)\n",
    "\n",
    "        memory = self.connector_ln( memory )\n",
    "        memory = self.connector_linear1( memory )\n",
    "        memory = self.connector_gleu( memory )\n",
    "        memory = self.connector_linear2( memory )\n",
    "        \n",
    "        return memory\n",
    "\n",
    "    def forward(self, images: torch.Tensor, targets: torch.Tensor, top_indices = None ):\n",
    "\n",
    "        self.device = images.device\n",
    "        \n",
    "        memory = self.clip_model( images ).last_hidden_state\n",
    "        memory = self.mlp_connector( memory )\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        emissions = self.linear( outputs )\n",
    "\n",
    "        if self.use_repeat_logits_half == True:\n",
    "            emissions = repeat_logits_half( emissions )\n",
    "\n",
    "\n",
    "        finalized_scores, finalized_tokens, top_probs, top_indices, crf_loss, sampled_beam_idx  = \\\n",
    "            self.toplayer.crf_layer._compute_many_values(emissions, targets, top_indices)\n",
    "            #self.crf_layer._compute_many_values(emissions, targets, top_indices)\n",
    "        \n",
    "        critical_value = self.ln_critical( top_probs )\n",
    "        critical_value = self.linear_critical( critical_value )\n",
    "\n",
    "        return finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "            critical_value[:,:,0], crf_loss, emissions, sampled_beam_idx\n",
    "        #return finalized_scores, finalized_tokens, top_probs, top_indices, crf_loss, emissions\n",
    "\n",
    "    def inference(self, images: torch.Tensor, inf_t = 'v' ):\n",
    "\n",
    "        self.device = images.device\n",
    "        \n",
    "        memory = self.clip_model( images ).last_hidden_state\n",
    "        memory = self.mlp_connector( memory )\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        emissions = self.linear( outputs )\n",
    "\n",
    "        if self.use_repeat_logits_half == True:\n",
    "            emissions = repeat_logits_half( emissions )\n",
    "\n",
    "        if inf_t == 'v':\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._viterbi_decode(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._viterbi_decode(emissions)\n",
    "        elif inf_t == 'v-nr':\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._compute_viterbi_no_repeat(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._compute_viterbi_no_repeat(emissions)\n",
    "        else:\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._compute_f_algorithm(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._compute_f_algorithm(emissions)\n",
    "\n",
    "        return finalized_scores, finalized_tokens\n",
    "\n",
    "    def repeat_logits_half(self, emissions ):\n",
    "        \n",
    "        penalty = 1.2\n",
    "        scores, preds = torch.max( emissions, 2 )\n",
    "        masks = emissions == scores[:,:,None]\n",
    "        masks = masks.permute( 1, 0, 2 )\n",
    "        new_mask = torch.zeros( (  masks.size(1), masks.size(2)), device = emissions.device, dtype=torch.bool )\n",
    "        new_masks = torch.zeros( ( masks.size(0), masks.size(1), masks.size(2)), device = emissions.device, dtype=torch.bool )\n",
    "        for i, mask in enumerate( masks ):\n",
    "            new_mask = torch.logical_or( mask,  new_mask  )\n",
    "            new_masks[i] = new_mask\n",
    "        new_masks = new_masks.transpose(0,1)\n",
    "        first_true_mask = ( new_masks.int().cumsum(dim = 1 ) == 1 ) & new_masks\n",
    "        new_masks = new_masks & ( ~first_true_mask )\n",
    "\n",
    "        p_masks = emissions > 0\n",
    "        m_masks = emissions < 0\n",
    "        p_new_masks = p_masks & new_masks\n",
    "        m_new_masks = m_masks & new_masks\n",
    "        emissions2 = emissions.clone()\n",
    "        emissions2[p_new_masks] = emissions[p_new_masks] / penalty\n",
    "        emissions2[m_new_masks] = emissions2[m_new_masks] * penalty\n",
    "\n",
    "        return emissions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path: str, img_directory: str, transforms, transforms2, tokenizer, length_max = None ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_directory = img_directory\n",
    "        self.transforms = transforms\n",
    "        self.transforms2 = transforms2 \n",
    "        # TODO: fix to original data\n",
    "        #画像の前処理\n",
    "        self.img_file = []\n",
    "        self.tokens = []\n",
    "        #vocab_size = len( tokenizer )\n",
    "        #c1 = torch.zeros( ( vocab_size ) )\n",
    "        #c2 = torch.zeros( ( vocab_size, vocab_size ) )\n",
    "        if length_max == None:\n",
    "            self.length_max = 0\n",
    "        else:\n",
    "            self.length_max = length_max\n",
    "        length_sum = 0\n",
    "\n",
    "        with open( file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for i, line_data in enumerate( data ):\n",
    "            if i % 100000 == 0:\n",
    "                print( \"i:\", i )\n",
    "            self.img_file.append( line_data['img_file'] )\n",
    "            id_tokens = line_data['id_tokens']\n",
    "            id_tokens.append( eos_token_id )\n",
    "            id_tokens.append( eos_token_id )\n",
    "            length_sum += len( id_tokens )\n",
    "            if length_max != None:\n",
    "                id_tokens = torch.tensor( id_tokens )[:self.length_max]\n",
    "            else:\n",
    "                if self.length_max < len( id_tokens ):\n",
    "                    self.length_max = len( id_tokens )\n",
    "                id_tokens = torch.tensor( id_tokens )\n",
    "            self.tokens.append( id_tokens )\n",
    "        # w1, w2 を作る時は length_max = None　でお願いします。\n",
    "        #    for i2 in range( len(id_tokens) ):\n",
    "        #        if i2 == len( id_tokens ) - 1:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #        else:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #            c2[id_tokens[i2], id_tokens[i2+1] ] += 1\n",
    "        '''\n",
    "        c1avg = int( torch.sum( c1 ) / torch.sum( torch.ne( c1, 0 ).int()) )\n",
    "        c2avg = int( torch.sum( torch.sum( c2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( c2, 0 ).int() ) )\n",
    "\n",
    "        c1[0] = c1avg\n",
    "\n",
    "        c2[:,0] = c2avg\n",
    "        c2[0,:] = c2avg\n",
    "        \n",
    "        sumc1 = torch.sum( c1, dim = 0 )\n",
    "        sumc2 = torch.sum( torch.sum( c2, dim = 1 ), dim = 0 )\n",
    "\n",
    "        prob1 = c1 / sumc1\n",
    "        prob2 = c2 / sumc2\n",
    "\n",
    "        self.w1 = prob1 ** -0.4\n",
    "        self.w1 = torch.nan_to_num( self.w1, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg1 = torch.sum( self.w1, dim = 0 ) / torch.sum( torch.ne( self.w1, 0.0 ).int() )\n",
    "        self.w1 = self.w1 / avg1\n",
    "\n",
    "        self.w2 = prob2 ** -0.4\n",
    "        self.w2 = torch.nan_to_num( self.w2, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg2 = torch.sum( torch.sum( self.w2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( self.w2, 0.0 ).int() )\n",
    "        self.w2 = self.w2 / avg2\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_unigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w1, f )\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_bigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w2, f )\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_unigram.pkl\", 'rb') as f:\n",
    "        #    self.w1 = pickle.load(f)\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_bigram.pkl\", 'rb') as f:\n",
    "        #    self.w2 = pickle.load(f)\n",
    "        \n",
    "        if length_max == None:\n",
    "            print( \"length max:\", self.length_max )\n",
    "            print( \"avg length:\", length_sum / len( self.tokens ) )\n",
    "    \n",
    "    # ここで取り出すデータを指定している\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        tokens = self.tokens[index]\n",
    "        img_file = self.img_file[index] + \".jpg\"\n",
    "        img_path = os.path.join( self.img_directory, img_file ) #index番目の画像のパスを取得\n",
    "        img = Image.open(img_path) #PIL形式で画像を読み込み\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        img1 = self.transforms(img)\n",
    "        img2 = self.transforms2(img)\n",
    "        \n",
    "        return img1, img2, tokens\n",
    "\n",
    "    # この method がないと DataLoader を呼び出す際にエラーを吐かれる\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def length_max(self):\n",
    "        return self.length_max\n",
    "\n",
    "    #def w1(self):\n",
    "    #    return self.w1\n",
    "\n",
    "    #def w2(self):\n",
    "    #    return self.w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, str]]], pad_index, length_max ):\n",
    "    imgs1, imgs2, tokens = zip(*batch)\n",
    "\n",
    "    max_length = length_max\n",
    "    #max_length = 0\n",
    "    #for target in tokens:\n",
    "    #    if max_length < len( target ):\n",
    "    #        max_length = len( target )\n",
    "    \n",
    "    targets = []\n",
    "    lengths = []\n",
    "    for target in tokens:\n",
    "        pad_len = max_length - len( target ) \n",
    "        #print( \"target:\", target )\n",
    "        input2= F.pad( target, (0, pad_len), mode='constant', value = pad_index)\n",
    "        targets.append( input2 )\n",
    "        lengths.append( len( target ) )\n",
    "    \n",
    "    imgs1 = torch.stack( imgs1, dim = 0 )\n",
    "    imgs2 = torch.stack( imgs2, dim = 0 )\n",
    "    targets = torch.stack( targets, dim = 0 )\n",
    "    lengths = torch.tensor( lengths, requires_grad = False  )\n",
    "\n",
    "    return imgs1, imgs2, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j4x-PO05mCS-"
   },
   "source": [
    "###学習におけるハイパーパラメータやオプションの設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "EQES3A8OG-V_"
   },
   "outputs": [],
   "source": [
    "class ConfigTrain(object):\n",
    "    '''\n",
    "    ハイパーパラメータ、システム共通変数の設定\n",
    "    '''\n",
    "    def __init__(self):\n",
    "\n",
    "        # ハイパーパラメータ\n",
    "        self.img_size = 336\n",
    "        self.dim_embedding = 1024   # 埋め込み層の次元\n",
    "        self.length_max = 97\n",
    "        self.lr_clip = 0.0\n",
    "        #self.lr_con = 1.66e-10\n",
    "        #self.lr_bert = 2.19e-8          # 学習率\n",
    "        #self.lr_cri = 7.20e-5\n",
    "        #self.lr_others = 5.82e-7\n",
    "        self.lr_con = 1.66e-11\n",
    "        self.lr_bert = 2.19e-9          # 学習率\n",
    "        self.lr_cri = 7.20e-6\n",
    "        self.lr_others = 5.82e-8\n",
    "        #self.clip_thresh_clip = 1\n",
    "        self.clip_thresh_con = 3.3e-4\n",
    "        self.clip_thresh_bert = 9e-3            # 学習率\n",
    "        self.clip_thresh_cri = 3.3e-4\n",
    "        self.clip_thresh_others = 3.3e-4\n",
    "        self.dropout = 0.0         # dropout確率\n",
    "        #self.batch_size = 160       # ミニバッチ数\n",
    "        self.batch_size = 128       # ミニバッチ数\n",
    "        #self.batch_size = 80       # ミニバッチ数\n",
    "        #self.batch_size = 64\n",
    "        #self.batch_size = 48\n",
    "        #self.batch_size = 40       # ミニバッチ数\n",
    "        #self.batch_size = 4       # ミニバッチ数\n",
    "        self.num_epochs = 1       # エポック数→Colab無料版でテストする際は10未満に修正を推奨\n",
    "        self.use_amp = True\n",
    "        #self.use_amp = False\n",
    "        self.use_saved_pth = True\n",
    "        #self.use_saved_pth = False\n",
    "        self.vocab_size = len( tokenizer )\n",
    "        self.weight_decay = 0.0232\n",
    "        self.betas = (0.9, 0.999 )\n",
    "        self.warmup = 0.1\n",
    "        self.metric = \"special\" # bleu, meteor, wer, rouge\n",
    "        #self.decode_t = \"ordinary\"\n",
    "        self.decode_t = \"no-pad\"\n",
    "        self.reward_t = \"ord+rep+len+unr\"\n",
    "        #self.reward_t = \"ordinary\"\n",
    "        self.clip_range = 0.105\n",
    "        self.clip_grad_threshold = 1.73\n",
    "        self.ord_coef = 1.0 #固定\n",
    "        self.cider_coef = 1.0 #固定\n",
    "        self.rouge_coef = 2.53\n",
    "        self.clip_coef = 1.65\n",
    "        self.bert_coef = 6.68\n",
    "        self.rep_coef = 5.84\n",
    "        self.repeat_thresh = [ 3,2,2,2]\n",
    "        self.repeat_weight = [ 1, 1, 1, 1 ]\n",
    "        self.len_coef = 2.10\n",
    "        self.unr_coef = 2.49\n",
    "        self.policy_coef = 1.0 #固定\n",
    "        self.crf_coef = 0.0784\n",
    "        self.ce_coef = 0.688\n",
    "        self.ent_coef = 0.00269\n",
    "        self.cri_coef = 0.0 # モンテカルロ法\n",
    "        self.gae_coef = 2.177 # GAE\n",
    "        self.kl_coef = 0.0401\n",
    "        self.target_kl = 8.0\n",
    "        self.buffer_kl = 1.2\n",
    "        self.kl_max = 0.1\n",
    "        self.kl_min = 0.1\n",
    "        self.gamma = 0.972\n",
    "        self.lam = 0.974\n",
    "        self.ratio_clamp_max = -1.0 # -1.0 ratio is free.  0.0 ratio calmp 1.1 for mainas advantage.  value > 0 ratio clamps value  \n",
    "        self.use_repeat_logits_half = False\n",
    "        self.use_ce_bert = True\n",
    "        self.display_include_coef = True\n",
    "        self.use_adaptive_KL = False\n",
    "        self.temp = 0.710\n",
    "        \n",
    "        # パスの設定\n",
    "        self.img_directory = '/mnt/ssd2/v7/img'\n",
    "        self.anno_file = '/mnt/ssd2/v7/data.pkl'\n",
    "        self.save_directory = './model'\n",
    "        #self.PATH = \"model/model_ar_hfgpt2_v7_curr.pth\"\n",
    "        #self.PATH = \"../test/model/model_bert_large_NAR_PAD_curr.pth\"\n",
    "        self.PATH = \"../pre_train_crf/model/model_bert_large_NAR_PAD_sft2_curr.pth\"\n",
    "        #self.PATH = \"model/model_bert_large_NAR_PAD_curr.pth\"\n",
    "        #self.PATH = \"model/model_bert_mask_curr.pth\"\n",
    "\n",
    "        # 検証に使う学習セット内のデータの割合\n",
    "        self.test_ratio = 0.1\n",
    "        self.val_ratio = 0.1\n",
    "        #self.val_ratio = 0.0004\n",
    "        #self.test_ratio = 0.0004\n",
    "        \n",
    "        # 学習に使うデバイス\n",
    "        #self.device = 'cuda'\n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        #self.device = 'cpu'\n",
    "        \n",
    "        # データローダーに使うCPUプロセスの数\n",
    "        #self.num_workers = 4\n",
    "        self.num_workers = 0 if self.device == torch.device('cpu') else 10\n",
    "        #self.num_workers = 0 if self.device == torch.device('cpu') else 4\n",
    "        #self.num_workers = 0\n",
    "        \n",
    "        # 移動平均で計算する損失の値の数\n",
    "        self.moving_avg = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--NNWCwZI5qS"
   },
   "source": [
    "### 学習を行う関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xBOP-3aIHFjB",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i: 0\n",
      "i: 100000\n",
      "i: 200000\n",
      "i: 300000\n",
      "i: 400000\n",
      "i: 500000\n",
      "config.device: cuda:0\n",
      "学習セット数: 3172\n",
      "評価セット数: 397\n",
      "テストセット数: 397\n",
      "use_amp: True\n",
      "use_saved_pth: True\n",
      "in TopLayer:\n",
      "in TopLayer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default tokenizer.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_global_steps: 3172\n",
      "warmup: 0.1\n",
      "num_warmup_steps: 317.20000000000005\n",
      "use_saved_pth: True\n",
      "PATH: ../pre_train_crf/model/model_bert_large_NAR_PAD_sft2_curr.pth\n",
      "exist saved_pth: True\n",
      "model parameters were loaded\n",
      "ref_model parameters were loaded\n",
      "begin_epoch: 0\n",
      "global_step: 0\n",
      "file_param: 5\n",
      "train_param: 105\n",
      "val_param: 39\n",
      "epochs: 1\n",
      "batch_size: 128\n",
      "lr_clip: 0.0\n",
      "lr_con: 1.66e-11\n",
      "lr_bert: 2.19e-09\n",
      "lr_cri: 7.2e-06\n",
      "lr_others: 5.82e-08\n",
      "weight_decay: 0.0232\n",
      "betas: (0.9, 0.999)\n",
      "metric: special\n",
      "reward_type: ord+rep+len+unr\n",
      "decode_type: no-pad\n",
      "clip_range ppo clip: 0.105\n",
      "clip_grad_threshold gradient norm: 1.73\n",
      "ord_coef: 1.0\n",
      "cider_coef: 1.0\n",
      "rouge_coef: 2.53\n",
      "clip_coef: 1.65\n",
      "rep_coef: 5.84\n",
      "repeat_thresh: [3, 2, 2, 2]\n",
      "repeat_weight: [1, 1, 1, 1]\n",
      "len_coef: 2.1\n",
      "unr_coef: 2.49\n",
      "policy_coef: 1.0\n",
      "crf_coef: 0.0784\n",
      "ce_coef: 0.688\n",
      "ent_coef: 0.00269\n",
      "gae_coef: 2.177\n",
      "kl_coef: 0.0401\n",
      "target_kl: 8.0\n",
      "buffer_kl: 1.2\n",
      "kl_max: 0.1\n",
      "kl_min: 0.1\n",
      "gamma: 0.972\n",
      "lambda: 0.974\n",
      "use_repeat_logits_half: False\n",
      "use_ce_bert: True\n",
      "ratio_clamp_max: -1.0\n",
      "display_include_coef: True\n",
      "temp: 0.71\n",
      "train_loss_file: ./model/MyOriginal_train_loss_20260207_154858.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276b7d8d6bf74b068d7bf963713a25e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3172 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_399440/2133460519.py:234: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  reward = torch.tensor(reward).to(self.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr con   : 5.2332912988650685e-14\n",
      "lr bert  : 6.904161412358133e-12\n",
      "lr cri   : 2.2698612862547284e-08\n",
      "lr others: 1.8348045397225723e-10\n",
      "Train epoch = 0.0, loss = 8.559783935546875, policy = -1.2289617101757244e-09, entropy_loss = -0.0027348455041646957, gae = 7.0471320152282715, kl_div = 0.07950864732265472, reward = 4.05485725402832, ord = 6.9692702293396, repeat = -4.290160655975342, length = -0.08032941073179245, adv = 0.926466703414917, rougeL = 0.47733697295188904, cider = 0.2550114095211029, clip = 0.2443714141845703, crf = 0.058361779898405075, ce = 1.3775149583816528, unr = 1.4560779333114624, ber = 0.8469656109809875\n",
      "refe: [CLS] this picture shows a backpack on the table [SEP]\n",
      "hypo: [CLS] in this picture we can see there is a bag. [SEP]\n",
      "samp: [CLS] in this image we can see two luggage on the floor. [SEP]\n",
      "lr con   : 5.547288776796972e-12\n",
      "lr bert  : 7.318411097099621e-10\n",
      "lr cri   : 2.406052963430012e-06\n",
      "lr others: 1.9448928121059264e-08\n",
      "Train epoch = 0.03310214375788146, loss = 8.853841781616211, policy = 0.24353818595409393, entropy_loss = -0.0027539317961782217, gae = 7.048803806304932, kl_div = 0.08279240876436234, reward = 4.092357635498047, ord = 7.018232822418213, repeat = -4.328281879425049, length = -0.08402759581804276, adv = 0.9364237189292908, rougeL = 0.4895220696926117, cider = 0.2473592311143875, clip = 0.2460416704416275, crf = 0.06030106544494629, ce = 1.421158790588379, unr = 1.4864344596862793, ber = 0.8492879867553711\n",
      "refe: [CLS] in this picture, we see many people are standing and most of them are holding the books in their hands. in the background, we see the pillars and a white wall on which photo frames are placed. at the top, we see the lights and the ceiling of the room. at the bottom, we see the floor. [SEP]\n",
      "hypo: [CLS] in this image i can see a group of people standing and in the floor. there is a photo frames. there is a wall. [SEP]\n",
      "samp: [CLS] in this image i can see some people standing and some people. in the background there is a photo frames, there is a wall. [SEP]\n",
      "lr con   : 1.1042244640605294e-11\n",
      "lr bert  : 1.456778058007566e-09\n",
      "lr cri   : 4.789407313997477e-06\n",
      "lr others: 3.871437578814627e-08\n",
      "Train epoch = 0.06620428751576292, loss = 9.687826156616211, policy = 0.4335997402667999, entropy_loss = -0.0026837827172130346, gae = 7.6877851486206055, kl_div = 0.08232808858156204, reward = 4.3491411209106445, ord = 7.022977828979492, repeat = -4.112457752227783, length = -0.09307929873466492, adv = 0.9879886507987976, rougeL = 0.49436113238334656, cider = 0.23995772004127502, clip = 0.2459624707698822, crf = 0.06125650927424431, ce = 1.425539493560791, unr = 1.5317001342773438, ber = 0.8484622836112976\n",
      "refe: [CLS] in this image, we can see a person wearing a helmet and in the water. in the background, we can see stones, waterfall and plants. [SEP]\n",
      "hypo: [CLS] in this image there is a man standing on the center of the rocks. in the water, there is a rock. [SEP]\n",
      "samp: [CLS] in this image there is a man i can see a man standing. in his can see there is a rock. [SEP]\n",
      "lr con   : 1.6537200504413617e-11\n",
      "lr bert  : 2.18171500630517e-09\n",
      "lr cri   : 7.172761664564942e-06\n",
      "lr others: 5.7979823455233285e-08\n",
      "Train epoch = 0.09930643127364439, loss = 9.880561828613281, policy = 0.4848419725894928, entropy_loss = -0.00263606826774776, gae = 7.834560871124268, kl_div = 0.08205923438072205, reward = 4.427957057952881, ord = 7.011866569519043, repeat = -4.041178226470947, length = -0.0948311910033226, adv = 1.0016518831253052, rougeL = 0.49578285217285156, cider = 0.23369137942790985, clip = 0.2458086907863617, crf = 0.061311304569244385, ce = 1.4204248189926147, unr = 1.5521000623703003, ber = 0.8476780652999878\n",
      "refe: [CLS] in this picture i can see many trees, plants, grass and a swimming pool. in that swimming pool i can see some tubes and balls. on the right there are three women who are wearing swimsuit and they are standing under the water. beside them there is a man. at the bottom there is a woman who is lying on this tube. in front of her there is a man who is smiling, beside him there is a woman who is wearing black swimwear. [SEP]\n",
      "hypo: [CLS] in this image we can see a group of the swimming pool. we can see a swimming pool. in the ground. [SEP]\n",
      "samp: [CLS] in this picture we can see a group of the swimming pool. there are pool. in the swimming pool. [SEP]\n",
      "lr con   : 1.599642707019756e-11\n",
      "lr bert  : 2.110372005044136e-09\n",
      "lr cri   : 6.938209331651954e-06\n",
      "lr others: 5.608385876418663e-08\n",
      "Train epoch = 0.13240857503152584, loss = 9.944077491760254, policy = 0.5031790733337402, entropy_loss = -0.0026190856005996466, gae = 7.881847381591797, kl_div = 0.08333408087491989, reward = 4.46036434173584, ord = 7.000038146972656, repeat = -3.9987142086029053, length = -0.09523583948612213, adv = 1.0028834342956543, rougeL = 0.49625664949417114, cider = 0.2317221760749817, clip = 0.24604620039463043, crf = 0.06128295511007309, ce = 1.417053461074829, unr = 1.5542768239974976, ber = 0.8464098572731018\n",
      "refe: [CLS] in this image we can see few buildings. in front of the buildings we can see few vehicles and persons. in the foreground we can see few poles. on the left side, we can see a pole with lights. behind the buildings we can see the trees. at the top we can see the sky. [SEP]\n",
      "hypo: [CLS] in this image we can see few vehicles on the road. on the road. on the road, poles, we can see the road. in the road. on the sky. [SEP]\n",
      "samp: [CLS] in this image i can see few buildings with windows, there are moving on the road. in the road. i can see the road. in the road. [SEP] the sky. at the sky. [SEP]. [SEP]. [SEP]\n",
      "lr con   : 1.5385876418663304e-11\n",
      "lr bert  : 2.029823455233291e-09\n",
      "lr cri   : 6.673392181588902e-06\n",
      "lr others: 5.394325346784363e-08\n",
      "Train epoch = 0.16551071878940732, loss = 10.311429023742676, policy = 0.5708116292953491, entropy_loss = -0.0025866674259305, gae = 8.183077812194824, kl_div = 0.08306267857551575, reward = 4.596428871154785, ord = 6.999902725219727, repeat = -3.8795053958892822, length = -0.0982312485575676, adv = 1.0233572721481323, rougeL = 0.4984014928340912, cider = 0.23027870059013367, clip = 0.2459433525800705, crf = 0.06110093742609024, ce = 1.4159616231918335, unr = 1.574262261390686, ber = 0.8459173440933228\n",
      "refe: [CLS] in this image in the center there is a sculpture of animal, and in the background there is a wall and some sculptures and pillar. at the bottom there is a walkway. [SEP]\n",
      "hypo: [CLS] in this image we can see there is a sculpture. [SEP] pillars. [SEP]\n",
      "samp: [CLS] in this image i can see there is an object. [SEP] pillars. [SEP]\n",
      "lr con   : 1.4775325767129044e-11\n",
      "lr bert  : 1.9492749054224466e-09\n",
      "lr cri   : 6.4085750315258504e-06\n",
      "lr others: 5.1802648171500625e-08\n",
      "Train epoch = 0.19861286254728877, loss = 10.803081512451172, policy = 0.6902976036071777, entropy_loss = -0.0025402221363037825, gae = 8.555968284606934, kl_div = 0.08294632285833359, reward = 4.778553009033203, ord = 6.999212741851807, repeat = -3.7244765758514404, length = -0.10211021453142166, adv = 1.048732876777649, rougeL = 0.5005391836166382, cider = 0.2293590009212494, clip = 0.24581077694892883, crf = 0.06149538606405258, ce = 1.4149136543273926, unr = 1.6059271097183228, ber = 0.8455976843833923\n",
      "refe: [CLS] in this image we can see a painting. [SEP]\n",
      "hypo: [CLS] in this image we can see a painting of a wall, we can see a person. [SEP]\n",
      "samp: [CLS] in this image i can see a poster with a person's face. [SEP]\n",
      "lr con   : 1.4164775115594786e-11\n",
      "lr bert  : 1.8687263556116012e-09\n",
      "lr cri   : 6.143757881462799e-06\n",
      "lr others: 4.966204287515762e-08\n",
      "Train epoch = 0.23171500630517025, loss = 10.885589599609375, policy = 0.6437505483627319, entropy_loss = -0.002532524522393942, gae = 8.689092636108398, kl_div = 0.08296819031238556, reward = 4.863150119781494, ord = 6.9952874183654785, repeat = -3.6461758613586426, length = -0.10181230306625366, adv = 1.0514088869094849, rougeL = 0.5025534629821777, cider = 0.22591058909893036, clip = 0.245931476354599, crf = 0.06104990467429161, ce = 1.4112590551376343, unr = 1.6158503293991089, ber = 0.8455823659896851\n",
      "refe: [CLS] in this image we can see chocolate covers of different colors like red, blue, green and pink. [SEP]\n",
      "hypo: [CLS] in this image we can see few objects. [SEP]\n",
      "samp: [CLS] in this image i can see the group of the row different colors. [SEP] packets. [SEP]\n",
      "lr con   : 1.3554224464060528e-11\n",
      "lr bert  : 1.7881778058007565e-09\n",
      "lr cri   : 5.878940731399747e-06\n",
      "lr others: 4.752143757881462e-08\n",
      "Train epoch = 0.2648171500630517, loss = 11.510505676269531, policy = 0.5822107791900635, entropy_loss = -0.0024628753308206797, gae = 9.358745574951172, kl_div = 0.08250324428081512, reward = 5.145359992980957, ord = 7.011948108673096, repeat = -3.4059603214263916, length = -0.10704133659601212, adv = 1.0933698415756226, rougeL = 0.5145402550697327, cider = 0.21747465431690216, clip = 0.24562881886959076, crf = 0.07144027948379517, ce = 1.418068289756775, unr = 1.6464130878448486, ber = 0.8454747200012207\n",
      "refe: [CLS] in this image i can see a woman hanging on a rope with a helmet. she is posing for the picture. i can see trees behind her. [SEP]\n",
      "hypo: [CLS] in this image we can see a woman standing on the background we can also there are trees and trees. [SEP]\n",
      "samp: [CLS] in this picture we can see a woman standing on the background we can also there are trees and trees. [SEP] sky. [SEP]\n",
      "lr con   : 1.2943673812526271e-11\n",
      "lr bert  : 1.7076292559899118e-09\n",
      "lr cri   : 5.614123581336696e-06\n",
      "lr others: 4.5380832282471625e-08\n",
      "Train epoch = 0.2979192938209332, loss = 12.14521598815918, policy = 0.48102039098739624, entropy_loss = -0.0023605376482009888, gae = 10.093656539916992, kl_div = 0.08085264265537262, reward = 5.456589221954346, ord = 7.044737339019775, repeat = -3.165231943130493, length = -0.10917019844055176, adv = 1.1423758268356323, rougeL = 0.5264347195625305, cider = 0.2121061384677887, clip = 0.2453555315732956, crf = 0.08297589421272278, ce = 1.4090707302093506, unr = 1.686254620552063, ber = 0.8459852337837219\n",
      "refe: [CLS] in this image we can see a dog. behind the dog there is a wooden fencing. in the back there are trees, hills and a building. [SEP]\n",
      "hypo: [CLS] in this image we can see a dog on the ground, we can see grass, trees. [SEP]\n",
      "samp: [CLS] in this image i can see a dog there is a dog on the bottom of the background i can see a wooden fence. [SEP]\n",
      "lr con   : 1.2333123160992013e-11\n",
      "lr bert  : 1.6270807061790668e-09\n",
      "lr cri   : 5.349306431273644e-06\n",
      "lr others: 4.324022698612862e-08\n",
      "Train epoch = 0.33102143757881464, loss = 12.502052307128906, policy = 0.31603163480758667, entropy_loss = -0.0022769200149923563, gae = 10.605648040771484, kl_div = 0.08026814460754395, reward = 5.678057670593262, ord = 7.066177845001221, repeat = -2.985267400741577, length = -0.1113286092877388, adv = 1.1730839014053345, rougeL = 0.5391671657562256, cider = 0.20007267594337463, clip = 0.24533596634864807, crf = 0.0953599363565445, ce = 1.4070227146148682, unr = 1.7084749937057495, ber = 0.8459593057632446\n",
      "refe: [CLS] this is a railing and here is the glass windows. outside the glass windows there is a train on the railway track. in the background we can observe sky and clouds. [SEP]\n",
      "hypo: [CLS] in this image we can see few buildings with windows, we can see windows, windows. [SEP]\n",
      "samp: [CLS] in this image we can see a train on the center of the bottom of the buildings, lights. [SEP]\n",
      "lr con   : 1.1722572509457753e-11\n",
      "lr bert  : 1.5465321563682219e-09\n",
      "lr cri   : 5.084489281210592e-06\n",
      "lr others: 4.109962168978562e-08\n",
      "Train epoch = 0.3641235813366961, loss = 12.975180625915527, policy = 0.2130374014377594, entropy_loss = -0.0021840271074324846, gae = 11.173803329467773, kl_div = 0.07987895607948303, reward = 5.919729232788086, ord = 7.087358474731445, repeat = -2.7839620113372803, length = -0.11430399119853973, adv = 1.2085057497024536, rougeL = 0.5516318678855896, cider = 0.18987956643104553, clip = 0.24495168030261993, crf = 0.10729562491178513, ce = 1.4033492803573608, unr = 1.7306365966796875, ber = 0.8454633951187134\n",
      "refe: [CLS] in this image i can see a person holding a fish and his smiling and his wearing a gray color t - shirt and in front of him there is a bottle kept on the table. front of him. on the right side i can see a bottle. and i can see a wall. and left side i can see a window. [SEP]\n",
      "hypo: [CLS] in this image we can see a man and holding an object. in his hand holding an object. in front of him. [SEP]\n",
      "samp: [CLS] in this image i can see a man and holding an object. in his hand holding an object. in front of him there are bottles, there are flask. [SEP]\n",
      "lr con   : 1.1112021857923497e-11\n",
      "lr bert  : 1.465983606557377e-09\n",
      "lr cri   : 4.819672131147541e-06\n",
      "lr others: 3.895901639344262e-08\n",
      "Train epoch = 0.39722572509457754, loss = 13.352373123168945, policy = 0.11050859093666077, entropy_loss = -0.0020976844243705273, gae = 11.644282341003418, kl_div = 0.07944684475660324, reward = 6.124237537384033, ord = 7.111083984375, repeat = -2.618910312652588, length = -0.11725293844938278, adv = 1.2367228269577026, rougeL = 0.562344491481781, cider = 0.18778078258037567, clip = 0.24477963149547577, crf = 0.11896096169948578, ce = 1.4012706279754639, unr = 1.7493172883987427, ber = 0.845299243927002\n",
      "refe: [CLS] at the bottom of the image there is grass on the surface. in the center of the image there are buildings. there are poles. in the background of the image there are trees, mountains and sky. on the left side of the image there is a tree. [SEP]\n",
      "hypo: [CLS] in this image in the image we can see grass, plants, in the background there is houses, houses. [SEP] mountains. [SEP]\n",
      "samp: [CLS] in this picture there is a house, houses, grass, plants, plants. background there is covered with clouds in the sky. [SEP]\n",
      "lr con   : 1.0501471206389239e-11\n",
      "lr bert  : 1.3854350567465322e-09\n",
      "lr cri   : 4.554854981084489e-06\n",
      "lr others: 3.681841109709962e-08\n",
      "Train epoch = 0.430327868852459, loss = 13.21008014678955, policy = 0.11187080293893814, entropy_loss = -0.0020999517291784286, gae = 11.501569747924805, kl_div = 0.07988960295915604, reward = 6.1055121421813965, ord = 7.1134819984436035, repeat = -2.6392672061920166, length = -0.11652206629514694, adv = 1.220252513885498, rougeL = 0.5615823864936829, cider = 0.19151075184345245, clip = 0.2446620911359787, crf = 0.11892205476760864, ce = 1.399928092956543, unr = 1.7478197813034058, ber = 0.8454976677894592\n",
      "refe: [CLS] in this image i can see a road and on it i can see two cars, one person and a bus. i can also see one man is sitting in the bus. on the right side of this image i can see a building, a bus stop, three persons where two are sitting and one is standing. i can also see few boards on the right side and on it i can see something is written. in the background i can see few poles, few trees, the\n",
      "hypo: [CLS] in this image, we can see a bus in the bus on the bus, we can see few people, we can see few people are people walking on the right side, there are walking on the sky. [SEP]\n",
      "samp: [CLS] in this image, we can see a bus in front of the bus in the road. on the left side of people walking on the left side there are walking on the vehicles on the left side. [SEP]\n",
      "lr con   : 9.890920554854979e-12\n",
      "lr bert  : 1.304886506935687e-09\n",
      "lr cri   : 4.290037831021437e-06\n",
      "lr others: 3.4677805800756614e-08\n",
      "Train epoch = 0.4634300126103405, loss = 13.011963844299316, policy = 0.11134479194879532, entropy_loss = -0.002099299570545554, gae = 11.2935791015625, kl_div = 0.08067983388900757, reward = 6.059212684631348, ord = 7.103336334228516, repeat = -2.6612660884857178, length = -0.11937442421913147, adv = 1.1990337371826172, rougeL = 0.5621652007102966, cider = 0.18551276624202728, clip = 0.24426788091659546, crf = 0.1195870041847229, ce = 1.408872365951538, unr = 1.736517071723938, ber = 0.8446096777915955\n",
      "refe: [CLS] in this picture we can see a hydrant on the road. [SEP]\n",
      "hypo: [CLS] in this image we can see an object. [SEP]\n",
      "samp: [CLS] in this image we can see an object. [SEP]\n",
      "lr con   : 9.280369903320723e-12\n",
      "lr bert  : 1.2243379571248423e-09\n",
      "lr cri   : 4.025220680958385e-06\n",
      "lr others: 3.253720050441362e-08\n",
      "Train epoch = 0.49653215636822196, loss = 13.025315284729004, policy = 0.11146331578493118, entropy_loss = -0.002093785908073187, gae = 11.3034086227417, kl_div = 0.08136460930109024, reward = 6.094249725341797, ord = 7.090028762817383, repeat = -2.609910011291504, length = -0.12248069047927856, adv = 1.1933317184448242, rougeL = 0.5645464062690735, cider = 0.17856931686401367, clip = 0.24379977583885193, crf = 0.11987683176994324, ce = 1.4112939834594727, unr = 1.736611008644104, ber = 0.8429884910583496\n",
      "refe: [CLS] in this picture we can see a kid is smiling in the front, at the bottom there are some leaves, we can see a blurry background. [SEP]\n",
      "hypo: [CLS] in this image we can see a boy smiling. [SEP]\n",
      "samp: [CLS] in this picture we can see a boy smiling. [SEP]\n",
      "lr con   : 8.669819251786464e-12\n",
      "lr bert  : 1.1437894073139976e-09\n",
      "lr cri   : 3.760403530895334e-06\n",
      "lr others: 3.039659520807062e-08\n",
      "Train epoch = 0.5296343001261034, loss = 13.193367958068848, policy = 0.08667688071727753, entropy_loss = -0.00209049298427999, gae = 11.489516258239746, kl_div = 0.0812709778547287, reward = 6.198419094085693, ord = 7.0878496170043945, repeat = -2.5167973041534424, length = -0.12704163789749146, adv = 1.201786756515503, rougeL = 0.5684651136398315, cider = 0.17003461718559265, clip = 0.24353908002376556, crf = 0.12034426629543304, ce = 1.4176510572433472, unr = 1.7544074058532715, ber = 0.8422900438308716\n",
      "refe: [CLS] on the right of this picture we can see a woman wearing a costume, smiling and seems to be dancing. in the background, we can see the wall and a group of persons standing and holding a white color object seems to be the feathers and we can see some other objects. [SEP]\n",
      "hypo: [CLS] in this image we can see a woman wearing a woman standing. [SEP]. [SEP]\n",
      "samp: [CLS] in this image i can see a woman wearing a woman standing. [SEP] dress. [SEP]\n",
      "lr con   : 8.059268600252206e-12\n",
      "lr bert  : 1.0632408575031527e-09\n",
      "lr cri   : 3.495586380832282e-06\n",
      "lr others: 2.8255989911727615e-08\n",
      "Train epoch = 0.5627364438839849, loss = 13.403144836425781, policy = 0.08355225622653961, entropy_loss = -0.002084200270473957, gae = 11.70743179321289, kl_div = 0.08109133690595627, reward = 6.3158440589904785, ord = 7.084463119506836, repeat = -2.4140443801879883, length = -0.12964555621147156, adv = 1.213471531867981, rougeL = 0.5736028552055359, cider = 0.16589830815792084, clip = 0.24329358339309692, crf = 0.11995915323495865, ce = 1.4131933450698853, unr = 1.7750717401504517, ber = 0.841594398021698\n",
      "refe: [CLS] in this image i can see a woman is standing. in the background i can see candles, boards which has something written on it. here i can see some text on the image. [SEP]\n",
      "hypo: [CLS] in this image there is a person standing on the center of the right side. [SEP]\n",
      "samp: [CLS] in this picture there is a person standing on the center of the background there are some text on the right side, there are some text on the table. [SEP]\n",
      "lr con   : 7.448717948717948e-12\n",
      "lr bert  : 9.826923076923077e-10\n",
      "lr cri   : 3.2307692307692305e-06\n",
      "lr others: 2.6115384615384613e-08\n",
      "Train epoch = 0.5958385876418664, loss = 13.720947265625, policy = 0.08577732741832733, entropy_loss = -0.0020719976164400578, gae = 12.027576446533203, kl_div = 0.08069811016321182, reward = 6.458587169647217, ord = 7.0965962409973145, repeat = -2.2989706993103027, length = -0.133269801735878, adv = 1.2308058738708496, rougeL = 0.5775798559188843, cider = 0.16429732739925385, clip = 0.24294602870941162, crf = 0.11979730427265167, ce = 1.4091699123382568, unr = 1.7942309379577637, ber = 0.8410633206367493\n",
      "refe: [CLS] in this picture we can see a crowd of people where in front woman wore red color jacket, spectacle and person here also wore red color jacket and catching a sheet with their hand and in the background we can see trees, sky, building and people are holding banners in their hand. here it is a pole, light and this is a ground. [SEP]\n",
      "hypo: [CLS] in this image, we can see few people standing. in their hands. in front of the background of the background. [SEP]\n",
      "samp: [CLS] in this image i can see a group of people, in their hands and holding an object. at the background there is a few people standing. [SEP] are lights. [SEP]\n",
      "lr con   : 6.83816729718369e-12\n",
      "lr bert  : 9.021437578814628e-10\n",
      "lr cri   : 2.965952080706179e-06\n",
      "lr others: 2.3974779319041614e-08\n",
      "Train epoch = 0.6289407313997478, loss = 14.085779190063477, policy = 0.08868054300546646, entropy_loss = -0.0020642043091356754, gae = 12.387482643127441, kl_div = 0.0801924392580986, reward = 6.630157470703125, ord = 7.101876258850098, repeat = -2.1596567630767822, length = -0.1366199404001236, adv = 1.2548938989639282, rougeL = 0.581229567527771, cider = 0.16019092500209808, clip = 0.24287085235118866, crf = 0.12019943445920944, ce = 1.4112876653671265, unr = 1.8245582580566406, ber = 0.8414658904075623\n",
      "refe: [CLS] in this picture we can observe a graphic. there is a person wearing black color dress. we can observe glass windows behind him. there is a photo frame fixed to the wall on the left side. in the background there is a sky with some clouds. [SEP]\n",
      "hypo: [CLS] in this image i can see an animated image in the image. [SEP]\n",
      "samp: [CLS] in this image i can see an animated image in the wall. [SEP]\n",
      "lr con   : 6.227616645649432e-12\n",
      "lr bert  : 8.215952080706179e-10\n",
      "lr cri   : 2.7011349306431273e-06\n",
      "lr others: 2.183417402269861e-08\n",
      "Train epoch = 0.6620428751576293, loss = 13.987944602966309, policy = 0.1523861438035965, entropy_loss = -0.002091886941343546, gae = 12.228602409362793, kl_div = 0.08038119226694107, reward = 6.5999979972839355, ord = 7.098532676696777, repeat = -2.1894967555999756, length = -0.13629795610904694, adv = 1.2409262657165527, rougeL = 0.5782502889633179, cider = 0.1625474989414215, clip = 0.24297861754894257, crf = 0.11533612012863159, ce = 1.4133286476135254, unr = 1.8272608518600464, ber = 0.8418238162994385\n",
      "refe: [CLS] here in this picture we can see a person standing over a place and carrying the baby in arms and we can also see a ring in one of the person's finger. [SEP]\n",
      "hypo: [CLS] in this image we can see a person is holding an object. in his hand in his hand. in his hand. [SEP]\n",
      "samp: [CLS] in this image we can see a person is holding an object. in his hand of his hand. in his hand. [SEP]. [SEP]. [SEP]\n",
      "lr con   : 5.617065994115174e-12\n",
      "lr bert  : 7.410466582597729e-10\n",
      "lr cri   : 2.4363177805800753e-06\n",
      "lr others: 1.969356872635561e-08\n",
      "Train epoch = 0.6951450189155107, loss = 13.235260009765625, policy = 0.26729729771614075, entropy_loss = -0.0021708880085498095, gae = 11.375083923339844, kl_div = 0.08134021610021591, reward = 6.302224636077881, ord = 7.0779619216918945, repeat = -2.440678596496582, length = -0.13187307119369507, adv = 1.1762750148773193, rougeL = 0.5644225478172302, cider = 0.1748899668455124, clip = 0.24391059577465057, crf = 0.10289674997329712, ce = 1.4108127355575562, unr = 1.7968149185180664, ber = 0.8426992893218994\n",
      "refe: [CLS] there is one person walking and wearing a red color t shirt and a bag, and holding two covers in the middle of this image. there are some drink items and food items are kept in a different racks as we can see in the background. there is a wall at the top of this image, and there is a floor at the bottom of this image. [SEP]\n",
      "hypo: [CLS] in this image we can see a person standing and a person wearing spectacles. in the background we can also we can see a bag. [SEP] we can see few objects. [SEP]\n",
      "samp: [CLS] in this image i can see a person standing and a person wearing spectacles. at the left side, we can see a bag. in the racks in the racks in the left side. [SEP]\n",
      "lr con   : 5.006515342580916e-12\n",
      "lr bert  : 6.604981084489281e-10\n",
      "lr cri   : 2.1715006305170237e-06\n",
      "lr others: 1.755296343001261e-08\n",
      "Train epoch = 0.7282471626733922, loss = 12.82742691040039, policy = 0.297167032957077, entropy_loss = -0.0021998926531523466, gae = 10.939936637878418, kl_div = 0.08193019032478333, reward = 6.163417816162109, ord = 7.072213172912598, repeat = -2.5572011470794678, length = -0.1277730017900467, adv = 1.1437801122665405, rougeL = 0.5580813884735107, cider = 0.18072548508644104, clip = 0.24436064064502716, crf = 0.09839610755443573, ce = 1.412196159362793, unr = 1.7761797904968262, ber = 0.8433590531349182\n",
      "refe: [CLS] in this image, we can see food on the white surface. [SEP]\n",
      "hypo: [CLS] in this image i can see food items. [SEP]\n",
      "samp: [CLS] in this image there is a piece of the food items. [SEP]\n",
      "lr con   : 4.395964691046658e-12\n",
      "lr bert  : 5.799495586380832e-10\n",
      "lr cri   : 1.906683480453972e-06\n",
      "lr others: 1.541235813366961e-08\n",
      "Train epoch = 0.7613493064312736, loss = 12.593310356140137, policy = 0.28637781739234924, entropy_loss = -0.0021954139228910208, gae = 10.723554611206055, kl_div = 0.08187868446111679, reward = 6.111751079559326, ord = 7.066331386566162, repeat = -2.5955660343170166, length = -0.12354408204555511, adv = 1.129083514213562, rougeL = 0.5559818148612976, cider = 0.18169327080249786, clip = 0.24490179121494293, crf = 0.0978696346282959, ce = 1.4058243036270142, unr = 1.7645297050476074, ber = 0.8441841006278992\n",
      "refe: [CLS] in this image we can see three people are sitting on the chairs. there are many objects placed on the table. there is an advertising board in the image. a lady is speaking into a microphone and holding a book in her hand. [SEP]\n",
      "hypo: [CLS] in this image we can see few people sitting in front of the table there is a table, in front of the table, there is a table on the table, we can see a table. [SEP]\n",
      "samp: [CLS] in this image i can see a man sitting in front of the table in front of the table in front of him there is a microphone in her hand of the table we can see a table. [SEP] blazer and smiling. [SEP]\n",
      "lr con   : 3.7854140395124e-12\n",
      "lr bert  : 4.994010088272383e-10\n",
      "lr cri   : 1.6418663303909203e-06\n",
      "lr others: 1.3271752837326606e-08\n",
      "Train epoch = 0.7944514501891551, loss = 12.236767768859863, policy = 0.2858331799507141, entropy_loss = -0.00220366008579731, gae = 10.36567497253418, kl_div = 0.08179526031017303, reward = 6.0013813972473145, ord = 7.064871788024902, repeat = -2.691018581390381, length = -0.12058908492326736, adv = 1.1034517288208008, rougeL = 0.5518757104873657, cider = 0.18169564008712769, clip = 0.24544082581996918, crf = 0.09804981201887131, ce = 1.4076168537139893, unr = 1.7481176853179932, ber = 0.8447681665420532\n",
      "refe: [CLS] in the image we can see there are four people wearing a life jacket and they are sitting on a boat. here we can see water. [SEP]\n",
      "hypo: [CLS] in this image there are few people are three people sitting. [SEP]\n",
      "samp: [CLS] in this image in the foreground there are sitting. [SEP]\n",
      "lr con   : 3.174863387978142e-12\n",
      "lr bert  : 4.1885245901639344e-10\n",
      "lr cri   : 1.3770491803278687e-06\n",
      "lr others: 1.1131147540983606e-08\n",
      "Train epoch = 0.8275535939470365, loss = 12.258206367492676, policy = 0.19330303370952606, entropy_loss = -0.002154311863705516, gae = 10.475078582763672, kl_div = 0.08116831630468369, reward = 6.057242393493652, ord = 7.080142021179199, repeat = -2.648869514465332, length = -0.11988461762666702, adv = 1.111232876777649, rougeL = 0.5558068752288818, cider = 0.18224142491817474, clip = 0.24559953808784485, crf = 0.10581598430871964, ce = 1.4049944877624512, unr = 1.7458540201187134, ber = 0.8453050255775452\n",
      "refe: [CLS] in this image we can see few plants with flowers and there is a butterfly and we can see grass on the ground. the image is blurred in the background. [SEP]\n",
      "hypo: [CLS] in this image we can see an insect, plants. [SEP]\n",
      "samp: [CLS] in this image i can see a dragon front of the plants. [SEP]\n",
      "lr con   : 2.5643127364438838e-12\n",
      "lr bert  : 3.3830390920554855e-10\n",
      "lr cri   : 1.112232030264817e-06\n",
      "lr others: 8.990542244640606e-09\n",
      "Train epoch = 0.860655737704918, loss = 12.673384666442871, policy = 0.08123549818992615, entropy_loss = -0.0020720362663269043, gae = 10.990337371826172, kl_div = 0.08045980334281921, reward = 6.265862941741943, ord = 7.093924522399902, repeat = -2.467719793319702, length = -0.12219065427780151, adv = 1.1487919092178345, rougeL = 0.5640935897827148, cider = 0.1741369664669037, clip = 0.24536113440990448, crf = 0.11808618903160095, ce = 1.4053375720977783, unr = 1.7618496417999268, ber = 0.8447397351264954\n",
      "refe: [CLS] in this picture we can see a bottle of drink and in the background we can see a monitor and on the right side of the image there is a keyboard, on the left bottom of the image i can see a toy we can read amazon on that. [SEP]\n",
      "hypo: [CLS] in this image we can see a bottle, there is a table. [SEP]\n",
      "samp: [CLS] in this image we can see a laptop, laptop. [SEP]\n",
      "lr con   : 1.9537620849096257e-12\n",
      "lr bert  : 2.5775535939470367e-10\n",
      "lr cri   : 8.474148802017654e-07\n",
      "lr others: 6.8499369482976035e-09\n",
      "Train epoch = 0.8937578814627994, loss = 12.671259880065918, policy = 0.08507534116506577, entropy_loss = -0.002050891751423478, gae = 10.977200508117676, kl_div = 0.0807669460773468, reward = 6.275587558746338, ord = 7.082712173461914, repeat = -2.446012496948242, length = -0.12391584366559982, adv = 1.1477410793304443, rougeL = 0.5608100295066833, cider = 0.17083095014095306, clip = 0.24502278864383698, crf = 0.12035040557384491, ce = 1.4099162817001343, unr = 1.762803554534912, ber = 0.8440597653388977\n",
      "refe: [CLS] in this image i can see the person statue. in the background i can see few trees in green color. [SEP]\n",
      "hypo: [CLS] in this image we can see a statue. in the background. [SEP]\n",
      "samp: [CLS] in this image we can see a statue. [SEP] on the background. [SEP]\n",
      "lr con   : 1.3432114333753678e-12\n",
      "lr bert  : 1.7720680958385878e-10\n",
      "lr cri   : 5.825977301387137e-07\n",
      "lr others: 4.709331651954603e-09\n",
      "Train epoch = 0.926860025220681, loss = 12.682879447937012, policy = 0.09518483281135559, entropy_loss = -0.002034541219472885, gae = 10.979601860046387, kl_div = 0.08084394782781601, reward = 6.28826904296875, ord = 7.077431678771973, repeat = -2.4296677112579346, length = -0.12373624742031097, adv = 1.1479246616363525, rougeL = 0.5578697323799133, cider = 0.1742745190858841, clip = 0.24494798481464386, crf = 0.12032536417245865, ce = 1.4089579582214355, unr = 1.7642408609390259, ber = 0.8438274264335632\n",
      "refe: [CLS] a man with black jacket is standing and playing a guitar. to her right side there is a man with grey jacket is standing and he wore white color shoes. and in the background there are trees. [SEP]\n",
      "hypo: [CLS] in this image we can see a man standing and he is a guitar and playing guitar. [SEP]\n",
      "samp: [CLS] in this image i can see a man who is playing guitar and he wore his hand. [SEP]\n",
      "lr con   : 7.326607818411096e-13\n",
      "lr bert  : 9.665825977301387e-11\n",
      "lr cri   : 3.17780580075662e-07\n",
      "lr others: 2.5687263556116013e-09\n",
      "Train epoch = 0.9599621689785625, loss = 12.64616584777832, policy = 0.09656732529401779, entropy_loss = -0.0020207951311022043, gae = 10.935223579406738, kl_div = 0.08095939457416534, reward = 6.283758640289307, ord = 7.064876556396484, repeat = -2.419938325881958, length = -0.12534263730049133, adv = 1.1453639268875122, rougeL = 0.5555893778800964, cider = 0.17475858330726624, clip = 0.2449103593826294, crf = 0.12092401832342148, ce = 1.4145129919052124, unr = 1.7641632556915283, ber = 0.8437440395355225\n",
      "refe: [CLS] there is a black and white handbag and the background is white in color. [SEP]\n",
      "hypo: [CLS] in this image we can see a bag. [SEP]\n",
      "samp: [CLS] in this picture we can see a bag. [SEP]\n",
      "lr con   : 1.221101303068516e-13\n",
      "lr bert  : 1.610970996216898e-11\n",
      "lr cri   : 5.2963430012610336e-08\n",
      "lr others: 4.281210592686002e-10\n",
      "Train epoch = 0.9930643127364439, loss = 12.721601486206055, policy = 0.10763842612504959, entropy_loss = -0.00200263480655849, gae = 11.004668235778809, kl_div = 0.08104071021080017, reward = 6.31489372253418, ord = 7.063627243041992, repeat = -2.391676425933838, length = -0.12494907528162003, adv = 1.150291919708252, rougeL = 0.5534474849700928, cider = 0.18152786791324615, clip = 0.2450312077999115, crf = 0.12055149674415588, ce = 1.4097062349319458, unr = 1.7678923606872559, ber = 0.8438069224357605\n",
      "refe: [CLS] in this image a lady is there in front of her there is a chair. in the background on the wall there is graffiti. [SEP]\n",
      "hypo: [CLS] in this image i can see a woman sitting on the chair, in the floor. [SEP]\n",
      "samp: [CLS] in this image we can see a woman sitting on the chair, trouser. [SEP]. [SEP]\n",
      "bsz 68 is not batch_size 128. skip\n",
      "Train loss: 12.7380952835083\n",
      "Train policy: 0.10811328142881393\n",
      "Train entropy: -0.0019993612077087164\n",
      "Train gae: 11.023822784423828\n",
      "Train kl_div: 0.08105257153511047\n",
      "Train reward: 6.323427200317383\n",
      "Train reward2: 1.7631983757019043\n",
      "Train ord: 7.063066005706787\n",
      "Train repeat: -2.3837838172912598\n",
      "Train pad: -0.1242738887667656\n",
      "Train adv: 1.151757836341858\n",
      "Train clip: 0.24498474597930908\n",
      "Train rougeL: 0.5525954961776733\n",
      "Train cider: 0.182052880525589\n",
      "Train crf: 0.12028823792934418\n",
      "Train ce: 1.4068161249160767\n",
      "Train unr: 1.7684189081192017\n",
      "Train bert: 0.8436982035636902\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7523ab53c621495f91e371681aea7d6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/397 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val epoch = 0, rougeL = 0.5954697728157043, cider = 0.15073002874851227, clip = 0.2443309873342514, bert = 0.847651481628418\n",
      "refe: [CLS] in this image i can see few trees which are green in color, few flowers which are red in color and in the background i can see a person standing, the road, few vehicles, few buildings, few trees and the sky. [SEP]\n",
      "hypo: [CLS] in this image we can see plants, there are plants, there are plants. in the road. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5897189378738403, cider = 0.2039428949356079, clip = 0.24424496293067932, bert = 0.8463338017463684\n",
      "refe: [CLS] this picture is clicked in a room. there are group of persons sitting on chairs besides a table. there is a man, towards the left and remaining are towards the right. a person in the center, he is wearing a black blazer and holding a mike. towards the right corner there is a table and a chair. [SEP]\n",
      "hypo: [CLS] in this image we can see a group of people sitting on the table there are sitting on the table. [SEP]\n",
      "Val epoch = 0, rougeL = 0.588978111743927, cider = 0.19581174850463867, clip = 0.24410943686962128, bert = 0.8458007574081421\n",
      "refe: [CLS] in this image we can see the buildings and trees. here we can see the flag poles on the top of the building. here we can see the vehicles on the road. [SEP]\n",
      "hypo: [CLS] in this image we can see many trees, there are many trees. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5876959562301636, cider = 0.19367007911205292, clip = 0.24397265911102295, bert = 0.845377504825592\n",
      "refe: [CLS] it is an edited image. in this image we can see the people. we can also see the zombies, trees and also many vehicles passing on the road. image also consists of buildings. sky is also visible. [SEP]\n",
      "hypo: [CLS] in this image we can see a group of people standing on the road. in the road. in the road. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5865421295166016, cider = 0.19040551781654358, clip = 0.2439475953578949, bert = 0.8452366590499878\n",
      "refe: [CLS] in this picture we can see a fish on a boat and this boat is on the water, fishing rod. [SEP]\n",
      "hypo: [CLS] in this image we can see a boat on the water. in the background we can see water. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5862807631492615, cider = 0.19240930676460266, clip = 0.24387402832508087, bert = 0.8448832631111145\n",
      "refe: [CLS] there is a small girl in the center of the image, it seems like trees and sky in the background area. [SEP]\n",
      "hypo: [CLS] in this image we can see a girl. in the background there is blurry. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5854862332344055, cider = 0.18883655965328217, clip = 0.24379178881645203, bert = 0.8447042107582092\n",
      "refe: [CLS] there are many people. some are holding candle on a stand. lady on the left side is wearing a bag and holding a mobile. near to her another lady is wearing a specs. in the back there is water. and it is dark in the background. [SEP]\n",
      "hypo: [CLS] in this image we can see a group of people standing. in their hands. in their hands. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5835235118865967, cider = 0.1874396950006485, clip = 0.24385002255439758, bert = 0.8443799018859863\n",
      "refe: [CLS] there are people, we can see railing. we can see board, glass and objects on the table. in the background we can see device on the table, speakers and door. far there are things. [SEP]\n",
      "hypo: [CLS] in this image we can see a group of people standing on the floor. in the floor. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5843363404273987, cider = 0.19327688217163086, clip = 0.24389377236366272, bert = 0.844816267490387\n",
      "refe: [CLS] in this image we can see a man and woman are sitting on the chairs in front of the mics. there is a glass on the table. in the background we can see a flower vase, window and a pillar. [SEP]\n",
      "hypo: [CLS] in this image we can see two persons sitting on the chair. in front of them there are sitting on the table. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5861594676971436, cider = 0.2002371996641159, clip = 0.24361999332904816, bert = 0.8453735113143921\n",
      "refe: [CLS] this picture is inside view of a room. we can see two girls are sitting and holding an objects in their hands. at the top of the image we can mirror, christmas tree and door, wall are present. at the bottom of the image floor is there. [SEP]\n",
      "hypo: [CLS] in this image we can see two children sitting on the floor. in their hands. in the background we can see a wall. [SEP]\n",
      "Val epoch = 0, rougeL = 0.5868113040924072, cider = 0.20479950308799744, clip = 0.24361278116703033, bert = 0.8453487157821655\n",
      "refe: [CLS] there are buildings having glass windows. in the background, there is blue sky. [SEP]\n",
      "hypo: [CLS] in this image we can see a building. [SEP]\n",
      "Validation rougeL: 0.5866187214851379\n",
      "Validation cider: 0.20941708981990814\n",
      "Validation clip: 0.24369139969348907\n",
      "Validation bert: 0.8454430103302002\n"
     ]
    }
   ],
   "source": [
    "#os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = 'true'\n",
    "config = ConfigTrain()\n",
    "\n",
    "#tokenizer = BertTokenizer.from_pretrained(pretrained_model_name_or_path = config.bert_model_path)\n",
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "# モデル出力用のディレクトリを作成\n",
    "os.makedirs(config.save_directory, exist_ok=True)\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((336, 336)),\n",
    "    v2.AutoAugment(),\n",
    "    #v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    ## Coco データセット 2017 train の平均と標準偏差\n",
    "    #v2.Normalize((0.456,0.427,0.401),(0.224,0.219,0.231) )\n",
    "    # ImageNetデータセットの平均と標準偏差\n",
    "    #v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    # Clip Model の config から引用。\n",
    "    v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "transforms2 = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(),\n",
    "    #v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "# v7 データセット\n",
    "train_dataset = MyDataset( file_path=config.anno_file,\n",
    "                           img_directory = config.img_directory,\n",
    "                           transforms=transforms, transforms2 = transforms2,\n",
    "                           tokenizer=tokenizer, length_max = config.length_max)\n",
    "\n",
    "# Subset samplerの生成\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, config.test_ratio, config.val_ratio )\n",
    "    \n",
    "# 学習時にランダムにサンプルするためのサンプラー\n",
    "train_sampler = SubsetRandomSampler(train_set)\n",
    "#test_sampler = SubsetRandomSampler(test_set)\n",
    "\n",
    "# DataLoaderを生成\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id, config.length_max)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=train_sampler,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "#train_loader = torch.utils.data.DataLoader(\n",
    "#                    train_dataset,\n",
    "#                    #batch_size=config.batch_size,\n",
    "#                    batch_size=config.batch_size,\n",
    "#                    num_workers=config.num_workers,\n",
    "#                    sampler=test_sampler,\n",
    "#                    collate_fn=collate_func_lambda)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=val_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    #batch_size=config.batch_size,\n",
    "                    batch_size=config.batch_size,\n",
    "                    num_workers=config.num_workers,\n",
    "                    sampler=test_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "\n",
    "print( \"config.device:\", config.device )\n",
    "print( \"学習セット数:\",len( train_loader ) )\n",
    "print( \"評価セット数:\",len( val_loader ))\n",
    "print( \"テストセット数:\",len( test_loader ))\n",
    "print( \"use_amp:\", config.use_amp )\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "\n",
    "# モデルの定義\n",
    "#model = CaptioningTransformer(img_size = config.img_size, length_max = config.length_max,  dim_embedding=config.dim_embedding, \\\n",
    "#                              vocab_size=vocab_size, tokenizer=tokenizer, \\\n",
    "#                             dropout = config.dropout, pad_token_id = tokenizer.pad_token_id ).to(config.device)\n",
    "# モデルの定義\n",
    "model = CaptioningTransformer( config.img_size,\n",
    "    config.dim_embedding, config.length_max, config.vocab_size,\n",
    "    tokenizer, config.dropout, pad_token_id = tokenizer.pad_token_id,\n",
    "    use_repeat_logits_half = config.use_repeat_logits_half,\n",
    "    crf_coef = config.crf_coef, temp=config.temp )\n",
    "model.to(config.device)\n",
    "\n",
    "\n",
    "ref_model = CaptioningTransformer( config.img_size,\n",
    "    config.dim_embedding, config.length_max, config.vocab_size,\n",
    "    tokenizer, config.dropout, pad_token_id = tokenizer.pad_token_id,\n",
    "    use_repeat_logits_half = config.use_repeat_logits_half,\n",
    "    crf_coef = config.crf_coef, temp=1.0 )\n",
    "ref_model.to(config.device)\n",
    "\n",
    "#compute_reward = ComputeReward( reward_t = config.reward_t, decode_t = config.decode_t, device = config.device, \n",
    "#                               sentence_level_metric=config.metric, repeat_thresh = config.repeat_thresh, \n",
    "#                          repeat_weight = config.repeat_weight, cider_coef = config.cider_coef, rouge_coef = config.rouge_coef, \n",
    "#                          clip_coef = config.clip_coef, use_amp = config.use_amp )\n",
    "compute_reward = ComputeReward( reward_t = config.reward_t, decode_t = config.decode_t, device = config.device, \n",
    "                                sentence_level_metric=config.metric, repeat_thresh = config.repeat_thresh, \n",
    "                                repeat_weight = config.repeat_weight, cider_coef = config.cider_coef, \n",
    "                                rouge_coef = config.rouge_coef, clip_coef = config.clip_coef, bert_coef = config.bert_coef,\n",
    "                                use_amp = config.use_amp )\n",
    "\n",
    "\n",
    "# 最適化手法の定義\n",
    "# Optimizerの生成, clipとそうでないモジュールとの\n",
    "# パラメータで異なる学習率を適用\n",
    "#params_clip = []\n",
    "params_con = []\n",
    "params_bert = []\n",
    "params_others = []\n",
    "params_cri = []\n",
    "for name, parameter in model.named_parameters():\n",
    "    if parameter.requires_grad:\n",
    "        if 'clip_model' in name:\n",
    "            #params_clip.append(parameter)\n",
    "            parameter.requires_grad = False\n",
    "        elif 'connector' in name:\n",
    "            params_con.append(parameter)\n",
    "        elif 'bert' in name and 'critical' not in name:\n",
    "            params_bert.append(parameter)\n",
    "        elif 'critical' in name:\n",
    "            params_cri.append(parameter)\n",
    "        else:\n",
    "            params_others.append(parameter)\n",
    "param_groups = [\n",
    "    #{'params': params_clip, 'lr': config.lr_clip},\n",
    "    {'params': params_con, 'lr': config.lr_con},\n",
    "    {'params': params_bert, 'lr': config.lr_bert},\n",
    "    {'params': params_cri, 'lr': config.lr_cri},\n",
    "    {'params': params_others, 'lr': config.lr_others}]\n",
    "\n",
    "optimizer = torch.optim.AdamW( param_groups, weight_decay = config.weight_decay, betas=config.betas )\n",
    "thresh_groups = {\n",
    "#    'clip': config.clip_thresh_clip,\n",
    "    'con': config.clip_thresh_con,\n",
    "    'bert': config.clip_thresh_bert,\n",
    "    'cri': config.clip_thresh_cri,\n",
    "    'others':config.clip_thresh_others\n",
    "}\n",
    "\n",
    "# 全ステップ数\n",
    "num_global_steps = len( train_loader ) * config.num_epochs\n",
    "print( \"num_global_steps:\", num_global_steps )\n",
    "print( \"warmup:\", config.warmup )\n",
    "num_warmup_steps = num_global_steps * config.warmup\n",
    "print( \"num_warmup_steps:\", num_warmup_steps )\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup( optimizer, num_warmup_steps, num_global_steps )   \n",
    "\n",
    "print( \"use_saved_pth:\", config.use_saved_pth )\n",
    "print( \"PATH:\", config.PATH )\n",
    "print( \"exist saved_pth:\", os.path.isfile(config.PATH) ) \n",
    "use_saved_pth = config.use_saved_pth\n",
    "if use_saved_pth and os.path.isfile(config.PATH):\n",
    "    checkpoint = torch.load(config.PATH, map_location=torch.device('cpu'))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict = False)\n",
    "    print( \"model parameters were loaded\")\n",
    "    ref_model.load_state_dict(checkpoint['model_state_dict'], strict = False)\n",
    "    ref_model.eval() # 必須：DropoutやBatchNormを無効化\n",
    "    for param in ref_model.parameters():\n",
    "        param.requires_grad = False # 必須：メモリ節約と誤学習防止\n",
    "    ref_model = ref_model.to(config.device )\n",
    "    print( \"ref_model parameters were loaded\")\n",
    "\n",
    "    #optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    #scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "    #device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    ## optimizerのstateを現在のdeviceに移す。これをしないと、保存前後でdeviceの不整合が起こる可能性がある。\n",
    "    #for state in optimizer.state.values():\n",
    "        #for k, v in state.items():\n",
    "            #if isinstance(v, torch.Tensor):\n",
    "                #state[k] = v.to(device)\n",
    "    #begin_epoch = checkpoint['epoch']\n",
    "    #loss = checkpoint['loss']\n",
    "    #global_step = checkpoint['global_step']    \n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "else:\n",
    "    begin_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "file_param = 5\n",
    "print( \"begin_epoch:\", begin_epoch )\n",
    "print( \"global_step:\", global_step )\n",
    "print( \"file_param:\", file_param )\n",
    "\n",
    "def get_nearest_multiple(a, b):\n",
    "    \"\"\"\n",
    "    a に最も近い b の倍数を求める\n",
    "    \"\"\"\n",
    "    # a/b を四捨五入して、それに b を掛ける\n",
    "    # round() は .5 の場合、偶数側に丸める性質があるため、\n",
    "    # 厳密な四捨五入が必要な場合は整数演算を使用する\n",
    "    return round(a / b) * b\n",
    "\n",
    "len_tr_loader = len( train_loader )\n",
    "train_param = len_tr_loader // 30\n",
    "#train_param = len_tr_loader // 10\n",
    "len_val_loader = len( val_loader )\n",
    "#train_param = len_val_loader // 3\n",
    "train_param = get_nearest_multiple( train_param, file_param )\n",
    "val_param = len_val_loader // 10\n",
    "print( \"train_param:\", train_param )\n",
    "print( \"val_param:\", val_param )\n",
    "\n",
    "print( \"epochs:\", config.num_epochs )\n",
    "print( \"batch_size:\", config.batch_size )\n",
    "print( \"lr_clip:\", config.lr_clip )\n",
    "print( \"lr_con:\", config.lr_con )\n",
    "print( \"lr_bert:\", config.lr_bert )\n",
    "print( \"lr_cri:\", config.lr_cri )\n",
    "print( \"lr_others:\", config.lr_others )\n",
    "#print( \"clip_grad_threshold:\", config.clip_grad_threshold ) \n",
    "if config.clip_grad_threshold == 0.0:\n",
    "    print( 'clip_thresh_con:', config.clip_thresh_con )\n",
    "    print( 'clip_thresh_bert:', config.clip_thresh_bert )\n",
    "    print( 'clip_thresh_cri:', config.clip_thresh_cri )\n",
    "    print( 'clip_thresh_others:',config.clip_thresh_others )\n",
    "print( \"weight_decay:\", config.weight_decay )\n",
    "print( \"betas:\", config.betas )\n",
    "print( \"metric:\", config.metric )\n",
    "print( \"reward_type:\", config.reward_t )\n",
    "print( \"decode_type:\", config.decode_t )\n",
    "print( \"clip_range ppo clip:\", config.clip_range )\n",
    "print( \"clip_grad_threshold gradient norm:\", config.clip_grad_threshold)\n",
    "print( \"ord_coef:\", config.ord_coef )\n",
    "print( \"cider_coef:\", config.cider_coef )\n",
    "print( \"rouge_coef:\", config.rouge_coef )\n",
    "print( \"clip_coef:\", config.clip_coef )\n",
    "print( \"rep_coef:\", config.rep_coef )\n",
    "print( \"repeat_thresh:\", config.repeat_thresh )\n",
    "print( \"repeat_weight:\", config.repeat_weight )\n",
    "print( \"len_coef:\", config.len_coef )\n",
    "print( \"unr_coef:\", config.unr_coef )\n",
    "print( \"policy_coef:\", config.policy_coef )\n",
    "print( \"crf_coef:\", config.crf_coef )\n",
    "print( \"ce_coef:\", config.ce_coef )\n",
    "print( \"ent_coef:\", config.ent_coef )\n",
    "#print( \"cri_coef:\", config.cri_coef )\n",
    "print( \"gae_coef:\", config.gae_coef )\n",
    "print( \"kl_coef:\", config.kl_coef )\n",
    "print( \"target_kl:\", config.target_kl )\n",
    "print( \"buffer_kl:\", config.buffer_kl )\n",
    "print( \"kl_max:\", config.kl_max )\n",
    "print( \"kl_min:\", config.kl_min )\n",
    "print( \"gamma:\", config.gamma )\n",
    "print( \"lambda:\", config.lam )\n",
    "print( \"use_repeat_logits_half:\", config.use_repeat_logits_half )\n",
    "print( \"use_ce_bert:\", config.use_ce_bert )\n",
    "print( \"ratio_clamp_max:\", config.ratio_clamp_max )\n",
    "print( \"display_include_coef:\", config.display_include_coef )\n",
    "print( \"temp:\", config.temp )\n",
    "\n",
    "# 学習経過の書き込み\n",
    "now = datetime.datetime.now()\n",
    "train_loss_file = '{}/MyOriginal_train_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(train_loss_file, 'a') as f:\n",
    "    print(f'{len_tr_loader}', file=f) \n",
    "print( \"train_loss_file:\", train_loss_file )\n",
    "val_loss_file = '{}/MyOriginal_val_loss_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "with open(val_loss_file, 'a') as f:\n",
    "    print(f'{len_val_loader}', file=f) \n",
    "norm_file = '{}/norm_{}.csv'\\\n",
    "    .format(config.save_directory, now.strftime('%Y%m%d_%H%M%S'))\n",
    "\n",
    "# 学習\n",
    "val_loss_best = float('inf')\n",
    "\n",
    "#fn = bleu_score.SmoothingFunction().method7\n",
    "#bleu_func = BLEU(effective_order=\"True\")\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# AMP用のスケーラー\n",
    "scaler = GradScaler(enabled=config.use_amp)\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "eps = 1e-8\n",
    "last_sample_log_probs = 0\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "def entropy_func(probs):\n",
    "    input_probs = torch.clamp( probs, eps )\n",
    "    log_probs = torch.log( input_probs )\n",
    "    p_log_p = probs * log_probs\n",
    "    return - p_log_p.sum(-1)\n",
    "\n",
    "def baseline( probs, targets, top_k ):\n",
    "    probs_k, preds_k = torch.topk( probs, dim = 2, k = top_k )\n",
    "    renorm_probs_k = probs_k / torch.sum( probs_k, dim = 2 )[:,:, None]\n",
    "    base_ = torch.stack( [ probs_k[:,:,k] * compue_reward._compute_reward(preds_k[:,:,k], targets, sources = None ) \\\n",
    "                        for k in range( top_k ) ], dim = 0 )\n",
    "    base = torch.sum( base_, dim = 0 )\n",
    "    \n",
    "    return base\n",
    "\n",
    "def custom_gradient_clipping(clip_params, gpt2_params, cri_params, others_params, \n",
    "                             clip_threshold, gpt2_threshold, cri_threshold, others_threshold):\n",
    "    # エンコーダーの勾配クリッピング\n",
    "    if clip_params:\n",
    "        # torch.nn.utils.clip_grad_norm_ は、与えられたパラメータのリストに勾配クリッピングを適用する\n",
    "        torch.nn.utils.clip_grad_norm_(clip_params, clip_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if gpt2_params:\n",
    "        torch.nn.utils.clip_grad_norm_(gpt2_params, gpt2_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if cri_params:\n",
    "        torch.nn.utils.clip_grad_norm_(cri_params, cri_threshold)\n",
    "    # デコーダーの勾配クリッピング\n",
    "    if gpt2_params:\n",
    "        torch.nn.utils.clip_grad_norm_(others_params, others_threshold)\n",
    "\n",
    "def generalized_advantage_estimation(rewards, values, gamma=0.99, lam=0.95):\n",
    "\n",
    "    B, T = rewards.size()\n",
    "    \n",
    "    # 各タイムステップでのTD誤差 (delta_t) を計算\n",
    "    # delta_t = R_t + gamma * V(s_{t+1}) - V(s_t)\n",
    "    # values[:, :-1] は V(s_t)、values[:, 1:] は V(s_{t+1}) に対応\n",
    "    deltas = rewards + gamma * values[:, 1:] - values[:, :-1]\n",
    "    # deltas の形状は (B, T)\n",
    "\n",
    "    advantages = torch.zeros((B, T), device=rewards.device, dtype=torch.float)\n",
    "    \n",
    "    # 最後のタイムステップのadvantageは delta_t そのもの\n",
    "    advantages[:, T-1] = deltas[:, T-1]\n",
    "    \n",
    "    for t in range(T - 2, -1, -1):\n",
    "        advantages[:, t] = deltas[:, t] + gamma * lam * advantages[:, t+1]\n",
    "        \n",
    "    return advantages\n",
    "    \n",
    "#if config.metric == 'rouge':\n",
    "#    rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "def my_index( list1, target ):\n",
    "    if target in list1:\n",
    "        return list1.index( target )\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for epoch in range(config.num_epochs):\n",
    "    with tqdm(train_loader) as pbar:\n",
    "    #with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[エポック {epoch + 1}]')\n",
    "\n",
    "        # 学習モードに設定\n",
    "        model.train()\n",
    "\n",
    "        train_losses = deque()\n",
    "        train_policys = deque()\n",
    "        train_entropies = deque()\n",
    "        train_critics = deque()\n",
    "        train_kl_divs = deque()\n",
    "        train_rewards = deque()\n",
    "        train_rewards2 = deque()\n",
    "        train_ord = deque()\n",
    "        train_repeat = deque()\n",
    "        train_length = deque()\n",
    "        train_adv = deque()\n",
    "        train_errors = deque()\n",
    "        train_bleus = deque()\n",
    "        train_crfs = deque()\n",
    "        train_ces = deque()\n",
    "        train_clips = deque()\n",
    "        train_unrs = deque()\n",
    "        train_berts = deque()\n",
    "        for n_batch, (imgs, imgs2, captions, caption_lengths) in enumerate( pbar ):\n",
    "            #print( \"captions[0]:\", captions[0] )\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            imgs2 = imgs2.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "\n",
    "            if imgs.size(0) != config.batch_size:\n",
    "                print( f\"bsz {imgs.size(0)} is not batch_size {config.batch_size}. skip\")\n",
    "                continue\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 最後の単語から次を予測する必要はないため最後の単語を除外\n",
    "            with autocast(str(config.device),enabled=config.use_amp):\n",
    "                finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "                critical_value, crf_loss, bert_logits, sampled_beam_idx  = \\\n",
    "                model( imgs, captions, top_indices = None )\n",
    "                bsz, seq_len, beam = top_probs.size()\n",
    "                if config.use_ce_bert == False:\n",
    "                    ce_tensor = torch.full((bsz, seq_len, vocab_size), float(eps), device=config.device)\n",
    "                    ce_tensor = torch.scatter( ce_tensor, 2, top_indices, top_probs )\n",
    "                    ce_tensor = torch.clamp( ce_tensor, eps )\n",
    "                    log_ce_tensor = torch.log( ce_tensor )\n",
    "                hypo_ids = finalized_tokens\n",
    "                reward_ord, reward_ord2, reward_repeat, reward_length, reward_unr, preds, sample_log_probs \\\n",
    "                    = compute_reward( top_probs, sampled_beam_idx, top_indices, captions, imgs2 )\n",
    "                with torch.no_grad():\n",
    "                    #if config.use_inference == True:\n",
    "                    #    _, ref_captions, _, _, _, _ = model.inference( imgs )\n",
    "                    #else:\n",
    "                    #    ref_probs = F.softmax( logits, dim = 2 )\n",
    "                    #    ref_captions = torch.multinomial( ref_probs.view( bsz * seq_len, -1 ), num_samples = 1 ).view( bsz, seq_len )\n",
    "                    #ref_logits, _ = ref_model( imgs )\n",
    "                    #ref_top_logits = torch.gather( ref_logits, -1, top_indices )\n",
    "                    #_, ref_top_probs, _ = model.toplayer( ref_logits, ref_top_logits, top_indices, \\\n",
    "                    #                                                   ref_top_logits, ref_captions, is_training = True )\n",
    "                    ref_captions = preds\n",
    "                    _, _, ref_top_probs, _, _, _, _, _ = ref_model( imgs, ref_captions, top_indices = top_indices )\n",
    "\n",
    "                \n",
    "                \n",
    "                # 1. Policy側の対数確率（学習対象）\n",
    "                tmp = torch.clamp( top_probs, min = eps )\n",
    "                top_log_probs = torch.log( tmp )\n",
    "                policy_lp = torch.gather(top_log_probs, -1, sampled_beam_idx).squeeze(-1) # lp は log_prob の略と思われる。 bsz * seq_len\n",
    "                \n",
    "                # 2. Reference側の対数確率（固定）\n",
    "                tmp = torch.clamp( ref_top_probs, min = eps )\n",
    "                ref_top_log_probs = torch.log(tmp)\n",
    "                ref_lp = torch.gather(ref_top_log_probs, -1, sampled_beam_idx).squeeze(-1)\n",
    "                \n",
    "                if config.decode_t == \"no-pad\":\n",
    "                    lengths = []\n",
    "                    for pred in preds:\n",
    "                        length = my_index( pred.tolist(), eos_token_id )\n",
    "                        if length != -1:\n",
    "                            lengths.append( length )\n",
    "                        else:\n",
    "                            lengths.append( 0 )\n",
    "                    lengths = torch.tensor( lengths, device = config.device )[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "                    arange1 = torch.arange( 0, preds.size(1), device = config.device )\n",
    "                    arange1 = arange1[None,:].expand( preds.size(0), -1 )\n",
    "                    \n",
    "                    masks = arange1 < ( lengths + 2 )\n",
    "                \n",
    "                    kl_divs = ( policy_lp - ref_lp ) * masks.float()\n",
    "                else:\n",
    "                    kl_divs = policy_lp - ref_lp \n",
    "                \n",
    "                rewards = config.ord_coef * reward_ord + config.rep_coef * reward_repeat + config.len_coef * reward_length \\\n",
    "                    + config.unr_coef * reward_unr\n",
    "                rewards2 = reward_ord2 + reward_repeat + reward_length + reward_unr\n",
    "                \n",
    "                if global_step == 0:\n",
    "                    last_sample_log_probs = sample_log_probs.detach()\n",
    "        \n",
    "                ratio = torch.exp(sample_log_probs - last_sample_log_probs[:sample_log_probs.size(0)]) # bsz * seq_len\n",
    "                rewards_t = torch.zeros( ( bsz, seq_len ) , device = config.device, dtype = torch.float)\n",
    "                rewards_t[:,-1] = rewards[:,0]\n",
    "                values = critical_value\n",
    "                zeros = torch.zeros((values.size(0), 1), device=values.device, dtype=torch.float)\n",
    "                values_cat = torch.cat((values, zeros), dim=1)\n",
    "                advantages = generalized_advantage_estimation(rewards_t, values_cat, gamma=config.gamma, lam=config.lam)\n",
    "                advantages_norm = (advantages - advantages.mean()) / (advantages.std() + eps) # bsz * seq_len\n",
    "        \n",
    "                if config.ratio_clamp_max == -1.0:\n",
    "\n",
    "                    # clipped surrogate loss\n",
    "                    policy_loss_1 = - advantages_norm * ratio\n",
    "                    policy_loss_2 = - advantages_norm * torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range)\n",
    "                \n",
    "                    policy_loss = - torch.min(policy_loss_1, policy_loss_2).mean() # bsz * seq_len の平均\n",
    "                \n",
    "                elif config.ratio_clamp_max != 0.0:                \n",
    "        \n",
    "                    ratio = torch.clamp( ratio , min = 0.0, max = config.ratio_clamp_max )\n",
    "                \n",
    "                    # clipped surrogate loss\n",
    "                    policy_loss_1 = - advantages_norm * ratio\n",
    "                    policy_loss_2 = - advantages_norm * torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range)\n",
    "                \n",
    "                    #policy_loss = - torch.min(policy_loss_1, policy_loss_2).mean() # bsz * seq_len の平均\n",
    "                    if config.decode_t == \"no-pad\":\n",
    "                        policy_loss = ( torch.min( policy_loss_1, policy_loss_2) )\n",
    "                        policy_loss = policy_loss.sum() / bsz / masks.float().sum()\n",
    "                    else:\n",
    "                        policy_loss = ( torch.min( policy_loss_1, policy_loss_2))\n",
    "                        policy_loss = policy_loss.sum() / bsz / seq_len\n",
    "                else:\n",
    "                    mul_minus = ( advantages_norm > 0.0 ).float() * 2.0 - 1.0 \n",
    "                \n",
    "                    # clipped surrogate loss\n",
    "                    policy_loss_1 = - advantages_norm * ratio\n",
    "                    policy_loss_2 = - advantages_norm * torch.clamp(ratio, 1 - config.clip_range, 1 + config.clip_range)\n",
    "                \n",
    "                    if config.decode_t == \"no-pad\":\n",
    "                        policy_loss_before = ( torch.min( torch.abs(policy_loss_1), torch.abs( policy_loss_2)) )\n",
    "                        policy_loss = ( policy_loss_before * mul_minus ).sum() / bsz / masks.float().sum()\n",
    "                    else:\n",
    "                        policy_loss_before = ( torch.min(torch.abs(policy_loss_1), torch.abs(policy_loss_2)))\n",
    "                        policy_loss = (policy_loss_before * mul_minus).sum() / bsz / seq_len\n",
    "\n",
    "                entropy = entropy_func(top_probs)\n",
    "                entropy_loss = - torch.mean(entropy)\n",
    "\n",
    "                targets = (values + advantages).detach()\n",
    "                gae_loss = nn.MSELoss()( targets, values )\n",
    "\n",
    "                kl_per_sample = torch.sum(kl_divs, dim=1) / (torch.sum(masks, dim=1) + 1e-8)\n",
    "                kl_div_loss = torch.mean(kl_per_sample)\n",
    "                if config.use_adaptive_KL:\n",
    "                    if n_batch % 100 == 0:\n",
    "                        if kl_div_loss < config.target_kl / config.buffer_kl:\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                            print(f\"Update beta (low KL): {config.kl_coef} -> {config.kl_coef / 2.0}\")\n",
    "                            config.kl_coef = max(config.kl_coef / 2.0, config.beta_min) # 下限 0.05\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                        elif kl_div_loss > config.target_kl * config.buffer_kl:\n",
    "                            print(f\"kl_coef:\", config.kl_coef )\n",
    "                            print(f\"Update beta (high KL): {config.kl_coef} -> {config.kl_coef * 2.0}\")\n",
    "                            config.kl_coef = min(config.kl_coef * 2.0, config.beta_max)  # 上限 5.0\n",
    "                            print(f\"bklcoef:\", config.kl_coef )\n",
    "                \n",
    "                loss =  config.policy_coef * policy_loss \\\n",
    "                    + config.ent_coef * entropy_loss + config.gae_coef * gae_loss + config.kl_coef * kl_div_loss\n",
    "                if config.crf_coef != 0.0:\n",
    "                    loss =  loss + config.crf_coef * crf_loss\n",
    "                if config.ce_coef != 0.0:\n",
    "                    if config.use_ce_bert:\n",
    "                        ce_loss = nn.CrossEntropyLoss()( bert_logits.transpose(1,2), captions )\n",
    "                    else:\n",
    "                        ce_loss = nn.NLLLoss()( log_ce_tensor.view( bsz * seq_len, -1 ), captions.view( bsz * seq_len ) )\n",
    "                    loss =  loss + config.ce_coef * ce_loss\n",
    "               \n",
    "            with torch.no_grad():\n",
    "                last_sample_log_probs = sample_log_probs.detach()\n",
    "                \n",
    "            scaler.scale(loss).backward()\n",
    "            loss_item = loss.item()\n",
    "            reward_item = rewards.mean().item()\n",
    "            reward2_item = rewards2.mean().item()\n",
    "            adv_item = advantages.mean().item()\n",
    "            if config.display_include_coef:\n",
    "                policy_item = config.policy_coef * policy_loss.item()\n",
    "                entropy_item = config.ent_coef * entropy_loss.item()\n",
    "                critic_item = config.gae_coef * gae_loss.item()\n",
    "                #critic_item = 0\n",
    "                kl_div_item = config.kl_coef * kl_div_loss.item()\n",
    "                ord_item = config.ord_coef * reward_ord.mean().item()\n",
    "                repeat_item = config.rep_coef * reward_repeat.mean().item()\n",
    "                length_item = config.len_coef * reward_length.mean().item()\n",
    "                if config.crf_coef != 0.0:\n",
    "                    crf_item = config.crf_coef * crf_loss.mean().item()\n",
    "                else:\n",
    "                    crf_item = 0.0\n",
    "                if config.ce_coef != 0.0:\n",
    "                    ce_item = config.ce_coef * ce_loss.mean().item()\n",
    "                else:\n",
    "                    ce_item = 0.0\n",
    "                unr_item = config.unr_coef * reward_unr.mean().item()\n",
    "            else:\n",
    "                policy_item = policy_loss.item()\n",
    "                entropy_item = entropy_loss.item()\n",
    "                critic_item = gae_loss.item()\n",
    "                #critic_item = 0\n",
    "                kl_div_item = kl_div_loss.item()\n",
    "                ord_item = reward_ord.mean().item()\n",
    "                repeat_item = reward_repeat.mean().item()\n",
    "                length_item = reward_length.mean().item()\n",
    "                if config.crf_coef != 0.0:\n",
    "                    crf_item = crf_loss.mean().item()\n",
    "                else:\n",
    "                    crf_item = 0.0\n",
    "                if config.ce_coef != 0.0:\n",
    "                    ce_item = ce_loss.mean().item()\n",
    "                else:\n",
    "                    ce_item = 0.0\n",
    "                unr_item = reward_unr.mean().item()\n",
    "            del loss, rewards, reward_ord, reward_repeat, reward_length\n",
    "            del policy_loss, policy_loss_1, policy_loss_2, entropy_loss, kl_div_loss\n",
    "            del sample_log_probs, advantages, ratio\n",
    "            del finalized_scores, finalized_tokens\n",
    "            del kl_divs, masks, kl_per_sample\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            scaler.unscale_(optimizer)\n",
    "            if config.clip_grad_threshold != 0.0:\n",
    "                torch.nn.utils.clip_grad_norm_(\\\n",
    "                   model.parameters(),\n",
    "                   config.clip_grad_threshold)\n",
    "            else:\n",
    "                custom_gradient_clipping(\n",
    "                    #params_clip, \n",
    "                    params_con, \n",
    "                    params_bert,\n",
    "                    params_cri,\n",
    "                    params_others,\n",
    "                    #thresh_groups['clip'], \n",
    "                    thresh_groups['con'], \n",
    "                    thresh_groups['bert'], \n",
    "                    thresh_groups['cri'], \n",
    "                    thresh_groups['others'], \n",
    "                )\n",
    "                      \n",
    "            # オプティマイザにより，パラメータを更新する\n",
    "            #for i, ( name, param ) in enumerate(model.named_parameters()):\n",
    "            #    #params.grad = grad[i]\n",
    "            #    print( \"parameter name:\", name )\n",
    "            #norm0 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[0].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            #norm1 = torch.sqrt( torch.norm( model.clip_model.vision_model.encoder.layers[12].self_attn.q_proj.weight.grad, p = 2 ) ).item()\n",
    "            norm0 = torch.norm( model.ln_critical.weight.grad, p = 2 ).item()\n",
    "            norm1 = torch.norm( model.linear_critical.weight.grad, p = 2 ).item()\n",
    "            norm2 = torch.norm( model.bert.encoder.layer[0].attention.self.query.weight.grad, p = 2 ).item()\n",
    "            norm3 = torch.norm( model.bert.encoder.layer[23].attention.self.query.weight.grad, p = 2 ).item()\n",
    "            mean_norm = torch.mean( torch.stack ([  torch.norm( param.grad, p = 2 ) \\\n",
    "                                                  for param in model.parameters() if param.grad is not None ] ) ).item()\n",
    "            #total_norm = torch.nn.utils.clip_grad_norm_(params_bert, thresh_groups['bert']).item()\n",
    "            #print( norm0, norm1, norm2, norm3, norm_mean )\n",
    "            with open(norm_file, 'a') as f:\n",
    "                print( \"epcoch:\", epoch, \", step:\", global_step, \", norm0:\", norm0, \", norm1:\", norm1, \", norm2:\", norm2, \\\n",
    "                       \", norm3:\", norm3, \", mean_norm:\", mean_norm, file=f  )\n",
    "                f.flush()\n",
    "        \n",
    "            #optimizer.step()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()            \n",
    "            scheduler.step()\n",
    "            #end_time = time.time()\n",
    "            #print( \"step time:\", end_time - start_time )\n",
    "            \n",
    "            if global_step % file_param == 0:                \n",
    "                #start_time = time.time()\n",
    "                hypo_sentence1 = []\n",
    "                ref_sentence1 = []\n",
    "                if config.metric == 'cider' or config.metric == 'special':\n",
    "                    if config.decode_t == 'no-endoftext':\n",
    "                        preds_str = [tokenizer.decode(\n",
    "                            [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                            ) for pred in hypo_ids]\n",
    "                        samps_str = [tokenizer.decode(\n",
    "                            [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                            ) for pred in preds]\n",
    "                        targets_str = [tokenizer.decode(\n",
    "                            [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                            ) for target in captions]\n",
    "                    elif config.decode_t == 'no-pad':\n",
    "                        preds_str = [\n",
    "                            tokenizer.decode([i for i in pred if i != eos_token_id \\\n",
    "                            and i != pad_token_id ] ) \n",
    "                            for pred in hypo_ids\n",
    "                        ]\n",
    "                        preds_str2 = [\n",
    "                            tokenizer.decode([i for i in pred if i != eos_token_id], skip_special_tokens=True) \n",
    "                            for pred in hypo_ids\n",
    "                        ]\n",
    "                        samps_str = [tokenizer.decode(\n",
    "                            [ i for i in pred \\\n",
    "                             if i != pad_token_id \\\n",
    "                             and i != eos_token_id ] \\\n",
    "                            ) for pred in preds]\n",
    "                        targets_str = [tokenizer.decode(\n",
    "                            [ i for i in target \\\n",
    "                            if i != pad_token_id \\\n",
    "                             and i != eos_token_id ]       \n",
    "                            ) for target in captions]\n",
    "                    else:\n",
    "                        preds_str = [tokenizer.decode(pred) for pred in hypo_ids]\n",
    "                        targets_str = [tokenizer.decode(target) for target in captions]\n",
    "                    pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "                    target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "                    avg_bleu, scores = compute_reward.scorer.compute_score(target_dict, pred_dict) # cider の計算\n",
    "                    #avg_error = compute_reward.meteor.compute(predictions=preds_str, references=targets_str)['meteor']\n",
    "                    #avg_error = bleu_score.corpus_bleu( targets_str, preds_str, smoothing_function=fn  )\n",
    "                    rouge_scores = [compute_reward.rougeL.score(target, pred)['rougeL'][0] for pred, target in zip(preds_str, targets_str)]\n",
    "                    avg_error = sum( rouge_scores ) / len( rouge_scores )\n",
    "                    #clip_scores = [compute_reward.metric( img2, pred).detach() for img2, pred in zip( imgs2, preds_str2 )]\n",
    "                    with autocast(str(config.device),enabled=config.use_amp):\n",
    "                        with torch.no_grad():\n",
    "                            clip_score = compute_reward.metric( imgs2, preds_str2 ) / 100.0\n",
    "                    bert_scores = compute_reward.bert.compute(predictions=preds_str, references=targets_str, model_type=model_name, \\\n",
    "                                                              lang='en',  device=config.device)['f1']\n",
    "                    bert_scores = [score for score in bert_scores]\n",
    "                    bert_score = sum( bert_scores ) / len( bert_scores )\n",
    "                    if  global_step % train_param == 0:\n",
    "                        hypo_sentence1 = [preds_str[0]]\n",
    "                        samp_sentence1 = [samps_str[0]]\n",
    "                        ref_sentence1 = [targets_str[0]]\n",
    "\n",
    "\n",
    "                #if config.display_include_coef:                \n",
    "                #    avg_bleu = config.cider_coef *  avg_bleu\n",
    "                #    avg_error = config.rouge_coef * avg_error\n",
    "                #    clip_score = config.clip_coef * clip_score\n",
    "                #    bert_score = config.bert_coef * bert_score\n",
    "    \n",
    "                            \n",
    "                # 学習時の損失をログに書き込み\n",
    "                #エポック内の平均\n",
    "                train_losses.append(loss_item)\n",
    "                train_policys.append(policy_item)\n",
    "                train_entropies.append(entropy_item)\n",
    "                train_critics.append(critic_item)\n",
    "                train_kl_divs.append(kl_div_item)\n",
    "                train_rewards.append(reward_item)\n",
    "                train_rewards2.append(reward2_item)\n",
    "                train_ord.append(ord_item)\n",
    "                train_repeat.append(repeat_item)\n",
    "                train_length.append(length_item)\n",
    "                train_adv.append(adv_item)\n",
    "                train_errors.append( avg_error )\n",
    "                train_bleus.append( avg_bleu )\n",
    "                train_crfs.append( crf_item )\n",
    "                train_ces.append( ce_item )\n",
    "                train_clips.append( clip_score )\n",
    "                train_unrs.append( unr_item )\n",
    "                train_berts.append( bert_score )\n",
    "                if len(train_losses) > config.moving_avg:\n",
    "                    train_losses.popleft()\n",
    "                    train_policys.popleft()\n",
    "                    train_entropies.popleft()\n",
    "                    train_critics.popleft()\n",
    "                    train_kl_divs.popleft()\n",
    "                    train_rewards.popleft()\n",
    "                    train_rewards2.popleft()\n",
    "                    train_ord.popleft()\n",
    "                    train_repeat.popleft()\n",
    "                    train_length.popleft()\n",
    "                    train_adv.popleft()\n",
    "                    train_errors.popleft()\n",
    "                    train_bleus.popleft()\n",
    "                    train_crfs.popleft()\n",
    "                    train_ces.popleft()\n",
    "                    train_clips.popleft()\n",
    "                    train_unrs.popleft()\n",
    "                    train_berts.popleft()\n",
    "                mean_loss = torch.Tensor(train_losses).mean().item()\n",
    "                mean_policy = torch.Tensor(train_policys).mean().item()\n",
    "                mean_entropy = torch.Tensor(train_entropies).mean().item()\n",
    "                mean_critic = torch.Tensor(train_critics).mean().item()\n",
    "                mean_kl_div = torch.Tensor(train_kl_divs).mean().item()\n",
    "                mean_reward = torch.Tensor(train_rewards).mean().item()\n",
    "                mean_reward2 = torch.Tensor(train_rewards2).mean().item()\n",
    "                mean_ord = torch.Tensor(train_ord).mean().item()\n",
    "                mean_repeat = torch.Tensor(train_repeat).mean().item()\n",
    "                mean_length = torch.Tensor(train_length).mean().item()\n",
    "                mean_adv = torch.Tensor(train_adv).mean().item()\n",
    "                mean_error = torch.Tensor(train_errors).mean().item()\n",
    "                mean_bleu = torch.Tensor(train_bleus).mean().item()\n",
    "                mean_crf = torch.Tensor(train_crfs).mean().item()\n",
    "                mean_ce = torch.Tensor(train_ces).mean().item()\n",
    "                mean_clip = torch.Tensor(train_clips).mean().item()\n",
    "                mean_unr = torch.Tensor(train_unrs).mean().item()\n",
    "                mean_bert = torch.Tensor(train_berts).mean().item()\n",
    "                #print( \"mean_reward2:\", mean_reward2 ) \n",
    "                pbar.set_postfix({\n",
    "                    'loss': mean_loss,\n",
    "                    'policy': mean_policy,\n",
    "                    'entropy': mean_entropy,\n",
    "                    'gae': mean_critic,\n",
    "                    'kl_div': mean_kl_div,\n",
    "                    'reward': mean_reward,\n",
    "                    'reward2': mean_reward2,\n",
    "                    'ord': mean_ord,\n",
    "                    'repeat': mean_repeat,\n",
    "                    'length': mean_length,\n",
    "                    'adv': mean_adv,\n",
    "                    'rougeL': mean_error,\n",
    "                    'cider': mean_bleu,\n",
    "                    'crf': mean_crf,\n",
    "                    'ce': mean_ce,\n",
    "                    'clip': mean_clip,\n",
    "                    'unr': mean_unr,\n",
    "                    'bert': mean_bert,\n",
    "                })\n",
    "                with open(train_loss_file, 'a') as f:\n",
    "                    print(f' {global_step}, {mean_loss}, {mean_policy}, {mean_entropy}, {mean_critic}, {mean_kl_div}, {mean_reward}, ' \\\n",
    "                          f'{mean_ord}, {mean_repeat}, {mean_length}, {mean_adv}, {mean_error}, {mean_bleu}, {mean_crf}, {mean_ce}, '\\\n",
    "                          f'{mean_clip}, {mean_unr}, {mean_bert}', file=f)\n",
    "                print_flag = 1\n",
    "                for ( hypo_se, ref_se, samp_se ) in zip( hypo_sentence1, ref_sentence1, samp_sentence1 ):\n",
    "                    if print_flag == 1:\n",
    "                        print( \"lr con   :\", optimizer.param_groups[0][\"lr\"] )\n",
    "                        print( \"lr bert  :\", optimizer.param_groups[1][\"lr\"] )\n",
    "                        print( \"lr cri   :\", optimizer.param_groups[2][\"lr\"] )\n",
    "                        print( \"lr others:\", optimizer.param_groups[3][\"lr\"] )\n",
    "                        print_flag = 0\n",
    "                    print(f'Train epoch = {global_step/len_tr_loader}, loss = {mean_loss}, policy = {mean_policy}, '\\\n",
    "                          f'entropy_loss = {mean_entropy}, gae = {mean_critic}, kl_div = {mean_kl_div}, reward = {mean_reward}, '\\\n",
    "                          f'ord = {mean_ord}, repeat = {mean_repeat}, length = {mean_length}, adv = {mean_adv}, '\\\n",
    "                          f'rougeL = {mean_error}, cider = {mean_bleu}, clip = {mean_clip}, crf = {mean_crf}, ce = {mean_ce}, '\\\n",
    "                          f'unr = {mean_unr}, ber = {mean_bert}' )\n",
    "                    print( \"refe:\", ref_se )\n",
    "                    print( \"hypo:\", hypo_se )\n",
    "                    print( \"samp:\", samp_se )\n",
    "                #end_time = time.time()\n",
    "                        #print( \"file time:\", end_time - start_time )\n",
    "\n",
    "            global_step += 1\n",
    "    #各値を表示\n",
    "    print(f'Train loss: {mean_loss}')\n",
    "    print(f'Train policy: {mean_policy}')\n",
    "    print(f'Train entropy: {mean_entropy}')\n",
    "    print(f'Train gae: {mean_critic}')\n",
    "    print(f'Train kl_div: {mean_kl_div}')\n",
    "    print(f'Train reward: {mean_reward}')\n",
    "    print(f'Train reward2: {mean_reward2}')\n",
    "    print(f'Train ord: {mean_ord}')\n",
    "    print(f'Train repeat: {mean_repeat}')\n",
    "    print(f'Train pad: {mean_length}')\n",
    "    print(f'Train adv: {mean_adv}')\n",
    "    print(f'Train clip: {mean_clip}')\n",
    "    print(f'Train rougeL: {mean_error}')        \n",
    "    print(f'Train cider: {mean_bleu}')\n",
    "    print(f'Train crf: {mean_crf}')        \n",
    "    print(f'Train ce: {mean_ce}')\n",
    "    print(f'Train unr: {mean_unr}')        \n",
    "    print(f'Train bert: {mean_bert}')\n",
    "    \n",
    "    \n",
    "    # 検証\n",
    "    with tqdm(val_loader) as pbar:\n",
    "        pbar.set_description(f'[検証]')\n",
    "\n",
    "        # 評価モード\n",
    "        model.eval()\n",
    "\n",
    "        #val_losses = deque()\n",
    "        #val_rewards = deque()\n",
    "        val_errors = deque()\n",
    "        val_bleus = deque()\n",
    "        val_clips = deque()\n",
    "        val_berts = deque()\n",
    "        for n_batch, (imgs, imgs2, captions, caption_lengths) in enumerate( pbar ):\n",
    "\n",
    "            # ミニバッチを設定\n",
    "            imgs = imgs.to(config.device)\n",
    "            imgs2 = imgs2.to(config.device)\n",
    "            captions = captions.to(config.device)\n",
    "                \n",
    "            with torch.no_grad():\n",
    "                finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "                critical_value, crf_loss, bert_logits, sampled_beam_idx  = \\\n",
    "                model( imgs, captions, top_indices = None )\n",
    "                hypo_ids = finalized_tokens\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "               \n",
    "            n = 0\n",
    "            hypo_sentence = []\n",
    "            ref_sentence = []\n",
    "            hypo_sentence1 = []\n",
    "            ref_sentence1 = []\n",
    "            total_error = 0\n",
    "            total_token_length = 0\n",
    "            total_bleu = 0\n",
    "            n2 = 0\n",
    "            if config.metric == 'cider' or config.metric == 'special':\n",
    "                if config.decode_t == 'no-endoftext':\n",
    "                    preds_str = [tokenizer.decode(\n",
    "                        [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                        ) for pred in hypo_ids]\n",
    "                    samps_str = [tokenizer.decode(\n",
    "                        [pred[i] for i in range( 1, len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                        ) for pred in preds]\n",
    "                    targets_str = [tokenizer.decode(\n",
    "                        [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                        ) for target in captions]\n",
    "                elif config.decode_t == 'no-pad':\n",
    "                    preds_str = [\n",
    "                        tokenizer.decode([i for i in pred if i != eos_token_id \\\n",
    "                        and i != pad_token_id ] ) \n",
    "                        for pred in hypo_ids\n",
    "                    ]\n",
    "                    preds_str2 = [\n",
    "                        tokenizer.decode([i for i in pred if i != eos_token_id], skip_special_tokens=True) \n",
    "                        for pred in hypo_ids\n",
    "                    ]\n",
    "                    samps_str = [tokenizer.decode(\n",
    "                        [ i for i in pred \\\n",
    "                         if i != pad_token_id \\\n",
    "                         and i != eos_token_id ] \\\n",
    "                        ) for pred in preds]\n",
    "                    targets_str = [tokenizer.decode(\n",
    "                        [ i for i in target \\\n",
    "                        if i != pad_token_id \\\n",
    "                         and i != eos_token_id ]       \n",
    "                        ) for target in captions]\n",
    "                else:\n",
    "                    preds_str = [tokenizer.decode(pred) for pred in hypo_ids]\n",
    "                    targets_str = [tokenizer.decode(target) for target in captions]\n",
    "                pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "                target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "                avg_bleu, scores = compute_reward.scorer.compute_score(target_dict, pred_dict) # cider の計算\n",
    "                #avg_error = compute_reward.meteor.compute(predictions=preds_str, references=targets_str)['meteor']\n",
    "                #avg_error = bleu_score.corpus_bleu( targets_str, preds_str, smoothing_function=fn  )\n",
    "                rouge_scores = [compute_reward.rougeL.score(target, pred)['rougeL'][0] for pred, target in zip(preds_str, targets_str)]\n",
    "                avg_error = sum( rouge_scores ) / len( rouge_scores )\n",
    "                #clip_scores = [compute_reward.metric( img2, pred).detach() for img2, pred in zip( imgs2, preds_str2 )]\n",
    "                with autocast(str(config.device),enabled=config.use_amp):\n",
    "                    with torch.no_grad():\n",
    "                        clip_score = compute_reward.metric( imgs2, preds_str2 ) / 100.0\n",
    "                        bert_scores = compute_reward.bert.compute(predictions=preds_str, references=targets_str, model_type=model_name, \\\n",
    "                                                          lang='en',  device=config.device)['f1']\n",
    "                        bert_score = sum( bert_scores ) / len( bert_scores )\n",
    "                if n_batch % val_param == 0:\n",
    "                    hypo_sentence1 = [preds_str[0]]\n",
    "                    samp_sentence1 = [samps_str[0]]\n",
    "                    ref_sentence1 = [targets_str[0]]\n",
    "\n",
    "\n",
    "            #if config.display_include_coef:                \n",
    "            #    avg_bleu = config.cider_coef *  avg_bleu\n",
    "            #    avg_error = config.rouge_coef * avg_error\n",
    "            #    clip_score = config.clip_coef * clip_score\n",
    "            #    bert_score = config.bert_coef * bert_score\n",
    "\n",
    "            val_errors.append( avg_error )\n",
    "            val_bleus.append( avg_bleu )\n",
    "            val_clips.append( clip_score )\n",
    "            val_berts.append( bert_score )\n",
    "            if len(val_errors) > config.moving_avg:\n",
    "                #val_losses.popleft()\n",
    "                #val_rewards.popleft()\n",
    "                val_errors.popleft()\n",
    "                val_bleus.popleft()\n",
    "                val_clips.popleft()\n",
    "                val_berts.popleft()\n",
    "             #mean_loss = torch.Tensor(val_losses).mean().item()\n",
    "            #mean_reward = torch.Tensor(val_rewards).mean().item()\n",
    "            mean_error = torch.Tensor(val_errors).mean().item()\n",
    "            mean_bleu = torch.Tensor(val_bleus).mean().item()\n",
    "            mean_clip = torch.Tensor(val_clips).mean().item()\n",
    "            mean_bert = torch.Tensor(val_berts).mean().item()\n",
    "            pbar.set_postfix({\n",
    "                #'loss': mean_loss,\n",
    "                #'reward': mean_reward,\n",
    "                'rougeL': mean_error,\n",
    "                'CIDER': mean_bleu,\n",
    "                'clip': mean_clip,\n",
    "                'bert': mean_bert,\n",
    "            })\n",
    "            # Validation Lossをログに書き込み\n",
    "            with open(val_loss_file, 'a') as f:\n",
    "                print(f'{epoch}, {mean_error}, {mean_bleu}, {mean_clip}, {mean_bert}', file=f)\n",
    "            \n",
    "            for ( hypo_se, ref_se ) in zip( hypo_sentence1, ref_sentence1 ):\n",
    "                print(f'Val epoch = {epoch}, rougeL = {mean_error}, cider = {mean_bleu}, clip = {mean_clip}, bert = {mean_bert}')\n",
    "                print( \"refe:\", ref_se )\n",
    "                print( \"hypo:\", hypo_se )\n",
    "   \n",
    "    # Loss 表示\n",
    "    #print(f'Validation loss: {val_loss}')\n",
    "    #print(f'Validation loss: {val_reward}')\n",
    "    print(f'Validation rougeL: {mean_error}')\n",
    "    print(f'Validation cider: {mean_bleu}')\n",
    "    print(f'Validation clip: {mean_clip}')\n",
    "    print(f'Validation bert: {mean_bert}')\n",
    "    \n",
    "    ## より良い検証結果が得られた場合、モデルを保存\n",
    "            \n",
    "    # モデルを保存\n",
    "    torch.save({'epoch': epoch,\n",
    "                'global_step': global_step,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'scheduler_state_dict': scheduler.state_dict(),\n",
    "        #'loss': loss,\n",
    "        },\n",
    "        f'{config.save_directory}/model_rl_ppo_critic_crf32_52_curr.pth')\n",
    "    ## モデルを保存\n",
    "## モデルを保存\n",
    "#torch.save({'epoch': epoch,\n",
    "#    'global_step': global_step,\n",
    "#    'model_state_dict': model.state_dict(),\n",
    "#    'optimizer_state_dict': optimizer.state_dict(),\n",
    "#    'scheduler_state_dict': scheduler.state_dict(),\n",
    "#    #'loss': loss,\n",
    "#    },\n",
    "#    f'{config.save_directory}/model_rl_ppo_critic_crf32_final.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデルを保存\n",
    "torch.save({'epoch': epoch,\n",
    "    'global_step': global_step,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'scheduler_state_dict': scheduler.state_dict(),\n",
    "    #'loss': loss,\n",
    "    },\n",
    "    f'{config.save_directory}/model_rl_ppo_critic_crf_ontheway.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10ac0991cf0d4346a2dd950c4e69de74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc134416b78b4e16ab143f7b3c9b4db1",
      "placeholder": "​",
      "style": "IPY_MODEL_3e3ad9c31b8244d9987cb5d09ac80909",
      "value": " 230M/230M [00:03&lt;00:00, 57.2MB/s]"
     }
    },
    "189ddc1a5e674235a5a4c9461ec37bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89ed549065394bad9c015c612b04c1d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f9994c1870c348b0ab983404decb722e",
      "value": "100%"
     }
    },
    "2030796b20a24135908446e8a05b2447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e3ad9c31b8244d9987cb5d09ac80909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741ba7eec1f94e6889152972dbf220c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89ed549065394bad9c015c612b04c1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8d4c88aa714714b864351862297512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_741ba7eec1f94e6889152972dbf220c0",
      "max": 241669177,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be3b477f459a4fdd80b055ef88cf8b52",
      "value": 241669177
     }
    },
    "be3b477f459a4fdd80b055ef88cf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc134416b78b4e16ab143f7b3c9b4db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1785c740b7449584ceec0225c64c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_189ddc1a5e674235a5a4c9461ec37bb7",
       "IPY_MODEL_af8d4c88aa714714b864351862297512",
       "IPY_MODEL_10ac0991cf0d4346a2dd950c4e69de74"
      ],
      "layout": "IPY_MODEL_2030796b20a24135908446e8a05b2447"
     }
    },
    "f9994c1870c348b0ab983404decb722e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
