{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fl7yR7P3CTTj"
   },
   "source": [
    "### ライブラリの準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A7IQqCE7mCeD"
   },
   "source": [
    "###モジュールのインポートとGoogleドライブのマウント"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WLEMbaPJOs9v",
    "outputId": "f47ad633-eb58-49cd-8893-914d53aa4475"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch.autograd\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import datetime\n",
    "#from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "import pickle\n",
    "import copy\n",
    "import gc\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from PIL import Image\n",
    "import skimage.transform\n",
    "from collections import deque\n",
    "from typing import Sequence, Dict, Tuple, Union, Optional\n",
    "#from argparse import Namespace\n",
    "from dataclasses import dataclass, field\n",
    "from sacrebleu.metrics import BLEU\n",
    "from evaluate import load\n",
    "import jiwer\n",
    "from comet import download_model, load_from_checkpoint\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torchvision import models\n",
    "import torchvision.transforms as T\n",
    "import torchvision.datasets as dataset\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "#from timm.scheduler import CosineLRScheduler\n",
    "from transformers import  get_linear_schedule_with_warmup\n",
    "\n",
    "#from transformers import AutoImageProcessor, AutoModel, AutoProcessor, CLIPVisionModel\n",
    "#from transformers import AutoTokenizer, CLIPVisionModel, AutoModelForCausalLM\n",
    "from transformers import BertTokenizer, BertModel, CLIPVisionModel, BertForPreTraining\n",
    "\n",
    "import sys\n",
    "from evaluate import load\n",
    "\n",
    "import util\n",
    "import levenshtein\n",
    "from nltk import bleu_score\n",
    "from torchmetrics.multimodal import CLIPScore\n",
    "import ssl\n",
    "from torch.amp import autocast, GradScaler\n",
    "from collections import OrderedDict\n",
    "from rouge_score import rouge_scorer\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import multiprocessing\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pycocoevalcap.cider.cider import Cider\n",
    "import json\n",
    "import collections\n",
    "from collections import Counter\n",
    "import plotly\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from optuna.storages import JournalStorage\n",
    "from optuna.storages.journal import JournalFileBackend\n",
    "fn = bleu_score.SmoothingFunction().method7\n",
    "import time\n",
    "from pycocoevalcap.spice.spice import Spice\n",
    "#logging.getLogger('rouge_score.rouge_scorer').setLevel(logging.WARNING)\n",
    "#logging.set_verbosity_warning()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    '''\n",
    "    位置埋め込み （Positional embedding）\n",
    "    dim_embedding: 埋込み次元\n",
    "    max_len      : 入力の最大系列長\n",
    "    '''\n",
    "    def __init__(self, dim_embedding: int, max_len: int=2048):\n",
    "        super().__init__()\n",
    "\n",
    "        self.pos_emb = nn.Embedding(max_len, dim_embedding)\n",
    "\n",
    "    '''\n",
    "    位置エンコーディングの順伝播\n",
    "    x: 位置エンコーディングを埋め込む対象のテンソル,\n",
    "       [バッチサイズ, 系列長, 埋め込み次元]\n",
    "    '''\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        seq = x.shape[1]\n",
    "        positions = torch.arange(start=0, end=seq, step=1, device=x.device).to(torch.long)\n",
    "        positions = self.pos_emb(positions)[:seq,:]\n",
    "        \n",
    "        return positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "a_token_id = tokenizer.encode( [ \"a\" ] )[1]\n",
    "the_token_id = tokenizer.encode( [ \"the\" ] )[1]\n",
    "and_token_id = tokenizer.encode( [ \"and\" ] )[1]\n",
    "in_token_id = tokenizer.encode( [ \"in\" ] )[1]\n",
    "we_token_id = tokenizer.encode( [ \"we\" ] )[1]\n",
    "i_token_id = tokenizer.encode( [ \"i\" ] )[1]\n",
    "he_token_id = tokenizer.encode( [ \"he\" ] )[1]\n",
    "she_token_id = tokenizer.encode( [ \"she\" ] )[1]\n",
    "it_token_id = tokenizer.encode( [ \"it\" ] )[1]\n",
    "they_token_id = tokenizer.encode( [ \"they\" ] )[1]\n",
    "period_token_id = tokenizer.encode( [ \".\" ] )[1]\n",
    "comma_token_id = tokenizer.encode( [ \",\" ] )[1]\n",
    "dbl_token_id = tokenizer.encode( [ '\"' ] )[1]\n",
    "sgl_token_id = tokenizer.encode( [ \"'\" ] )[1]\n",
    "\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "class ComputeReward(nn.Module):\n",
    "    def __init__(self, device, reward_t = 'ordinary', decode_t = 'ordinary', sentence_level_metric=\"bleu\", \n",
    "                 repeat_thresh = [4,2,2,2], repeat_weight = [0.5, 1, 1, 2], cider_coef = 1.0, rouge_coef = 1.0, clip_coef = 2.0, \n",
    "                 bert_coef = 1.0, use_amp = True ):\n",
    "        super().__init__()\n",
    "        self.metric = sentence_level_metric\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tgt_lang = \"en\"\n",
    "        self.device = device\n",
    "\n",
    "        if sentence_level_metric =='special':\n",
    "            #self.bleu = BLEU(effective_order=\"True\")\n",
    "            self.scorer = Cider()\n",
    "            #self.meteor = load('meteor')\n",
    "            self.rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "            self.bert = load('bertscore')\n",
    "            self.metric = CLIPScore(model_name_or_path=\"openai/clip-vit-base-patch32\").to(self.device)\n",
    "            for param in self.metric.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.spider = Spice()\n",
    "        elif sentence_level_metric == 'bleu':\n",
    "            self.bleu = BLEU(effective_order=\"True\")\n",
    "        elif sentence_level_metric == 'meteor':\n",
    "            self.meteor = load('meteor')\n",
    "        elif sentence_level_metric == 'rouge':\n",
    "            self.rougeL = rouge_scorer.RougeScorer(['rouge1', 'rougeL'], use_stemmer=True)\n",
    "        elif sentence_level_metric == 'cider':\n",
    "            self.scorer = Cider() \n",
    "        elif sentence_level_metric == 'ter':\n",
    "            #self.ter = load('ter')\n",
    "            pass\n",
    "        elif sentence_level_metric == 'bert':\n",
    "            #self.bert = load('bertscore')\n",
    "            pass\n",
    "        elif sentence_level_metric == 'bleurt':\n",
    "            #self.bleurt = load('bleurt', module_type='metric', checkpoint='bleurt-large-128')\n",
    "            pass\n",
    "        elif self.metric == \"comet\":\n",
    "            model_path = download_model(\"Unbabel/wmt22-comet-da\")\n",
    "            self.comet = load_from_checkpoint(model_path)\n",
    "        self.reward_t = reward_t\n",
    "        self.repeat_thresh = repeat_thresh\n",
    "        self.repeat_weight = repeat_weight\n",
    "        self.decode_t = decode_t\n",
    "        self.cider_coef = cider_coef\n",
    "        self.rouge_coef = rouge_coef\n",
    "        self.clip_coef = clip_coef\n",
    "        self.bert_coef = bert_coef\n",
    "        self.use_amp = use_amp\n",
    "    \n",
    "    def _compute_reward_ord(self, preds, targets, imgs2, sources=None):\n",
    "        \"\"\"\n",
    "        Compute reward metric for a batch of prediction and target sentences\n",
    "        \"\"\"\n",
    "        model_name = \"distilbert-base-uncased\"\n",
    "        # detokenize (convert to str) preds & targets\n",
    "        if self.decode_t == 'no-endoftext':\n",
    "            preds_str = [self.tokenizer.decode(\n",
    "                [pred[i] for i in range( 1,  len( pred )  ) if not (pred[i-1] == endoftext_token_id and pred[i] == endoftext_token_id) ]\n",
    "                ) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(\n",
    "                [target[i] for i in range( 1,  len( target )  ) if not (target[i-1] == endoftext_token_id and target[i] == endoftext_token_id) ]\n",
    "                ) for target in targets]\n",
    "        elif self.decode_t == 'no-pad':\n",
    "            preds_str = [self.tokenizer.decode(\n",
    "                [ i for i in pred \\\n",
    "                 if i != pad_token_id \\\n",
    "                 and i != eos_token_id ] \\\n",
    "                ) for pred in preds]\n",
    "            preds_str2 = [self.tokenizer.decode(\n",
    "                [ i for i in pred \\\n",
    "                 if  i != eos_token_id  ] \\\n",
    "                 , skip_special_tokens = True ) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(\n",
    "                [ i for i in target \\\n",
    "                if i != pad_token_id \\\n",
    "                 and i != eos_token_id ]       \n",
    "                ) for target in targets]\n",
    "        else:\n",
    "            preds_str = [self.tokenizer.decode(pred) for pred in preds]\n",
    "            targets_str = [self.tokenizer.decode(target) for target in targets]\n",
    "        sources_str = [self.tokenizer.decode(source, ref=\"src\") for source in sources] if sources is not None else None\n",
    "        #print( \"preds size:\", preds.size() )\n",
    "        #print( \"targets size:\", targets.size() )\n",
    "        \n",
    "        #print(f'1st target sent: {targets_str[0]}')\n",
    "        #print(f'1st pred sent: {preds_str[0]}')\n",
    "\n",
    "        # compute reward metric\n",
    "        seq_len = preds.shape[1]\n",
    "\n",
    "        if self.metric == 'special':\n",
    "            #reward_bleu = [[self.bleu.sentence_score(pred, [target]).score] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward_bleu = [[bleu_score.sentence_bleu( target, pred, smoothing_function=fn)] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward_bleu = torch.tensor(reward_bleu).to(self.device) / 100.0\n",
    "            #wer_scores = [jiwer.wer(target, pred) for pred, target in zip(targets_str, preds_str)]\n",
    "            #reward = [[score] * seq_len for score in wer_scores]\n",
    "            #reward_wer = - torch.tensor( reward ).to(self.device)\n",
    "            #start_time = time.time()\n",
    "            pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "            target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "            score, scores = self.scorer.compute_score(target_dict, pred_dict)\n",
    "            reward_cider = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            #end_time = time.time()\n",
    "            #print( \"cider time:\", end_time - start_time )\n",
    "            #start_time = time.time()\n",
    "            reward_rouge = [[self.rougeL.score(target, pred)['rougeL'][0]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            reward_rouge = torch.tensor( reward_rouge ).to( self.device )\n",
    "            #end_time = time.time()\n",
    "            #print( \"rouge time:\", end_time - start_time )\n",
    "            #start_time = time.time()\n",
    "            with autocast(str(self.device),enabled=self.use_amp):\n",
    "                with torch.no_grad():\n",
    "                    #clip_scores = [[self.metric( img2, pred).detach()] * seq_len for img2, pred in zip( imgs2, preds_str2 )]\n",
    "                    #tmp = self.metric( imgs2, preds_str2 )\n",
    "                    #print( \"tmp size:\", tmp.size() )\n",
    "                    #clip_scores = [[self.metric( imgs2, preds_str2 ).detach()] * seq_len ]\n",
    "                    processed = self.metric.processor(text=preds_str2, images=imgs2, return_tensors=\"pt\", padding=True, \\\n",
    "                                                      truncation=True, max_length=77 ).to(self.device)\n",
    "                    outputs = self.metric.model(**processed)\n",
    "                    # 特徴量の正規化\n",
    "                    image_features = outputs.image_embeds / outputs.image_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                    text_features = outputs.text_embeds / outputs.text_embeds.norm(p=2, dim=-1, keepdim=True)\n",
    "                    # 3. コサイン類似度を一括計算 (100倍してマイナスをカットするのが CLIPScore の定義)\n",
    "                    # 各画像ペアの個別スコア (Batch Size,) が得られる\n",
    "                    individual_scores = torch.clamp( (image_features * text_features).sum(axis=-1), min=0)\n",
    "                    #print( \"individual_scores size:\", individual_scores.size() )\n",
    "                    clip_scores = individual_scores[:,None].expand( -1, seq_len ).to( self.device ) / 100.0\n",
    "                    #print( \"clip_scores size:\", clip_scores.size() )\n",
    "                    reward_clip = clip_scores\n",
    "                    #end_time = time.time()\n",
    "                    #print( \"clip time:\", end_time - start_time )\n",
    "                    #start_time = time.time()\n",
    "                    bert_scores = self.bert.compute(predictions=preds_str, references=targets_str, use_fast_tokenizer=True, \\\n",
    "                                            model_type=model_name, lang='en',  device=self.device)['f1']\n",
    "                    reward_bert = torch.tensor( bert_scores )[:,None].expand( -1, seq_len ).to( self.device )\n",
    "                    #end_time = time.time()\n",
    "                    #print( \"bert time:\", end_time - start_time )\n",
    "            #image = torch.randint(255, (3, 224, 224), generator=torch.Generator().manual_seed(42))\n",
    "            #score = metric(image, \"a photo of a cat\")\n",
    "            #average_score, scores = self.spice.compute_score(target_dict, pred_dict)\n",
    "            #print( \"average_score:\", average_score ) \n",
    "            #reward_spice = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            #meteor_scores = [self.meteor.compute(predictions=[preds], references=[targets])['meteor'] for preds, targets in zip(preds_str, targets_str)]\n",
    "            #reward_meteor = [[score] * seq_len for score in meteor_scores]\n",
    "            #reward_meteor = torch.tensor( reward_meteor ).to( self.device )\n",
    "            #reward = reward_bleu + reward_cider\n",
    "            #print( \"reward_bleu:\", reward_bleu )\n",
    "            #print( \"reward_cider:\", reward_cider )\n",
    "            #print( \"reward_meteor:\", reward_meteor )\n",
    "            #reward = reward_meteor + reward_cider\n",
    "            #print( \"self.bert_coef:\", self.bert_coef )\n",
    "            #print( \"reward_bert:\", reward_bert )\n",
    "            #reward = self.clip_coef * reward_clip + self.bert_coef * reward_bert\n",
    "            reward = self.cider_coef * reward_cider + self.rouge_coef * reward_rouge \\\n",
    "                + self.clip_coef * reward_clip + self.bert_coef * reward_bert\n",
    "            reward2 = reward_cider + reward_rouge + reward_bert + reward_clip\n",
    "            #score, scores = self.spider.compute_score(gts, res)\n",
    "            #reward_rouge = [[self.spider.compute_score(target, pred)[1]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "            #reward2 = reward_rouge + reward_bert + reward_clip\n",
    "            #reward = self.rouge_coef * reward_rouge + self.cider_coef * reward_cider + self.clip_coef * reward_clip \\\n",
    "            #    + self.bert_coef * reward_bert\n",
    "            #reward = reward_bleu + reward_cider\n",
    "            #reward = reward_bleu + reward_cider + reward_meteor\n",
    "            #reward = reward_wer + reward_cider\n",
    "            #reward = reward_bleu + reward_wer + reward_cider\n",
    "            #reward = reward_bleu + reward_wer + reward_cider + reward_meteor\n",
    "            #print( \"wer:\", reward_wer )\n",
    "            #print( \"cider:\", reward_cider )\n",
    "        elif self.metric == \"bleu\":\n",
    "            reward = [[self.bleu.sentence_score(pred, [target]).score] * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "        \n",
    "        elif self.metric == \"meteor\":\n",
    "            meteor_scores = [self.meteor.compute(predictions=[preds], references=[targets])['meteor'] for preds, targets in zip(preds_str, targets_str)]\n",
    "            reward = [[score] * seq_len for score in meteor_scores]\n",
    "            \n",
    "        elif self.metric == \"rouge\":\n",
    "            #rouge_scores = self.rouge.compute(predictions=preds_str, references=targets_str, use_aggregator=False )['rougeL']\n",
    "            reward = [[self.rouge.score(target, pred)['rougeL'][0]]  * seq_len for pred, target in zip(preds_str, targets_str)]\n",
    "        \n",
    "        elif self.metric == \"wer\":\n",
    "            wer_scores = [jiwer.wer(target, pred) for pred, target in zip(targets_str, preds_str)]\n",
    "            reward = [[score] * seq_len for score in wer_scores]\n",
    "            reward = - torch.tensor( reward )\n",
    "\n",
    "        elif self.metric == 'cider':\n",
    "            pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "            target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "            #score, all_scores = parallel_cider_evaluation( target_dict, pred_dict )\n",
    "            score, scores = self.scorer.compute_score(target_dict, pred_dict)\n",
    "            reward = torch.tensor( scores ).to( self.device )[:,None].expand( -1, seq_len )\n",
    "            \n",
    "        #elif self.metric == \"bert\":\n",
    "        #    bert_scores = self.bert.compute(predictions=preds_str, references=targets_str, lang='en')['f1']\n",
    "        #    reward = [[score] * seq_len for score in bert_scores]\n",
    "\n",
    "        #elif self.metric == \"bleurt\":\n",
    "        #    bleurt_scores = self.bleurt.compute(predictions=preds_str, references=targets_str)['scores']\n",
    "        #    reward = [[score] * seq_len for score in bleurt_scores]\n",
    "            \n",
    "        elif self.metric == \"comet\":\n",
    "            data = [{\"src\": source, \"mt\": pred, \"ref\": target} for source, pred, target in zip(sources_str, preds_str, targets_str)]\n",
    "            reward = self.comet.predict(data, batch_size=8, gpus=1)['scores']\n",
    "            reward = [[score] * seq_len for score in reward]\n",
    "        else:\n",
    "            raise ValueError(f\"metric {self.metric} not supported\")\n",
    "        if self.metric != 'cider' and self.metric != 'special':\n",
    "            reward = torch.tensor(reward).to(self.device)\n",
    "        \n",
    "        #print( \"reward size:\", reward.size() )\n",
    "        #return reward\n",
    "        return reward, reward2\n",
    "\n",
    "    def my_index(self, list1, target ):\n",
    "        if target in list1:\n",
    "            return list1.index( target )\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def compute_length_reward( self, preds, targets ):\n",
    "\n",
    "        #def differentiable_argamx( logits, tau ):\n",
    "\n",
    "        #    tmp = F.gumbel_softmax( logits, tau, hard=True )\n",
    "        #    tmp1 = torch.arange( 0, logits.size(2) )[None,None] * tmp\n",
    "        #    tokens = torch.sum( tmp1, dim = 2 )\n",
    "\n",
    "        #    return tokens\n",
    "        '''\n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        #print( first_index )\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / preds.size(1)\n",
    "        #print( pred_lengths ) \n",
    "        \n",
    "        reward = pred_lengths[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "        return reward         \n",
    "        '''\n",
    "        \n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / preds.size(1)\n",
    "        \n",
    "        target_index = targets == eos_token_id\n",
    "        first_index = ( target_index.int().cumsum(dim = 1 ) == 1 ) & target_index\n",
    "        target_lengths = torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / targets.size(1)\n",
    "        #target_lengths = 1.1 * torch.sum( first_index.float() * arange_index.float(), dim = 1 ) / targets.size(1)\n",
    "        #target_lengths = torch.clamp( target_lengths, max = 1.0 )\n",
    "        #target_lengths = torch.full((preds.size(0),), self.target_length / preds.size(1), device = self.device )\n",
    "        \n",
    "        reward_lengths = - nn.MSELoss(reduction='none')( pred_lengths, target_lengths )\n",
    "        #reward_lengths = - torch.abs( pred_lengths - target_lengths )\n",
    "        \n",
    "        reward = reward_lengths[:,None].expand( -1, preds.size(1) )\n",
    "        \n",
    "        return reward  \n",
    "        \n",
    "        #reward = torch.tensor( [ self.my_index( pred.tolist(), eos_token_id ) for pred in preds ] ).to(self.device )\n",
    "        #reward = reward[:,None].expand( -1, preds.size(1) ).float()\n",
    "        \n",
    "        ##tokens = differentiable_argamx( logits, tau ) #logits から token を算出。微分可能 B * T\n",
    "        \n",
    "        '''\n",
    "        tmp1 = torch.abs( preds - tokenizer.pad_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        tmp2 = torch.abs( preds - eos_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        tmp = tmp1 * tmp2\n",
    "        pad_preds = F.sigmoid( 10 - 100 * tmp ) # pad のところだけ 1, あとは 0。 B * T  \n",
    "        reward = ( torch.tensor( preds.size(1))[None] - torch.sum( pad_preds, dim = 1 ))[:,None].expand( -1, preds.size(1) ) \n",
    "        '''\n",
    "        # 固定長97 から　pad と eos の長さを引いて、文章の長さ。文章の長さが大きいほどよい。\n",
    "        \n",
    "        #tmp1 = torch.abs( targets - tokenizer.pad_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        #tmp2 = torch.abs( targets - eos_token_id ) # pad だけ 0 あとは1以上の正の数 B * T\n",
    "        #tmp = tmp1 * tmp2\n",
    "        #pad_targets = F.sigmoid( 10 - 100 * tmp ) # pad のところだけ 1, あとは 0。 B * T\n",
    "        \n",
    "        #reward = - nn.MSELoss( reduction = 'none' )( pad_preds, pad_targets )\n",
    "        \n",
    "        #return  reward\n",
    "    \n",
    "    def calc_ngram_repeat( self, preds ):\n",
    "\n",
    "        bsz, seq_len = preds.size()\n",
    "        \n",
    "        ngram_cnt = torch.zeros( (bsz), device = preds.device, dtype = torch.float )\n",
    "        for n in range( 2, 4 ):\n",
    "            for i, pred in enumerate( preds ):\n",
    "                pred = pred.tolist()\n",
    "                ngrams = zip(*[pred[i:] for i in range(n)])\n",
    "                counts = Counter(ngrams)\n",
    "                count_sum = 0\n",
    "                for count in counts.values():\n",
    "                    if count >= self.repeat_thresh:\n",
    "                        count_sum = count_sum + count\n",
    "                ngram_cnt[i] = ngram_cnt[i] + count_sum\n",
    "\n",
    "        return - ngram_cnt[:,None].expand( -1, seq_len ) / seq_len\n",
    "\n",
    "    def unique_ngram_ratio(self, preds):\n",
    "        \n",
    "        bsz, seq_len = preds.size()\n",
    "        ng = 5\n",
    "        unr = torch.zeros( (bsz, ng), device=preds.device, dtype=torch.float)\n",
    "\n",
    "        pred_index = preds == eos_token_id\n",
    "        first_index = ( pred_index.int().cumsum(dim = 1 ) == 1 ) & pred_index\n",
    "        arange_index = torch.arange( 0, first_index.size(1), device = self.device )\n",
    "        pred_lengths = torch.sum( first_index * arange_index, dim = 1 )\n",
    "        \n",
    "        for b in range( bsz ):\n",
    "            for n in range( 0, ng - 1 ): # n は ngram - 1\n",
    "                if pred_lengths[b] > 0:\n",
    "                    #print( \"pred_lengths[b]:\", pred_lengths[b] )\n",
    "                    pred_tmp = preds[b,:pred_lengths[b]]\n",
    "                    #print( \"pred_tmp:\", pred_tmp )\n",
    "                else:\n",
    "                    pred_tmp = preds[b]\n",
    "                ngram_tensor = pred_tmp.unfold(0, n + 1, 1)\n",
    "                ngram_count = len( ngram_tensor )\n",
    "                unique_count = len( torch.unique( ngram_tensor ) )\n",
    "                #unique_count = len( set( map(tuple, ngram_tensor.tolist())))\n",
    "                unr[b,n] = unique_count / ngram_count\n",
    "\n",
    "        return torch.mean( unr, dim = 1)[:, None].expand(-1, seq_len)\n",
    "    \n",
    "    def calc_ngram_repeat_fast(self, preds):\n",
    "        bsz, seq_len = preds.size()\n",
    "        ngram_cnt = torch.zeros(bsz, device=preds.device, dtype=torch.float)\n",
    "        \n",
    "        for n in range(1, 5):\n",
    "            # 無視するトークンのリスト\n",
    "            #ignore_ids = [self.pad_token_id, self.eos_token_id, self.cls_token_id, self.sep_token_id]\n",
    "            if n == 1:\n",
    "                ignore_ids = [pad_token_id, eos_token_id, cls_token_id, sep_token_id, a_token_id, the_token_id, \\\n",
    "                              period_token_id, comma_token_id, and_token_id, in_token_id ]\n",
    "            else:\n",
    "                ignore_ids = [pad_token_id, eos_token_id, cls_token_id, sep_token_id]\n",
    "            \n",
    "            # 1. 無視すべきトークンの位置を特定 (bsz, seq_len)\n",
    "            # ignore_mask[b, i] が True なら、そのトークンは無視対象\n",
    "            ignore_mask = torch.zeros_like(preds, dtype=torch.bool)\n",
    "            for idx in ignore_ids:\n",
    "                ignore_mask |= (preds == idx)\n",
    "            \n",
    "            if seq_len < n:\n",
    "                continue\n",
    "            \n",
    "            # n-gram を抽出 (bsz, num_ngrams, n)\n",
    "            ngrams = preds.unfold(dimension=1, size=n, step=1)\n",
    "            \n",
    "            # 2. 各 n-gram に無視対象トークンが含まれているか判定\n",
    "            # n-gram内のいずれかが ignore_mask で True なら True\n",
    "            # (bsz, num_ngrams)\n",
    "            ngram_ignore_mask = ignore_mask.unfold(dimension=1, size=n, step=1).any(dim=-1)\n",
    "        \n",
    "            for b in range(bsz):\n",
    "                # このバッチの有効な n-gram だけを抽出\n",
    "                valid_ngrams = ngrams[b][~ngram_ignore_mask[b]]\n",
    "                \n",
    "                if valid_ngrams.size(0) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # ユニークな n-gram とそのカウントを取得\n",
    "                unique_ngrams, counts_per_ngram = torch.unique(valid_ngrams, dim=0, return_counts=True)\n",
    "                \n",
    "                # 閾値以上のカウントを合計\n",
    "                mask = counts_per_ngram >= self.repeat_thresh[n-1]\n",
    "                ngram_cnt[b] += counts_per_ngram[mask].sum().float() * self.repeat_weight[n-1]\n",
    "\n",
    "        penalty = - torch.clamp( torch.pow( 2, ngram_cnt -1 ) / seq_len, max = 1.0 )    \n",
    "        \n",
    "        return penalty[:, None].expand(-1, seq_len)\n",
    "    \n",
    "    def calc_cnt_repeat( self, preds ):\n",
    "\n",
    "        B, T = preds.size()\n",
    "\n",
    "        repeat_count = torch.zeros( ( bsz, vocab_size ), device = preds.device )\n",
    "        for i, pred in enumerate( preds ):\n",
    "            repeat_count0 = torch.bincount( pred )\n",
    "            repeat_count[i,:len(repeat_count0)] = repeat_count0 \n",
    "\n",
    "        repeat_count[:,pad_token_id] = 0\n",
    "        repeat_count[:,eos_token_id] = 0\n",
    "        repeat_count[:,cls_token_id] = 0\n",
    "        repeat_count[:,sep_token_id] = 0\n",
    "        thresh_masks = repeat_count >= self.repeat_thresh\n",
    "        repeat_count = repeat_count * thresh_masks\n",
    "\n",
    "        repeat = torch.sum( repeat_count, dim = 1 )\n",
    "        \n",
    "        return - repeat[:,None].expand( -1, T ).float() / seq_len\n",
    "        '''\n",
    "        B, T = preds.size()\n",
    "\n",
    "        repeat_count = torch.zeros( ( bsz, vocab_size ), device = preds.device )\n",
    "        for i, pred in enumerate( preds ):\n",
    "            repeat_count0 = torch.bincount( pred )\n",
    "            repeat_count[i,:len(repeat_count0)] = repeat_count0 \n",
    "\n",
    "        repeat_times = torch.sum( repeat_count >= self.repeat_thresh, dim = 1 )\n",
    "        #print( \"repeat_times:\",repeat_times )\n",
    "\n",
    "        pad_count = repeat_count[:,pad_token_id]\n",
    "        pad_times = (pad_count >= self.repeat_thresh).long()\n",
    "        #print( \"pad_times:\", pad_times )\n",
    "\n",
    "        eos_count = repeat_count[:,eos_token_id]\n",
    "        eos_times = (eos_count >= self.repeat_thresh).long()\n",
    "        #print( \"eos_times:\", eos_times )\n",
    "\n",
    "        repeat = repeat_times - pad_times - eos_times\n",
    "        #print( repeat )\n",
    "\n",
    "        #print( \"repeat size:\", repeat.size() )\n",
    "        \n",
    "        return - repeat[:,None].expand( -1, T ).float()\n",
    "        '''\n",
    "        '''\n",
    "        tokens = preds\n",
    "        #tokens[0,0] = 1000\n",
    "        #tokens[0,1] = 1000\n",
    "        #print( \"tokens size:\", tokens.size() )\n",
    "        #print( \"tokens[:,0]:\", tokens[:,0] )\n",
    "    \n",
    "        cnt = torch.zeros( (B, T),  device=preds.device, dtype=torch.float16 )\n",
    "        for i in range( T ):\n",
    "            cntj = torch.zeros( (B),  device=preds.device, dtype=torch.float16 )\n",
    "            num_j = 0\n",
    "            for j in range( max( 0, i - self.c ), min(  T, i + self.c ) ):\n",
    "                if j != i:\n",
    "                    ##print( \"torch.eq\", torch.eq( tokens[:,i], tokens[:,j]))\n",
    "                    #tmp1 = torch.eq( tokens[:,i], tokens[:,j]) \n",
    "                    #tmp2 = torch.ne( tokens[:,i], eos_token_id )\n",
    "                    #tmp3 = torch.ne( tokens[:,i], tokenizer.pad_token_id )\n",
    "                    ##print( \"tmp1:\", tmp1 )\n",
    "                    ##print( \"tmp2:\", tmp2 )\n",
    "                    ##print( \"tmp3:\", tmp3 )\n",
    "                    #tmp = (torch.logical_and(torch.logical_and( tmp1 , tmp2 ),tmp3 )).to(torch.float16)\n",
    "                    if self.decode_t == 'no-endoftext':\n",
    "                        tmp = ((tokens[:,i] != tokens[:,j] ).to(torch.float) ) * 10 \\\n",
    "                            +  ((tokens[:,i] == endoftext_token_id).to(torch.float) )  * 10\n",
    "                    elif self.decode_t == 'no-pad':\n",
    "                        tmp = ((tokens[:,i] != tokens[:,j] ).to(torch.float) ) * 10 \\\n",
    "                            + ((tokens[:,i] == pad_token_id).to(torch.float) )  * 10 \\\n",
    "                            + ((tokens[:,i] == eos_token_id).to(torch.float) ) * 10\n",
    "                            # tokens[:,i] と tokens[:,j] が同じで、tokens[:,i] が　eos、pad でなければ 0 その他は 10 以上の整数 \n",
    "                    tmp = F.sigmoid( 10 - 100 * tmp ) # tokens[:,i]とtokens[:,j] が同じで eos pad ではないところだけ 1, あとは 0.\n",
    "                    #if torch.any( tmp != 0 ):\n",
    "                    #    print( \"tmp:\", tmp )\n",
    "                    cntj = cntj + tmp # repeat の数。[B]  jについて足しこんでいる。\n",
    "                    #if torch.any( cntj != 0 ):\n",
    "                    #    print( \"cntj:\", cntj )\n",
    "                    num_j = num_j + 1\n",
    "            cnt[:,i] = cntj / num_j # repeat の数。[B] i について足しこんでいる。\n",
    "        \n",
    "        #mask = cnt != 0\n",
    "        #print( \"cnt mask:\", cnt[mask] )\n",
    "        #return - torch.mean( cnt, dim = 1 ) # \n",
    "        return - torch.mean( cnt, dim = 1 )[:,None].expand(-1, T )  # B * T repeat の数が少ないほど良い。repeat の数の -1 倍が多いほどよい。\n",
    "        '''\n",
    "    def forward(self, top_probs, sampled_beam_idx, top_indices, targets, imgs2,  sources=None, masks=None):\n",
    "        \"\"\"\n",
    "        outputs: batch x len x d_model\n",
    "        targets: batch x len\n",
    "        sources: batch x len\n",
    "        masks:   batch x len\n",
    "        \"\"\"\n",
    "        self.device = sampled_beam_idx.device\n",
    "        # input to device\n",
    "        targets = targets.to(self.device)\n",
    "        bsz, seq_len, beam = top_probs.size()\n",
    "        eps = 1e-8\n",
    "\n",
    "        preds = torch.gather( top_indices, -1, sampled_beam_idx ).squeeze( -1 )\n",
    "        tmp = torch.clamp( top_probs, eps )\n",
    "        top_log_probs = torch.log( tmp )\n",
    "        sample_log_probs = torch.gather( top_log_probs, -1, sampled_beam_idx ).squeeze( -1 )\n",
    "\n",
    "        if self.reward_t == 'ordinary':\n",
    "            reward_ord = self.compute_reward(preds, targets, imgs2, sources)   #  bsz\n",
    "            reward_repeat = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_length = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == \"ord+rep\":\n",
    "            reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "            #reward = reward_ord + reward_repeat - b # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == \"ord+rep+len\":\n",
    "            reward_ord, reward_ord2 = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == 'ord+len':\n",
    "            reward_ord = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            #reward_repeat = self.calc_cnt_repeat( preds ) + self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = torch.zeros( ( bsz, seq_len ),  device = preds.device, dtype=torch.float ) # bsz * seq_len\n",
    "        elif self.reward_t == 'ord+rep+len+unr':\n",
    "            reward_ord, reward_ord2 = self._compute_reward_ord(preds, targets, imgs2, sources)  # bsz * seq_len\n",
    "            reward_repeat = self.calc_ngram_repeat_fast( preds ) # bsz * seq_len\n",
    "            reward_length = self.compute_length_reward( preds, targets ) # bsz * seq_len\n",
    "            reward_unr = self.unique_ngram_ratio(preds)\n",
    "        \n",
    "        ## apply mask\n",
    "        #if masks is not None:\n",
    "        #    masks = masks.to(self.device)\n",
    "        #    probs, targets = probs[masks], targets[masks]\n",
    "        #    # outputs, targets = outputs[masks], targets[masks]\n",
    "        #    reward, preds = reward[masks], preds[masks]\n",
    "       \n",
    "        return reward_ord, reward_ord2, reward_repeat, reward_length, reward_unr, preds, sample_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "siyThdM4Icll"
   },
   "source": [
    "### CaptioningTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logsumexp(x, dim=1):\n",
    "    return torch.logsumexp(x.float(), dim=dim).type_as(x)\n",
    "\n",
    "class DynamicCRF(nn.Module):\n",
    "    def __init__(self, num_embedding, low_rank=32, beam_size=64, crf_coef=1.0, temp = 0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        #low_rank = num_embedding\n",
    "        self.E1 = nn.Embedding(num_embedding, low_rank)\n",
    "        self.E2 = nn.Embedding(num_embedding, low_rank)\n",
    "\n",
    "        self.vocb = num_embedding\n",
    "        self.rank = low_rank\n",
    "        self.beam = beam_size\n",
    "        self.crf_coef = crf_coef\n",
    "        self.temp = temp\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return \"vocab_size={}, low_rank={}, beam_size={}\".format(\n",
    "            self.vocb, self.rank, self.beam)\n",
    "\n",
    "    def forward(self, emissions, top_logits, top_indices, targets, masks, beam=None):\n",
    "        numerator = self._compute_score(emissions, targets, masks)\n",
    "        denominator = self._compute_normalizer(emissions, targets, masks, beam )\n",
    "        beam_probs = self._compute_normalizer2(top_logits, top_indices, targets, masks, beam)\n",
    "\n",
    "        return numerator - denominator, beam_probs\n",
    "    \n",
    "    def forward_decoder(self, emissions, masks=None, beam=None):\n",
    "        return self._viterbi_decode(emissions, masks, beam)\n",
    "\n",
    "    def _compute_score(self, emissions, targets, masks=None):\n",
    "        batch_size, seq_len = targets.size()\n",
    "\n",
    "        emission_scores = emissions.gather(2, targets[:, :, None])[:, :, 0]  # B x T\n",
    "        transition_scores = (self.E1(targets[:, :-1]) * self.E2(targets[:, 1:])).sum(2)\n",
    "       \n",
    "        scores = emission_scores\n",
    "        scores[:, 1:] += transition_scores\n",
    "        \n",
    "        if masks is not None:\n",
    "            scores = scores * masks.type_as(scores)\n",
    "\n",
    "        return scores.sum(-1)\n",
    "        \n",
    "    def _compute_normalizer(self, emissions, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        if targets is not None:\n",
    "            #_emissions = emissions.scatter(2, targets[:, :, None], np.float('inf'))\n",
    "            _emissions = emissions.scatter(2, targets[:, :, None], float('inf'))\n",
    "            beam_targets = _emissions.topk(beam, 2)[1]\n",
    "            beam_emission_scores = emissions.gather(2, beam_targets)\n",
    "        else:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D; position i - 1, previous step.\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D; position i, current step.\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam)\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        for i in range(1, seq_len):\n",
    "            next_score = score[:, :, None] + beam_transition_matrix[:, i-1]\n",
    "            next_score = logsumexp(next_score, dim=1) + beam_emission_scores[:, i]\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i:i+1], next_score, score)\n",
    "            else:\n",
    "                score = next_score\n",
    "\n",
    "        return logsumexp(score, dim=1)\n",
    "\n",
    "    def _compute_top_probs(self, beam_emission_scores, beam_transition_matrix, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "\n",
    "            # greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "\n",
    "            # multinomial selection\n",
    "            B, C, W = _score.shape\n",
    "            flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            _index_flat = torch.multinomial(probs, num_samples=1)\n",
    "            _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            _index = _index_flat.view(B, W)\n",
    "            _score = _score_flat.view(B, W)\n",
    "\n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "        \n",
    "        all_scores = traj_scores\n",
    "        all_scores.append( score )\n",
    "        all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 ).to(device)\n",
    "        #beam_probs = F.softmax( all_scores / self.temp, dim = 2 )\n",
    "        beam_probs = F.softmax( all_scores, dim = 2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        #finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return beam_probs, finalized_tokens.unsqueeze(-1)\n",
    "\n",
    "    def _compute_top_probs2(self, beam_emission_scores, beam_transition_matrix, beam_targets, n_best = 10, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        window = 5\n",
    "\n",
    "        exclude_token_id2 = torch.tensor( [pad_token_id, eos_token_id, a_token_id, the_token_id, and_token_id, in_token_id, \\\n",
    "            we_token_id, i_token_id, he_token_id, she_token_id, it_token_id, they_token_id, \\\n",
    "            period_token_id, comma_token_id, dbl_token_id, sgl_token_id], device=device )\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "    \n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        _score = beam_emission_scores[:, 0][:,None,:].expand(-1,beam,-1) # t = -1 → 0 への beam_transition_matrix bsz*beam C*beam W は　0 \n",
    "        #_score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "        B, C, W = _score.shape\n",
    "        flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "        probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "        _index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )\n",
    "        _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "        _index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "        _score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "    \n",
    "        traj_scores2 = []\n",
    "        traj_tokens2 = []\n",
    "    \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score) # bsz * beam\n",
    "            traj_scores2.append( _score2 )\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            \n",
    "            ## greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            #_score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "            \n",
    "            ## multinomial selection\n",
    "            B, C, W = _score.shape\n",
    "            flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            #print( \"flat_score size:\", flat_score.size() )\n",
    "            #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            _index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )  # C が n_best になる。\n",
    "            _score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            _index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "            _score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "            \n",
    "            #_score = _score + beam_emission_scores[:, i] # bsz, beam        \n",
    "    \n",
    "            _score2 = _score2 + beam_emission_scores[:, i][:,None,:].expand(-1,beam,-1).gather( 1, _index2 ) # bsz, n_best, beam       \n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score2[:,0,:], _index2[:,0,:]\n",
    "            traj_tokens.append(index)\n",
    "            traj_tokens2.append( _index2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        used_index = torch.zeros( bsz, seq_len, dtype=torch.long, device=device)\n",
    "        used_index[:,seq_len - 1] = best_index\n",
    "        beam_probs = torch.zeros( bsz, seq_len, beam, dtype=torch.float, device=device )\n",
    "        beam_probs[:,seq_len - 1,:] = score \n",
    "        for t_reverse, (idx2, scs2) in enumerate( zip( reversed(traj_tokens2), reversed(traj_scores2))):\n",
    "            t = seq_len - t_reverse - 2\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            #print( \"previous_index size:\", previous_index.size() )\n",
    "            selected_idx = idx2[:,0,:].gather( 1, previous_index )\n",
    "            selected_scs = scs2[:,0,:].gather( 1, previous_index )\n",
    "            beam_probs[ :, t ,: ] = scs2[:,0,: ]\n",
    "            for n in range( 1, n_best ):\n",
    "                for b in range( bsz ):\n",
    "                    t_max = torch.min( t + window, seq_len - 1 )\n",
    "                    used_index_tmp = used_index[b,t+1:t_max]\n",
    "                    corrected_selected_idx = torch.gather( beam_targets[b,t,:], 0, selected_idx[b] )\n",
    "                    #if corrected_selected_idx in used_index[b] and corrected_selected_idx not in exclude_token_id2:\n",
    "                    if corrected_selected_idx in used_index_tmp and corrected_selected_idx not in exclude_token_id2:\n",
    "                        selected_idx[b] = idx2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        selected_scs[b] = scs2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        beam_probs[ b, t ,: ] = scs2[b, n,: ]\n",
    "                    else:\n",
    "                        break\n",
    "            #print( \"beam_targets[:,t,:].size():\",beam_targets[:,t,:].size())\n",
    "            #print( \"selected_idx.size():\", selected_idx.size() )\n",
    "            used_index[:,t] =  torch.gather( beam_targets[:,t,:], -1, selected_idx )[:,0] # bsz * 1 → bsz\n",
    "            #print( \"used_index[:,:]:\", used_index[:,:] )\n",
    "            #used_index[:,t] = selected_idx[:,0]\n",
    "            finalized_tokens.append( selected_idx )\n",
    "            finalized_scores.append( selected_scs )\n",
    "       \n",
    "        finalized_tokens.reverse()\n",
    "        sampled_beam_idx = torch.cat(finalized_tokens, 1)\n",
    "        #finalized_tokens = beam_targets.gather(2, sampled_beam_idx[:, :, None])[:, :, 0]\n",
    "        #print( \"finalized_tokens:\", finalized_tokens )\n",
    "        \n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        beam_probs = F.softmax( beam_probs, dim = 2 )\n",
    "        \n",
    "        #return finalized_scores, finalized_tokens\n",
    "        return beam_probs, sampled_beam_idx.unsqueeze(-1)\n",
    "\n",
    "    def _compute_viterbi_no_repeat(self, beam_emission_scores, beam_transition_matrix, beam_targets, n_best = 10, targets=None, masks=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        device = beam_emission_scores.device\n",
    "        exclude_token_id2 = torch.tensor( [pad_token_id, eos_token_id, a_token_id, the_token_id, and_token_id, in_token_id, \\\n",
    "            we_token_id, i_token_id, he_token_id, she_token_id, it_token_id, they_token_id, \\\n",
    "            period_token_id, comma_token_id, dbl_token_id, sgl_token_id], device=device )\n",
    "        window = 5\n",
    "        \n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = beam_emission_scores.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "    \n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        _score = beam_emission_scores[:, 0][:,None,:].expand(-1,beam,-1) # t = -1 → 0 への beam_transition_matrix bsz*beam C*beam W は　0 \n",
    "        _score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "        #B, C, W = _score.shape\n",
    "        #flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "        #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "        #_index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )\n",
    "        #_score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "        #_index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "        #_score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "    \n",
    "        traj_scores2 = []\n",
    "        traj_tokens2 = []\n",
    "    \n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score) # bsz * beam\n",
    "            traj_scores2.append( _score2 )\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            \n",
    "            ## greedy selection\n",
    "            #_score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score2, _index2 = torch.topk( _score, n_best, dim = 1 )\n",
    "            \n",
    "            ### multinomial selection\n",
    "            #B, C, W = _score.shape\n",
    "            #flat_score = _score.permute(0, 2, 1).reshape(-1, C)\n",
    "            ##print( \"flat_score size:\", flat_score.size() )\n",
    "            ##probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            #probs = F.softmax(flat_score / self.temp, dim=-1)\n",
    "            #_index_flat = torch.multinomial(probs, num_samples=n_best, replacement = False )  # C が n_best になる。\n",
    "            #_score_flat = torch.gather(flat_score, -1, _index_flat)\n",
    "            #_index2 = _index_flat.view(B, W, n_best).transpose(1,2)\n",
    "            #_score2 = _score_flat.view(B, W, n_best).transpose(1,2)\n",
    "            \n",
    "            #_score = _score + beam_emission_scores[:, i] # bsz, beam        \n",
    "    \n",
    "            _score2 = _score2 + beam_emission_scores[:, i][:,None,:].expand(-1,beam,-1).gather( 1, _index2 ) # bsz, n_best, beam       \n",
    "            \n",
    "            #if masks is not None:\n",
    "            #    score = torch.where(masks[:, i: i+1], _score, score)\n",
    "            #    index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            #else:\n",
    "            score, index = _score2[:,0,:], _index2[:,0,:]\n",
    "            traj_tokens.append(index)\n",
    "            traj_tokens2.append( _index2 )\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        used_index = torch.zeros( bsz, seq_len, dtype=torch.long, device=device )\n",
    "        used_index[:,seq_len - 1] = best_index\n",
    "        beam_probs = torch.zeros( bsz, seq_len, beam, dtype=torch.float, device=device )\n",
    "        beam_probs[:,seq_len - 1,:] = score \n",
    "        for t_reverse, (idx2, scs2) in enumerate( zip( reversed(traj_tokens2), reversed(traj_scores2))):\n",
    "            t = seq_len - t_reverse - 2\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            #print( \"previous_index size:\", previous_index.size() )\n",
    "            selected_idx = idx2[:,0,:].gather( 1, previous_index )\n",
    "            selected_scs = scs2[:,0,:].gather( 1, previous_index )\n",
    "            beam_probs[ :, t ,: ] = scs2[:,0,: ]\n",
    "            for n in range( 1, n_best ):\n",
    "                for b in range( bsz ):\n",
    "                    t_max = torch.min( t + window, seq_len - 1 )\n",
    "                    used_index_tmp = used_index[b,t+1:t_max]\n",
    "                    corrected_selected_idx = torch.gather( beam_targets[b,t,:], 0, selected_idx[b] )\n",
    "                    #if corrected_selected_idx in used_index[b] and corrected_selected_idx not in exclude_token_id2:\n",
    "                    if corrected_selected_idx in used_index_tmp and corrected_selected_idx not in exclude_token_id2:\n",
    "                        selected_idx[b] = idx2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        selected_scs[b] = scs2[b,n,:].gather( 0, previous_index[b] )\n",
    "                        beam_probs[ b, t ,: ] = scs2[b, n,: ]\n",
    "                    else:\n",
    "                        break\n",
    "            #print( \"beam_targets[:,t,:].size():\",beam_targets[:,t,:].size())\n",
    "            #print( \"selected_idx.size():\", selected_idx.size() )\n",
    "            used_index[:,t] =  torch.gather( beam_targets[:,t,:], -1, selected_idx )[:,0] # bsz * 1 → bsz\n",
    "            #print( \"used_index[:,:]:\", used_index[:,:] )\n",
    "            #used_index[:,t] = selected_idx[:,0]\n",
    "            finalized_tokens.append( selected_idx )\n",
    "            finalized_scores.append( selected_scs )\n",
    "       \n",
    "        finalized_tokens.reverse()\n",
    "        sampled_beam_idx = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, sampled_beam_idx[:, :, None])[:, :, 0]\n",
    "        #print( \"finalized_tokens:\", finalized_tokens )\n",
    "        \n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        #beam_probs = F.softmax( beam_probs, dim = 2 )\n",
    "        \n",
    "        return finalized_scores, finalized_tokens\n",
    "    \n",
    "    \n",
    "    def _compute_f_algorithm(self, emissions, temp = 1.0, targets=None, beam=None):\n",
    "\n",
    "        eps = 1e-8\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        bsz = batch_size\n",
    "        \n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "   \n",
    "        # 結果格納用\n",
    "        scores = torch.zeros((batch_size, seq_len, beam), device=emissions.device)\n",
    "        all_preds = []\n",
    "        sampled_beam_idx = []\n",
    "        \n",
    "        # --- ステップ 0 ---\n",
    "        scores[:, 0, :] = beam_emission_scores[:, 0]\n",
    "        probs_0 = F.softmax(scores[:, 0, :] / temp, dim=-1)\n",
    "        sampled_beam_idx_0 = torch.multinomial(probs_0, 1)\n",
    "\n",
    "        # 256個中の位置から、実際のVocab IDに変換\n",
    "        all_preds.append(  torch.gather(beam_targets[:, 0], 1, sampled_beam_idx_0).squeeze(1) )\n",
    "        sampled_beam_idx.append( sampled_beam_idx_0 )\n",
    "        \n",
    "        # --- ステップ 1 以降 ---\n",
    "        for i in range(1, seq_len):\n",
    "            # 1. 直前に自分が選んだ単語(Vocab ID)を取得\n",
    "            prev_vocab_idx = all_preds[-1]\n",
    "            \n",
    "            # 2. 遷移スコアの計算\n",
    "            prev_trans_feat = self.E1(prev_vocab_idx) # B x D\n",
    "            curr_trans_feat = self.E2(beam_targets[:, i]) # B x beam x D\n",
    "        \n",
    "            # B x 1 x D  @  B x D x beam  -> B x 1 x beam\n",
    "            #D = prev_trans_feat.size(-1) # 次元のサイズ\n",
    "            #current_trans_scores = torch.bmm(\n",
    "            #    prev_trans_feat.unsqueeze(1), \n",
    "            #    curr_trans_feat.transpose(1, 2)\n",
    "            #).squeeze(1) / (D ** 0.5) \n",
    "            current_trans_scores = torch.bmm(\n",
    "                prev_trans_feat.unsqueeze(1), \n",
    "                curr_trans_feat.transpose(1, 2)\n",
    "            ).squeeze(1)\n",
    "\n",
    "            # 3. 現在のスコアを確定\n",
    "            scores[:, i, :] = beam_emission_scores[:, i] + current_trans_scores\n",
    "        \n",
    "            # 4. 次のステップのために、現在の単語をサンプリング（正規化が必要）\n",
    "            probs_i = F.softmax(scores[:, i, :] / temp , dim=-1)\n",
    "            sampled_beam_idx_i = torch.multinomial(probs_i, 1).long()\n",
    "            all_preds.append( torch.gather(beam_targets[:, i], 1, sampled_beam_idx_i).squeeze(1) )\n",
    "            sampled_beam_idx.append( sampled_beam_idx_i )\n",
    "                              \n",
    "\n",
    "        all_preds = torch.stack( all_preds, dim = 0 ).transpose(0,1)\n",
    "        sampled_beam_idx = torch.stack( sampled_beam_idx, dim = 0 ).transpose(0,1)\n",
    "        \n",
    "        ## --- 全体の log_probs 算出（ratio計算用） ---\n",
    "        #max_score, _ = torch.max(scores, dim=2, keepdim=True)\n",
    "        #exp1 = torch.exp(scores - max_score)\n",
    "        #sumof = torch.sum(exp1, dim=2, keepdim=True)\n",
    "        #denominator1 = torch.log(sumof + 1e-8) + max_score\n",
    "    \n",
    "        #beam_log_probs = scores - denominator1\n",
    "        ##sample_log_probs = torch.gather( beam_log_probs, -1, sampled_beam_idx ).squeeze(-1)\n",
    "        #beam_probs = torch.exp(beam_log_probs)\n",
    "\n",
    "        #return beam_probs, preds, sample_log_probs, sampled_beam_idx\n",
    "        return scores, all_preds\n",
    "\n",
    "    def _viterbi_decode(self, emissions, masks=None, beam=None):\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        #print( \"score size:\", score.size() )\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam \n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None])\n",
    "        finalized_scores.append(best_score[:, None])\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)):\n",
    "            previous_index = finalized_tokens[-1]\n",
    "            finalized_tokens.append(idx.gather(1, previous_index))\n",
    "            finalized_scores.append(scs.gather(1, previous_index))\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "\n",
    "        return finalized_scores, finalized_tokens\n",
    "    \n",
    "    def _compute_many_values(self, emissions, targets, top_indices = None, masks=None, beam=None):\n",
    "\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        \n",
    "        if top_indices == None:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        else:\n",
    "            beam_emission_scores = torch.gather( emissions, -1, top_indices )\n",
    "            beam_targets = top_indices\n",
    "        \n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "\n",
    "        traj_tokens, traj_scores = [], []\n",
    "        finalized_tokens, finalized_scores = [], []\n",
    "\n",
    "        # compute the normalizer in the log-space\n",
    "        score = beam_emission_scores[:, 0]  # B x K\n",
    "        dummy = torch.arange(beam, device=score.device).expand(*score.size()).contiguous()\n",
    "\n",
    "        for i in range(1, seq_len):\n",
    "            traj_scores.append(score)\n",
    "            _score = score[:, :, None] + beam_transition_matrix[:, i-1] # bsz, beam, beam\n",
    "            _score, _index = _score.max(dim=1) # bsz, beam     bsz, beam  step i-1 における 256 → 256 の max から 256 への遷移確率と \n",
    "                                                # 256 → 256 の前の 256 の max のインデックストークン\n",
    "                                                # index b * 256 の 位置が i の token で、値が i-1 のtoken   \n",
    "\n",
    "            _score = _score + beam_emission_scores[:, i] # bsz, beam i における 256 の遷移確率ではない確率を加える。i における 256 の全確率。\n",
    "\n",
    "            if masks is not None:\n",
    "                score = torch.where(masks[:, i: i+1], _score, score)\n",
    "                index = torch.where(masks[:, i: i+1], _index, dummy)\n",
    "            else:\n",
    "                score, index = _score, _index\n",
    "            traj_tokens.append(index)\n",
    "\n",
    "        all_scores = traj_scores\n",
    "        all_scores.append( score )\n",
    "        all_scores = torch.stack( all_scores, dim = 0 ).transpose( 0, 1 )\n",
    "        \n",
    "        # now running the back-tracing and find the best\n",
    "        best_score, best_index = score.max(dim=1)\n",
    "        finalized_tokens.append(best_index[:, None]) # 時刻 T における b*256 の確率最大の token\n",
    "        finalized_scores.append(best_score[:, None]) #時刻 T における b*256 の確率最大の score\n",
    "\n",
    "        for idx, scs in zip(reversed(traj_tokens), reversed(traj_scores)): #idx,scs は、反転時刻 i と i-1における b * 256のトークンと確率\n",
    "            previous_index = finalized_tokens[-1] # 時刻 Tなど 求めたいトークンと確率の一個後 における token　b * 1\n",
    "            finalized_tokens.append(idx.gather(1, previous_index)) # 時刻 一個後iのトークン previou_index に至るための時刻i-1 のトークン\n",
    "                                                                    # b* 256 の token から b * 1 の previous_idnex token で gather\n",
    "            finalized_scores.append(scs.gather(1, previous_index)) # 時刻一個後 i のトークンに至るための時刻 i-1 の確率\n",
    "\n",
    "        finalized_tokens.reverse()\n",
    "        finalized_tokens = torch.cat(finalized_tokens, 1)\n",
    "        finalized_tokens = beam_targets.gather(2, finalized_tokens[:, :, None])[:, :, 0]\n",
    "\n",
    "        finalized_scores.reverse()\n",
    "        finalized_scores = torch.cat(finalized_scores, 1)\n",
    "        finalized_scores[:, 1:] = finalized_scores[:, 1:] - finalized_scores[:, :-1]\n",
    "        \n",
    "        if self.crf_coef != 0.0:\n",
    "            numerator = self._compute_score(emissions, targets)\n",
    "            denominator = self._compute_normalizer(emissions, targets)\n",
    "            crf_loss = - ( numerator - denominator ).mean() / seq_len\n",
    "        else:\n",
    "            crf_loss = torch.zeros( (1), device = emissions.device, dtype = torch.float )\n",
    "        \n",
    "        top_probs, sampled_beam_idx = self._compute_top_probs(beam_emission_scores, beam_transition_matrix)\n",
    "        \n",
    "        return finalized_scores, finalized_tokens, top_probs, beam_targets, crf_loss, sampled_beam_idx\n",
    "    '''\n",
    "    \n",
    "    def _compute_many_values(self, emissions, targets, top_indices = None, masks=None, beam=None):\n",
    "\n",
    "        beam = beam if beam is not None else self.beam\n",
    "        batch_size, seq_len = emissions.size()[:2]\n",
    "        \n",
    "        if top_indices == None:\n",
    "            beam_emission_scores, beam_targets = emissions.topk(beam, 2)\n",
    "        else:\n",
    "            beam_emission_scores = torch.gather( emissions, -1, top_indices )\n",
    "            beam_targets = top_indices\n",
    "        \n",
    "        beam_transition_score1 = self.E1(beam_targets[:, :-1])  # B x (T-1) x K x D\n",
    "        beam_transition_score2 = self.E2(beam_targets[:, 1:])   # B x (T-1) x K x D\n",
    "        beam_transition_matrix = torch.bmm(\n",
    "            beam_transition_score1.view(-1, beam, self.rank),\n",
    "            beam_transition_score2.view(-1, beam, self.rank).transpose(1, 2))\n",
    "        beam_transition_matrix = beam_transition_matrix.view(batch_size, -1, beam, beam) # bsz, seq_len, beam, beam\n",
    "        \n",
    "        if self.crf_coef != 0.0:\n",
    "            numerator = self._compute_score(emissions, targets)\n",
    "            denominator = self._compute_normalizer(emissions, targets)\n",
    "            #denominator = self._compute_normalizer2(emissions, targets)\n",
    "            crf_loss = - ( numerator - denominator ).mean() / seq_len\n",
    "        else:\n",
    "            crf_loss = torch.zeros( (1), device = emissions.device, dtype = torch.float )\n",
    "\n",
    "        finalized_scores, finalized_tokens = self._compute_viterbi_no_repeat(beam_emission_scores, beam_transition_matrix, beam_targets)\n",
    "        top_probs, sampled_beam_idx = self._compute_top_probs2(beam_emission_scores, beam_transition_matrix, beam_targets)\n",
    "        \n",
    "        return finalized_scores, finalized_tokens, top_probs, beam_targets, crf_loss, sampled_beam_idx\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopLayer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, crf_low_rank, crf_beam_size, dropout, padding_idx,\n",
    "                crf_coef = 1.0, temp = 0.5 ):\n",
    "        super(TopLayer, self).__init__()\n",
    "\n",
    "        self.embed_dim = embed_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.dropout = dropout\n",
    "        self.padding_idx = padding_idx\n",
    "        print( \"in TopLayer:\" )\n",
    "        self.crf_layer = DynamicCRF(num_embedding = vocab_size, low_rank = crf_low_rank, \n",
    "                                    beam_size = crf_beam_size, crf_coef=crf_coef, temp=temp)\n",
    "\n",
    "        #self.one_more_layer_norm = nn.LayerNorm(embed_dim)\n",
    "        #self.tgt_word_prj = nn.Linear(self.embed_dim, self.vocab_size)\n",
    "        ## gae 学習用\n",
    "        #self.linear_critical = nn.Linear(crf_beam_size, 1 )\n",
    "\n",
    "    def forward(self, src_representation, top_logits, top_indices, src_input, tgt_input, is_training ):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        #assert src_input.size() == tgt_input.size()\n",
    "\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        #seqlen, bsz = src_input.size()\n",
    "        seqlen, bsz = src_input.shape[:2]\n",
    "\n",
    "        src_representation = F.dropout(src_representation, p=self.dropout, training=is_training)\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "\n",
    "        src = src_representation\n",
    "\n",
    "        #emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "        emissions = src_representation\n",
    "        #log_probs = torch.log_softmax(emissions, -1)\n",
    "        #assert log_probs.size() == torch.Size([seqlen, bsz, self.vocab_size])\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz x src_len x vocab_size]\n",
    "        #emission_mask = ~tgt_input.eq(self.padding_idx) # [bsz x src_len] #pad のところは 0 padでないところが 1\n",
    "        emission_mask = torch.ones_like( tgt_input, dtype=torch.bool ) #全部　pad でないとして 1\n",
    "        batch_crf_loss, top_probs = self.crf_layer(emissions, top_logits, top_indices, tgt_input, emission_mask) # [bsz]\n",
    "        #critical_value = self.linear_critical( top_probs )\n",
    "        #critical_value = torch.zeros( ( 1,1,1) )\n",
    "        batch_crf_loss = - batch_crf_loss\n",
    "        assert batch_crf_loss.size() == torch.Size([bsz])\n",
    "        return batch_crf_loss, top_probs\n",
    "\n",
    "    def decoding(self, src_representation, src_input):\n",
    "        '''\n",
    "            src_representation : bsz x seqlen x embed_dim\n",
    "            src_input : bsz x seqlen\n",
    "            tgt_input : bsz x seqlen\n",
    "        '''\n",
    "        src_input = src_input.transpose(0, 1) # src_len x bsz\n",
    "        seqlen, bsz = src_input.size()\n",
    "\n",
    "        src_representation = src_representation.transpose(0, 1) # seqlen x bsz x embed_dim\n",
    "        src = src_representation\n",
    "\n",
    "        emissions = self.tgt_word_prj(src.contiguous().view(-1, self.embed_dim)).view(seqlen, bsz, self.vocab_size)\n",
    "\n",
    "        emissions = emissions.transpose(0, 1) # [bsz, seqlen, vocab_size]\n",
    "        _, finalized_tokens = self.crf_layer.forward_decoder(emissions)\n",
    "        assert finalized_tokens.size() == torch.Size([bsz, seqlen])\n",
    "        return finalized_tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CaptioningTransformer(nn.Module):\n",
    "    \n",
    "    #CaptioningTransformerのコンストラクタ\n",
    "    #dim_embedding  : 埋め込み次元\n",
    "    #dim_feedforward: FNNの中間特徴次元\n",
    "    #num_heads      : マルチヘッドアテンションのヘッド数\n",
    "    #num_layers     : Transformerデコーダ層の数\n",
    "    #vocab_size     : 辞書の次元\n",
    "    #null_index     : NULLのID\n",
    "    #dropout        : ドロップアウト確率\n",
    "    \n",
    "    def __init__(self, img_size: int,  dim_embedding: int, length_max: int, vocab_size: int, tokenizer, dropout: float = 0.0, \\\n",
    "                 pad_token_id: int=0, use_repeat_logits_half=False, crf_coef = 1.0, temp=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        #CLIP\n",
    "        model_id = \"openai/clip-vit-large-patch14-336\"\n",
    "        self.clip_model = CLIPVisionModel.from_pretrained(model_id )\n",
    "        memory = self.clip_model( torch.randn( 1, 3, 336, 336 ) )\n",
    "        memory = memory.last_hidden_state\n",
    "        img_length = memory.size(1)\n",
    "        clip_dim = memory.size(2)\n",
    "        self.connector_pool = nn.AdaptiveAvgPool1d(length_max - 1 )\n",
    "        self.connector_ln = nn.LayerNorm( clip_dim )\n",
    "        self.connector_linear1 = nn.Linear( clip_dim, dim_embedding )\n",
    "        self.connector_gleu = nn.GELU()\n",
    "        self.connector_linear2 = nn.Linear( dim_embedding, dim_embedding )\n",
    "\n",
    "       \n",
    "        # Connector\n",
    "        self.connector_pool = nn.AdaptiveAvgPool1d(length_max - 1 )\n",
    "       # Down Sampling\n",
    "        cls_token = memory[:, :1, :] # (bsz, 1, 1024)\n",
    "        patch_tokens = memory[:, 1:, :] # (bsz, 576, 1024)\n",
    "        # パッチ部分を 576 -> 96 に圧縮\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 1024, 576)\n",
    "        patch_tokens = self.connector_pool(patch_tokens)\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 96, 1024)\n",
    "        # CLSと結合して合計 97 トークンにする\n",
    "        memory = torch.cat([cls_token, patch_tokens], dim=1) # (bsz, 97, 1024)\n",
    "\n",
    "        self.pos_emb = PositionalEmbedding( dim_embedding )\n",
    "\n",
    "        model_id = \"google-bert/bert-large-uncased\"\n",
    "        self.bert = BertModel.from_pretrained( model_id )\n",
    "\n",
    "        ## 単語出力分布計算\n",
    "        self.ln_outputs = nn.LayerNorm( dim_embedding )\n",
    "        self.linear = nn.Linear(dim_embedding, vocab_size)\n",
    "\n",
    "        crf_low_rank = 32\n",
    "        crf_beam_size = 256\n",
    "        self.crf_beam_size = crf_beam_size\n",
    "        top_dropout = 0.0\n",
    "        tgt_padding_idx = tokenizer.pad_token_id\n",
    "        self.toplayer = TopLayer( vocab_size, dim_embedding, crf_low_rank, crf_beam_size, top_dropout, \n",
    "                                  tgt_padding_idx, crf_coef = crf_coef, temp=temp )\n",
    "        \n",
    "        ### GAE 用\n",
    "        self.ln_critical = nn.LayerNorm( crf_beam_size )\n",
    "        self.linear_critical = nn.Linear( crf_beam_size, 1)\n",
    "        \n",
    "        self.dim_embedding = dim_embedding\n",
    "        self.use_repeat_logits_half = use_repeat_logits_half\n",
    "\n",
    "\n",
    "    def mlp_connector(self, memory ):\n",
    "\n",
    "        cls_token = memory[:, :1, :] # (bsz, 1, 1024)\n",
    "        patch_tokens = memory[:, 1:, :] # (bsz, 576, 1024)\n",
    "\n",
    "        # パッチ部分を 576 -> 96 に圧縮\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 1024, 576)\n",
    "        patch_tokens = self.connector_pool(patch_tokens)\n",
    "        patch_tokens = patch_tokens.transpose(1, 2) # (bsz, 96, 1024)\n",
    "\n",
    "        # CLSと結合して合計 97 トークンにする\n",
    "        memory = torch.cat([cls_token, patch_tokens], dim=1) # (bsz, 97, 1024)\n",
    "\n",
    "        memory = self.connector_ln( memory )\n",
    "        memory = self.connector_linear1( memory )\n",
    "        memory = self.connector_gleu( memory )\n",
    "        memory = self.connector_linear2( memory )\n",
    "        \n",
    "        return memory\n",
    "\n",
    "    def forward(self, images: torch.Tensor, targets: torch.Tensor, top_indices = None ):\n",
    "\n",
    "        self.device = images.device\n",
    "        \n",
    "        memory = self.clip_model( images ).last_hidden_state\n",
    "        memory = self.mlp_connector( memory )\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        emissions = self.linear( outputs )\n",
    "\n",
    "        if self.use_repeat_logits_half == True:\n",
    "            emissions = repeat_logits_half( emissions )\n",
    "\n",
    "\n",
    "        finalized_scores, finalized_tokens, top_probs, top_indices, crf_loss, sampled_beam_idx  = \\\n",
    "            self.toplayer.crf_layer._compute_many_values(emissions, targets, top_indices)\n",
    "            #self.crf_layer._compute_many_values(emissions, targets, top_indices)\n",
    "        \n",
    "        critical_value = self.ln_critical( top_probs )\n",
    "        critical_value = self.linear_critical( critical_value )\n",
    "\n",
    "        return finalized_scores, finalized_tokens, top_probs, top_indices, \\\n",
    "            critical_value[:,:,0], crf_loss, emissions, sampled_beam_idx\n",
    "        #return finalized_scores, finalized_tokens, top_probs, top_indices, crf_loss, emissions\n",
    "\n",
    "    def inference(self, images: torch.Tensor, inf_t = 'v' ):\n",
    "\n",
    "        self.device = images.device\n",
    "        \n",
    "        memory = self.clip_model( images ).last_hidden_state\n",
    "        memory = self.mlp_connector( memory )\n",
    "        memory += self.pos_emb( memory )\n",
    "        \n",
    "        outputs = self.bert( inputs_embeds = memory ).last_hidden_state\n",
    "        outputs = self.ln_outputs( outputs )\n",
    "        emissions = self.linear( outputs )\n",
    "\n",
    "        if self.use_repeat_logits_half == True:\n",
    "            emissions = repeat_logits_half( emissions )\n",
    "\n",
    "        if inf_t == 'v':\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._viterbi_decode(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._viterbi_decode(emissions)\n",
    "        elif inf_t == 'v-nr':\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._compute_viterbi_no_repeat(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._compute_viterbi_no_repeat(emissions)\n",
    "        else:\n",
    "            finalized_scores, finalized_tokens, = self.toplayer.crf_layer._compute_f_algorithm(emissions)\n",
    "            #finalized_scores, finalized_tokens, = self.crf_layer._compute_f_algorithm(emissions)\n",
    "\n",
    "        return finalized_scores, finalized_tokens\n",
    "\n",
    "    def repeat_logits_half(self, emissions ):\n",
    "        \n",
    "        penalty = 1.2\n",
    "        scores, preds = torch.max( emissions, 2 )\n",
    "        masks = emissions == scores[:,:,None]\n",
    "        masks = masks.permute( 1, 0, 2 )\n",
    "        new_mask = torch.zeros( (  masks.size(1), masks.size(2)), device = emissions.device, dtype=torch.bool )\n",
    "        new_masks = torch.zeros( ( masks.size(0), masks.size(1), masks.size(2)), device = emissions.device, dtype=torch.bool )\n",
    "        for i, mask in enumerate( masks ):\n",
    "            new_mask = torch.logical_or( mask,  new_mask  )\n",
    "            new_masks[i] = new_mask\n",
    "        new_masks = new_masks.transpose(0,1)\n",
    "        first_true_mask = ( new_masks.int().cumsum(dim = 1 ) == 1 ) & new_masks\n",
    "        new_masks = new_masks & ( ~first_true_mask )\n",
    "\n",
    "        p_masks = emissions > 0\n",
    "        m_masks = emissions < 0\n",
    "        p_new_masks = p_masks & new_masks\n",
    "        m_new_masks = m_masks & new_masks\n",
    "        emissions2 = emissions.clone()\n",
    "        emissions2[p_new_masks] = emissions[p_new_masks] / penalty\n",
    "        emissions2[m_new_masks] = emissions2[m_new_masks] * penalty\n",
    "\n",
    "        return emissions2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, file_path: str, img_directory: str, transforms, transforms2, tokenizer, length_max = None ) -> None:\n",
    "        super().__init__()\n",
    "        self.img_directory = img_directory\n",
    "        self.transforms = transforms\n",
    "        self.transforms2 = transforms2 \n",
    "        # TODO: fix to original data\n",
    "        #画像の前処理\n",
    "        self.img_file = []\n",
    "        self.tokens = []\n",
    "        #vocab_size = len( tokenizer )\n",
    "        #c1 = torch.zeros( ( vocab_size ) )\n",
    "        #c2 = torch.zeros( ( vocab_size, vocab_size ) )\n",
    "        if length_max == None:\n",
    "            self.length_max = 0\n",
    "        else:\n",
    "            self.length_max = length_max\n",
    "        length_sum = 0\n",
    "\n",
    "        with open( file_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        for i, line_data in enumerate( data ):\n",
    "            if i % 100000 == 0:\n",
    "                print( \"i:\", i )\n",
    "            self.img_file.append( line_data['img_file'] )\n",
    "            id_tokens = line_data['id_tokens']\n",
    "            id_tokens.append( eos_token_id )\n",
    "            id_tokens.append( eos_token_id )\n",
    "            length_sum += len( id_tokens )\n",
    "            if length_max != None:\n",
    "                id_tokens = torch.tensor( id_tokens )[:self.length_max]\n",
    "            else:\n",
    "                if self.length_max < len( id_tokens ):\n",
    "                    self.length_max = len( id_tokens )\n",
    "                id_tokens = torch.tensor( id_tokens )\n",
    "            self.tokens.append( id_tokens )\n",
    "        # w1, w2 を作る時は length_max = None　でお願いします。\n",
    "        #    for i2 in range( len(id_tokens) ):\n",
    "        #        if i2 == len( id_tokens ) - 1:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #        else:\n",
    "        #            c1[id_tokens[i2]] += 1\n",
    "        #            c2[id_tokens[i2], id_tokens[i2+1] ] += 1\n",
    "        '''\n",
    "        c1avg = int( torch.sum( c1 ) / torch.sum( torch.ne( c1, 0 ).int()) )\n",
    "        c2avg = int( torch.sum( torch.sum( c2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( c2, 0 ).int() ) )\n",
    "\n",
    "        c1[0] = c1avg\n",
    "\n",
    "        c2[:,0] = c2avg\n",
    "        c2[0,:] = c2avg\n",
    "        \n",
    "        sumc1 = torch.sum( c1, dim = 0 )\n",
    "        sumc2 = torch.sum( torch.sum( c2, dim = 1 ), dim = 0 )\n",
    "\n",
    "        prob1 = c1 / sumc1\n",
    "        prob2 = c2 / sumc2\n",
    "\n",
    "        self.w1 = prob1 ** -0.4\n",
    "        self.w1 = torch.nan_to_num( self.w1, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg1 = torch.sum( self.w1, dim = 0 ) / torch.sum( torch.ne( self.w1, 0.0 ).int() )\n",
    "        self.w1 = self.w1 / avg1\n",
    "\n",
    "        self.w2 = prob2 ** -0.4\n",
    "        self.w2 = torch.nan_to_num( self.w2, nan = 0.0, posinf=0.0, neginf=0.0 )\n",
    "        avg2 = torch.sum( torch.sum( self.w2, dim = 1 ), dim = 0 ) / torch.sum( torch.ne( self.w2, 0.0 ).int() )\n",
    "        self.w2 = self.w2 / avg2\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_unigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w1, f )\n",
    "\n",
    "        with open( \"/mnt/ssd2/v7/w_bigrma.pkl\", mode=\"wb\" ) as f:\n",
    "            pickle.dump( self.w2, f )\n",
    "        \n",
    "        '''\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_unigram.pkl\", 'rb') as f:\n",
    "        #    self.w1 = pickle.load(f)\n",
    "\n",
    "        #with open( \"/mnt/ssd2/v7/w_bigram.pkl\", 'rb') as f:\n",
    "        #    self.w2 = pickle.load(f)\n",
    "        \n",
    "        if length_max == None:\n",
    "            print( \"length max:\", self.length_max )\n",
    "            print( \"avg length:\", length_sum / len( self.tokens ) )\n",
    "    \n",
    "    # ここで取り出すデータを指定している\n",
    "    def __getitem__(\n",
    "        self,\n",
    "        index: int\n",
    "    ):\n",
    "        tokens = self.tokens[index]\n",
    "        img_file = self.img_file[index] + \".jpg\"\n",
    "        img_path = os.path.join( self.img_directory, img_file ) #index番目の画像のパスを取得\n",
    "        img = Image.open(img_path) #PIL形式で画像を読み込み\n",
    "        if img.mode != 'RGB':\n",
    "            img = img.convert(\"RGB\")\n",
    "        img1 = self.transforms(img)\n",
    "        img2 = self.transforms2(img)\n",
    "        \n",
    "        return img1, img2, tokens\n",
    "\n",
    "    # この method がないと DataLoader を呼び出す際にエラーを吐かれる\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.tokens)\n",
    "\n",
    "    def length_max(self):\n",
    "        return self.length_max\n",
    "\n",
    "    #def w1(self):\n",
    "    #    return self.w1\n",
    "\n",
    "    #def w2(self):\n",
    "    #    return self.w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_func(batch: Sequence[Tuple[Union[torch.Tensor, str]]], pad_index, length_max ):\n",
    "    imgs1, imgs2, tokens = zip(*batch)\n",
    "\n",
    "    max_length = length_max\n",
    "    #max_length = 0\n",
    "    #for target in tokens:\n",
    "    #    if max_length < len( target ):\n",
    "    #        max_length = len( target )\n",
    "    \n",
    "    targets = []\n",
    "    lengths = []\n",
    "    for target in tokens:\n",
    "        pad_len = max_length - len( target ) \n",
    "        #print( \"target:\", target )\n",
    "        input2= F.pad( target, (0, pad_len), mode='constant', value = pad_index)\n",
    "        targets.append( input2 )\n",
    "        lengths.append( len( target ) )\n",
    "    \n",
    "    imgs1 = torch.stack( imgs1, dim = 0 )\n",
    "    imgs2 = torch.stack( imgs2, dim = 0 )\n",
    "    targets = torch.stack( targets, dim = 0 )\n",
    "    lengths = torch.tensor( lengths, requires_grad = False  )\n",
    "\n",
    "    return imgs1, imgs2, targets, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"google-bert/bert-large-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_id)\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "cls_token_id = tokenizer.cls_token_id\n",
    "sep_token_id = tokenizer.sep_token_id\n",
    "sos_token_id = tokenizer.encode( [ \"[unused0]\" ] )[1]\n",
    "eos_token_id = tokenizer.encode( [ \"[unused1]\" ] )[1]\n",
    "a_token_id = tokenizer.encode( [ \"a\" ] )[1]\n",
    "the_token_id = tokenizer.encode( [ \"the\" ] )[1]\n",
    "and_token_id = tokenizer.encode( [ \"and\" ] )[1]\n",
    "in_token_id = tokenizer.encode( [ \"in\" ] )[1]\n",
    "we_token_id = tokenizer.encode( [ \"we\" ] )[1]\n",
    "i_token_id = tokenizer.encode( [ \"i\" ] )[1]\n",
    "he_token_id = tokenizer.encode( [ \"he\" ] )[1]\n",
    "she_token_id = tokenizer.encode( [ \"she\" ] )[1]\n",
    "it_token_id = tokenizer.encode( [ \"it\" ] )[1]\n",
    "they_token_id = tokenizer.encode( [ \"they\" ] )[1]\n",
    "period_token_id = tokenizer.encode( [ \".\" ] )[1]\n",
    "comma_token_id = tokenizer.encode( [ \",\" ] )[1]\n",
    "dbl_token_id = tokenizer.encode( [ '\"' ] )[1]\n",
    "sgl_token_id = tokenizer.encode( [ \"'\" ] )[1]\n",
    "\n",
    "\n",
    "# 辞書サイズを保存\n",
    "vocab_size = len( tokenizer )\n",
    "\n",
    "# 画像のtransformsを定義\n",
    "transforms = v2.Compose([\n",
    "    v2.Resize((336, 336)),\n",
    "    #v2.AutoAugment(),\n",
    "    #v2.ToTensor(),\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    ## Coco データセット 2017 train の平均と標準偏差\n",
    "    #v2.Normalize((0.456,0.427,0.401),(0.224,0.219,0.231) )\n",
    "    ## ImageNetデータセットの平均と標準偏差\n",
    "    #v2.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "    # clip の preprocessor_config.json の平均と標準偏差\n",
    "    v2.Normalize((0.48145466, 0.4578275, 0.40821073), (0.26862954, 0.26130258, 0.27577711))\n",
    "])\n",
    "\n",
    "transforms2 = v2.Compose([\n",
    "    v2.Resize((224, 224)),\n",
    "    v2.ToImage(),\n",
    "    #v2.ToDtype(torch.float32, scale=True),\n",
    "])\n",
    "\n",
    "# v7 データセット\n",
    "train_dataset = MyDataset( file_path=\"/mnt/ssd2/v7/data.pkl\",\n",
    "                           img_directory = \"/mnt/ssd2/v7/img\",\n",
    "                           transforms=transforms, transforms2 = transforms2, tokenizer = tokenizer, length_max = 97 )\n",
    "\n",
    "# Subset samplerの生成\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, 0.1, 0.1 )\n",
    "    \n",
    "# 学習時にランダムにサンプルするためのサンプラー\n",
    "train_sampler = SubsetRandomSampler(train_set)\n",
    "\n",
    "# DataLoaderを生成\n",
    "collate_func_lambda = lambda x: collate_func(x, tokenizer.pad_token_id, length_max = 97 )\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    #batch_size=config.batch_size,\n",
    "                    batch_size=4,\n",
    "                    num_workers=0,\n",
    "                    sampler=test_set,\n",
    "                    collate_fn=collate_func_lambda)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "#model = CaptioningTransformer( img_size = 336, dim_embedding=1024, length_max = 97, vocab_size=len(tokenizer),\n",
    "#                 tokenizer=tokenizer, dropout=0.1).to(device)\n",
    "#model = CaptioningTransformer( 336,\n",
    "#    1024, 97, len( tokenizer ),\n",
    "#    tokenizer, 0.0, pad_token_id = tokenizer.pad_token_id,\n",
    "#    use_repeat_logits_half = False,\n",
    "#    crf_coef = 0.0, temp=1.0 )\n",
    "model = CaptioningTransformer( 336,\n",
    "    1024, 97, len( tokenizer ),\n",
    "    tokenizer, 0.0, pad_token_id = tokenizer.pad_token_id,\n",
    "    use_repeat_logits_half = False,\n",
    "    crf_coef = 0.0, temp=0.710 )\n",
    "#model.to(config.device)\n",
    "model.to(device)\n",
    "\n",
    "PATH = \"/mnt/ssd2/model_rl_ppo_critic_crf32_52_curr.pth\"\n",
    "if os.path.isfile(PATH):\n",
    "    checkpoint = torch.load(PATH, map_location=torch.device(\"cpu\"))\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    print( \"paramerters were loaded.\" )\n",
    "\n",
    "compute_reward = ComputeReward(reward_t = \"ord+rep+len+unr\", decode_t = \"no-pad\", \n",
    "                          device = device, sentence_level_metric=\"special\", repeat_thresh = [3,2,2,2], \n",
    "                          repeat_weight = [1,1,1,1], cider_coef = 1.0, rouge_coef = 1.0, \n",
    "                          clip_coef = 1.0, bert_coef = 1.0, use_amp = False )\n",
    "\n",
    "images = torch.randn( ( 2, 3, 336,336 ), device = device )\n",
    "scores, tokens = model.inference( images, inf_t= 'v' )\n",
    "\n",
    "print( scores.size() )\n",
    "print( tokens.size() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_num = 80\n",
    "batch_size = 8\n",
    "\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "my_decode = False\n",
    "#my_decode = True\n",
    "\n",
    "# Subset samplerの生成\n",
    "test_set, val_set, train_set = util.generate_subset_test_val_train(\n",
    "    train_dataset, 0.1, 0.1 )\n",
    "\n",
    "test_set = test_set[:test_num]\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                    train_dataset,\n",
    "                    #batch_size=config.batch_size,\n",
    "                    batch_size=batch_size,\n",
    "                    num_workers=0,\n",
    "                    sampler=test_set,\n",
    "                    collate_fn=collate_func_lambda)\n",
    "\n",
    "test_pr_coef = 1\n",
    "\n",
    "fn = bleu_score.SmoothingFunction().method7\n",
    "\n",
    "transforms_inv = v2.Compose([\n",
    "    v2.Normalize((-0.48145466/0.26862954, -0.4578275/0.26130258, -0.40821073/0.27577711), (1/0.26862954,1/0.26130258,1/0.27577711)),\n",
    "    v2.ToPILImage()\n",
    "])\n",
    "\n",
    "# test\n",
    "with tqdm(test_loader) as pbar:\n",
    "    pbar.set_description(f'[テスト]')\n",
    "\n",
    "    # 評価モード\n",
    "    model.eval()\n",
    "\n",
    "    test_ciders = deque()\n",
    "    test_rouges = deque()\n",
    "    test_clips = deque()\n",
    "    test_berts = deque()\n",
    "    n_iter = 0\n",
    "    for k, (imgs, imgs2, captions, caption_lengths) in enumerate( pbar ):\n",
    "        print( \"k:\", k )\n",
    "        imgs = imgs.to(device)\n",
    "        captions = captions.to(device)\n",
    "        #caption_lengths = torch.tensor( caption_lengths ).to(config.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            finalized_scores, finalized_tokens = model.inference( imgs, inf_t = 'v' )\n",
    "            hypo_ids = finalized_tokens\n",
    "        \n",
    "        n = 0\n",
    "        hypo_sentence = []\n",
    "        ref_sentence = []\n",
    "        ref_imgs = []\n",
    "        total_error = 0\n",
    "        total_token_length = 0\n",
    "        total_bleu = 0\n",
    "        preds_str = [\n",
    "            tokenizer.decode([i for i in pred if i != eos_token_id \\\n",
    "            and i != pad_token_id ] ) \n",
    "            for pred in hypo_ids\n",
    "        ]\n",
    "        preds_str2 = [\n",
    "            tokenizer.decode([i for i in pred if i != eos_token_id], skip_special_tokens=True) \n",
    "            for pred in hypo_ids\n",
    "        ]\n",
    "        targets_str = [tokenizer.decode(\n",
    "            [ i for i in target \\\n",
    "            if i != pad_token_id \\\n",
    "             and i != eos_token_id ]       \n",
    "            ) for target in captions]\n",
    "        hypo_tokens = tokenizer.tokenize( preds_str[0] )\n",
    "        ref_tokens = tokenizer.tokenize( targets_str[0] )\n",
    "\n",
    "        pred_dict = { str(i): [item] for i, item in enumerate( preds_str)}\n",
    "        target_dict = { str(i): [item] for i, item in enumerate( targets_str)}\n",
    "        avg_cider, scores = compute_reward.scorer.compute_score(target_dict, pred_dict) # cider の計算        \n",
    "        rouge_scores = [compute_reward.rougeL.score(target, pred)['rougeL'][0] for pred, target in zip(preds_str, targets_str)]\n",
    "        avg_rouge = sum( rouge_scores ) / len( rouge_scores )\n",
    "        clip_scores = [compute_reward.metric( img2, pred).detach() for img2, pred in zip( imgs2, preds_str2 )]\n",
    "        clip_score = sum( clip_scores ) / len( clip_scores ) / 100.0\n",
    "        bert_scores = compute_reward.bert.compute(predictions=preds_str, references=targets_str, model_type=model_name, \\\n",
    "                                                              lang='en',  device=device)['f1']\n",
    "        bert_scores = [score for score in bert_scores]\n",
    "        bert_score = sum( bert_scores ) / len( bert_scores )\n",
    "            \n",
    "        ## 認識誤りを計算\n",
    "        #(error, substitute, delete, insert, ref_length) = levenshtein.calculate_error(hypo_tokens,ref_tokens)\n",
    "        \n",
    "        ## 誤り文字数を累積する\n",
    "        #total_error += error\n",
    "        ## 文字の総数を累積する\n",
    "        #total_token_length += ref_length\n",
    "\n",
    "\n",
    "        inv_img = transforms_inv( imgs[0])\n",
    "        plt.imshow( inv_img )\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "        print( \"hypo:\", preds_str[0] )\n",
    "        print( \"refe:\", targets_str[0] )\n",
    "        print( \"\\n\" )\n",
    "        \n",
    "        for pred, target in zip( preds_str, targets_str ):\n",
    "            print( \"hypo:\", pred )\n",
    "            print( \"refe:\", target )\n",
    "  \n",
    "        print( \"these pics. cider :\", avg_cider )\n",
    "        print( \"these pics. rouge:\", avg_rouge )\n",
    "        print( \"these pics. clip_score:\", clip_score )\n",
    "        print( \"these pics. bert_score:\", bert_score )\n",
    "        test_ciders.append(avg_cider)\n",
    "        test_rouges.append(avg_rouge)\n",
    "        test_clips.append(clip_score)\n",
    "        test_berts.append(bert_score)\n",
    "        n_iter += 1\n",
    "        print(f'test number = {n_iter} average, cider = {torch.Tensor(test_ciders).mean().item()}, ' \\\n",
    "                f'rouge = {torch.Tensor(test_rouges).mean().item()}, clip_score = {torch.Tensor(test_clips).mean().item()}, ' \\\n",
    "                f'bert_score = {torch.Tensor(test_berts).mean().item()}' )\n",
    "        print( \"\\n\\n\" )\n",
    "        \n",
    "            \n",
    "        #if len(test_errors) > config.moving_avg:\n",
    "        if len(test_ciders) > 100:\n",
    "            test_ciders.popleft()\n",
    "            test_rouges.popleft()\n",
    "            test_clips.popleft()\n",
    "            test_berts.popleft()\n",
    "        pbar.set_postfix({\n",
    "            #'loss': torch.Tensor(test_losses).mean().item(),\n",
    "            'cider': torch.Tensor(test_ciders).mean().item(),\n",
    "            'rougeL': torch.Tensor(test_rouges).mean().item(),\n",
    "            'clip_score': torch.Tensor(test_clips).mean().item(),\n",
    "            'bert_score': torch.Tensor(test_berts).mean().item()\n",
    "        })                \n",
    "\n",
    "# 表示\n",
    "test_cider = np.mean( test_ciders )\n",
    "test_rouge = np.mean( test_rouges )\n",
    "test_clip = np.mean( test_clips )\n",
    "test_bert = np.mean( test_berts )\n",
    "print(f'test {n_iter} * {batch_size} average cider : {test_cider}')\n",
    "print(f'test {n_iter} * {batch_size} average rougeL: {test_rouge}')\n",
    "print(f'test {n_iter} * {batch_size} average clip_score: {test_clip}')\n",
    "print(f'test {n_iter} * {batch_size} average bert_score: {test_bert}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "b411586366dd8bdb0fc2800d6aef8fbd6cc14d2a48eec1b457375028a8621915"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "10ac0991cf0d4346a2dd950c4e69de74": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dc134416b78b4e16ab143f7b3c9b4db1",
      "placeholder": "​",
      "style": "IPY_MODEL_3e3ad9c31b8244d9987cb5d09ac80909",
      "value": " 230M/230M [00:03&lt;00:00, 57.2MB/s]"
     }
    },
    "189ddc1a5e674235a5a4c9461ec37bb7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89ed549065394bad9c015c612b04c1d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f9994c1870c348b0ab983404decb722e",
      "value": "100%"
     }
    },
    "2030796b20a24135908446e8a05b2447": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e3ad9c31b8244d9987cb5d09ac80909": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "741ba7eec1f94e6889152972dbf220c0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89ed549065394bad9c015c612b04c1d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "af8d4c88aa714714b864351862297512": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_741ba7eec1f94e6889152972dbf220c0",
      "max": 241669177,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_be3b477f459a4fdd80b055ef88cf8b52",
      "value": 241669177
     }
    },
    "be3b477f459a4fdd80b055ef88cf8b52": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "dc134416b78b4e16ab143f7b3c9b4db1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ee1785c740b7449584ceec0225c64c78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_189ddc1a5e674235a5a4c9461ec37bb7",
       "IPY_MODEL_af8d4c88aa714714b864351862297512",
       "IPY_MODEL_10ac0991cf0d4346a2dd950c4e69de74"
      ],
      "layout": "IPY_MODEL_2030796b20a24135908446e8a05b2447"
     }
    },
    "f9994c1870c348b0ab983404decb722e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
